{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "704be11d",
   "metadata": {},
   "source": [
    "### GIZ Initial Data Exploration\n",
    "#### author: Emily Robitschek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ca671e",
   "metadata": {},
   "source": [
    "Purpose: Before we build any model, we need to be able to take a look at the documents we have.\n",
    "\n",
    "Some resources: \n",
    "\n",
    "#### papers mentioned in project proposal: \n",
    "https://medium.com/fiscalnoteworthy/citing-your-sources-79062248f468\n",
    "https://www2.deloitte.com/us/en/insights/focus/cognitive-technologies/natural-language-processing-examples-in-government-data.html\n",
    "https://documents1.worldbank.org/curated/en/634591516387264234/pdf/WPS8310.pdf\n",
    "\n",
    "#### NLP related links: \n",
    "- https://spacy.io/usage/spacy-101\n",
    "- https://towardsdatascience.com/text-pre-processing-stop-words-removal-using-different-libraries-f20bac19929a\n",
    "- https://arunm8489.medium.com/getting-started-with-natural-language-processing-6e593e349675\n",
    "- https://towardsdatascience.com/natural-language-processing-pipeline-decoded-f97a4da5dbb7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a581e19b",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2d30a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T09:13:43.850210Z",
     "start_time": "2021-10-09T09:13:42.671340Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import json\n",
    "\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#set up packages for NLP analysis\n",
    "import contractions\n",
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm') #or the multi-language one: spacy.load('xx_ent_wiki_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334ff084",
   "metadata": {},
   "source": [
    "### Define helper functions for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbb2783",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T09:13:43.866983Z",
     "start_time": "2021-10-09T09:13:43.853684Z"
    }
   },
   "outputs": [],
   "source": [
    "#input data helper functions\n",
    "def get_docs_df_from_folder(policy_doc_folder):\n",
    "    \"\"\"\n",
    "    Takes in a folder (can also be with different subfolders) with policy-related text documents \n",
    "    and gathers txt docs to analyze from those folders and makes a dataframe of their names and paths.\n",
    "    \n",
    "    NOTE: If want to preserve names and paths of the documents and make them easily searchable, it might be useful \n",
    "    to export the dictionary/keep that as well to add more summary information about the document for instance. \n",
    "    \"\"\"\n",
    "    #get the paths and file names\n",
    "    policy_doc_names, policy_doc_paths = list_docs(policy_doc_folder)\n",
    "    #print the number of docs and the names of some of them \n",
    "    print((\"There are %d policy docs\" % (len(policy_doc_names))),\n",
    "          \"Some of the policy docs include: \", policy_doc_names[:10])\n",
    "\n",
    "    policy_doc_dict = {'policy_doc_names': policy_doc_names, 'policy_doc_paths': policy_doc_paths}\n",
    "    policy_doc_df = pd.DataFrame(data=policy_doc_dict, dtype='string')\n",
    "    #set index as policy doc names (can clean up/add other column with a neater name without the .txt pieces later)\n",
    "    policy_doc_df['policy_doc_name_clean'] = (policy_doc_df['policy_doc_names']\n",
    "                                              .apply(lambda x: x.split('.txt')[0].split('.pdf.ocr')[0]))\n",
    "    policy_doc_df.index = policy_doc_df['policy_doc_names']\n",
    "    del policy_doc_df['policy_doc_names'] #remove duplicate column\n",
    "    return policy_doc_df\n",
    "\n",
    "\n",
    "\n",
    "def list_docs(folder):\n",
    "    \"\"\"\n",
    "    Generates a list of document names for reference and tracking. \n",
    "    This command currently extracts the .txt documents from all the subfolders of a parent folder, \n",
    "    and filters out the ones containing source information, which we might not want to use in our analysis.\n",
    "    \"\"\"\n",
    "    doc_names = []\n",
    "    doc_paths = []\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt') and (file not in ['Source.txt', 'Source Link.txt', 'Source Links.txt']):\n",
    "                doc_names.append(file)\n",
    "                doc_paths.append(os.path.join(root, file))  \n",
    "    return doc_names, doc_paths\n",
    "\n",
    "#NLP related helper functions\n",
    "#used these resources: \n",
    "#https://www.geeksforgeeks.org/nlp-expand-contractions-in-text-processing/)\n",
    "#https://realpython.com/natural-language-processing-spacy-python/#how-to-download-models-and-data\n",
    "\n",
    "def fix_contractions(document): \n",
    "    \"\"\"\n",
    "    Switches contractions like can't to cannot so potentially important words/pieces of words are \n",
    "    removed with punctuation removal.\n",
    "    \"\"\"\n",
    "    # original contracted text\n",
    "    text = open(document).read()\n",
    "    \n",
    "    #creating an empty list\n",
    "    expanded_words = []    \n",
    "    for word in text.split():\n",
    "        #using contractions.fix to expand the shortened words \n",
    "        expanded_words.append(contractions.fix(word))   \n",
    "\n",
    "    expanded_text = ' '.join(expanded_words)\n",
    "    print('Original text: ', len(text))\n",
    "    print('Expanded_text: ', len(expanded_text))\n",
    "    return expanded_text\n",
    "\n",
    "def is_token_allowed(token):\n",
    "    '''\n",
    "        Only allow valid tokens which are not stop words\n",
    "        and punctuation symbols.\n",
    "    '''\n",
    "    if (not token or not token.text.strip() or\n",
    "        token.is_stop or token.is_punct):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def preprocess_token(token):\n",
    "    # Reduce token to its lowercase lemma form\n",
    "    return token.lemma_.strip().lower()\n",
    "\n",
    "def preprocess_doc(doc_path): \n",
    "    \"\"\"\n",
    "    Applies NLP preprocessing framework to a document returns the tokens and sentences.\n",
    "    NOTE: may want to also return sentences and other objects too depending on the use case\n",
    "    \"\"\"\n",
    "    #remove contracted words and tokenize the document\n",
    "    tokens = nlp(fix_contractions(doc_path))\n",
    "    \n",
    "    #extract list of word tokens\n",
    "    token_list = [token for token in tokens]\n",
    "    #print('These are some of the unfiltered tokens: ', tokens[0:50], '\\n')\n",
    "\n",
    "    #find sentences\n",
    "    sentences = list(tokens.sents)\n",
    "    for sentence in sentences[:10]:\n",
    "        print(sentence)\n",
    "    return tokens, token_list, sentences\n",
    "\n",
    "def filter_modify_tokens(tokens):\n",
    "    \"\"\"\n",
    "    This function takes a collection of tokens from the nlp() function applied to text \n",
    "    and generates a list of filtered tokens that we then convert into a filtered text and \n",
    "    collection of filtered tokens.\n",
    "    \n",
    "    NOTE: still need to filter out super weird non words and may want to filter numbers and\n",
    "    may want to find some important accronyms too (so maybe modify this function later)\n",
    "    \"\"\"\n",
    "    #filter tokens, and make lowercase and lemmatize (with preprocess function): \n",
    "    filtered_text_list = [preprocess_token(token) for token in \n",
    "                       tokens if is_token_allowed(token)]\n",
    "\n",
    "    filtered_text = ' '.join(filtered_text_list)\n",
    "    filtered_tokens = nlp(filtered_text)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e16b60",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6accc9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T09:13:43.880984Z",
     "start_time": "2021-10-09T09:13:43.868790Z"
    }
   },
   "outputs": [],
   "source": [
    "#keywords (just for testing purposes - from the policy proposal from GIZ)\n",
    "policy = [\"policy\", \"integrate\", \"implement\", \"committee\", \"consultation\"]\n",
    "food = [\"nutritions\", \"diets\", \"farm\", \"agriculture\", \"ecology\"]\n",
    "ndc_national_adaption_plan = [\"nap\", \"sector plan\", \"nccrp\", \"vulnerable sector\", \n",
    "                              \"geographic vulnerability\"]\n",
    "ndc_climate_change = [\"adaption\", \"program\", \"projects\", \"resilience\", \"institution\",\n",
    "                      \"capacity\", \"response\", \"budget\", \"reprioritisation\", \"development\", \n",
    "                      \"planner\", \"regulator\", \"practitioners\", \"geographical\", \n",
    "                      \"circumstances\", \"land\", \"scheme\", \"authorisation\", \"system\", \n",
    "                      \"spluma\"]\n",
    "ndc_early_warning = [\"system\", \"vulnerability\", \"needs\", \"assessment\", \"network\", \"weather\",\n",
    "   \"earth\", \"observation\", \"academic\", \"community\"]\n",
    "\n",
    "#keywrods from json file\n",
    "json_keywords_SA_file = '../ndc_keywords/ndc_south_africa.json'\n",
    "keywords_SA_dict = None\n",
    "with open(json_keywords_SA_file, 'r') as f: \n",
    "    keywords_SA_dict = json.load(f)\n",
    "keywords_SA_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df47609",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T09:13:43.898429Z",
     "start_time": "2021-10-09T09:13:43.883188Z"
    }
   },
   "outputs": [],
   "source": [
    "#here is the general folder with the different types of policy documents\n",
    "policy_doc_folder = '../../giz-policy_tracking_docs/SouthAfrica/Data'\n",
    "#load df of docs\n",
    "policy_doc_df = get_docs_df_from_folder(policy_doc_folder)\n",
    "policy_doc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57d7cae",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdd64a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T09:13:51.338402Z",
     "start_time": "2021-10-09T09:13:43.900698Z"
    }
   },
   "outputs": [],
   "source": [
    "##lets take a look at the document from South Africa used in the example in the proposal first:\n",
    "doc_name = '2nd National Biodiversity Strategy Action Plan 2015-2025.pdf_ocr.txt'\n",
    "##lets try with another document\n",
    "#test_doc_name = 'InSessionSpecialEditionBudget2021.pdf.ocr.txt'\n",
    "doc_path = policy_doc_df.loc[doc_name]['policy_doc_paths']\n",
    "\n",
    "#can also simply specify the first or whichever document in the list by order: \n",
    "#(this will be helpful for future versions that involve more interrating over different documents)\n",
    "i=40\n",
    "doc_name = policy_doc_df.iloc[i]['policy_doc_name_clean']\n",
    "doc_path = policy_doc_df.iloc[i]['policy_doc_paths']\n",
    "print(doc_name, doc_path)\n",
    "tokens, token_list, sentences = preprocess_doc(doc_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300b2e61",
   "metadata": {},
   "source": [
    "#### The token object: \n",
    "The tokens have all sorts of useful information association with them, for instance their positions (in token.idx) which we can use these later to define windows. See below for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a514aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T09:13:51.345525Z",
     "start_time": "2021-10-09T09:13:51.340709Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for token in token_list[:20]:\n",
    "    print (token, token.idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89452ae3",
   "metadata": {},
   "source": [
    "We can see from above that the tokens need to be filtered and it might be useful if the words are all made lowercase and the words are lemmatized so the different forms of a word are recognized as the same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e2e270",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T09:13:54.761478Z",
     "start_time": "2021-10-09T09:13:51.347648Z"
    }
   },
   "outputs": [],
   "source": [
    "filtered_tokens = filter_modify_tokens(tokens)\n",
    "print('These are some of the filtered tokens: ', filtered_tokens[200:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddeddce",
   "metadata": {},
   "source": [
    "### Find most common (and unique) words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bb1aa3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T09:13:54.777546Z",
     "start_time": "2021-10-09T09:13:54.762788Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# Remove stop words and punctuation symbols\n",
    "words = [token.text for token in filtered_tokens]\n",
    "word_freq = Counter(words)\n",
    "# 5 commonly occurring words with their frequencies\n",
    "common_words = word_freq.most_common(30)\n",
    "print(common_words)\n",
    "\n",
    "# Unique words\n",
    "#unique_words = [word for (word, freq) in word_freq.items() if freq == 1]\n",
    "#print (unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431501fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T09:13:54.782499Z",
     "start_time": "2021-10-09T09:13:54.778942Z"
    }
   },
   "outputs": [],
   "source": [
    "word_freq['environmental']\n",
    "ndc_climate_change_scores = []\n",
    "for word in ndc_climate_change:\n",
    "    #print(word, word_freq[word])\n",
    "    ndc_climate_change_scores.append(word_freq[word])\n",
    "print(\"This document has the following number of words related to climate change NDCs: \",\n",
    "      sum(ndc_climate_change_scores), '\\n')\n",
    "\n",
    "food_scores = []\n",
    "for word in food:\n",
    "    #print(word, word_freq[word])\n",
    "    food_scores.append(word_freq[word])\n",
    "print(\"This document has the following number of words related to food: \",\n",
    "      sum(food_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada47c6b",
   "metadata": {},
   "source": [
    "### Make some plots of the NDC/Thematic key words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcf73db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T09:13:55.552604Z",
     "start_time": "2021-10-09T09:13:54.785655Z"
    }
   },
   "outputs": [],
   "source": [
    "#just to test - need to modularise/make more reproducible and tailor output to be useful across documents\n",
    "#could also link this back to the df of the documents to output a table with some summary metrics for keywords \n",
    "#in eaach document of interest\n",
    "\n",
    "graphs_folder = '../../outputs/test_plots/bar_charts/'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "x = food\n",
    "y = food_scores\n",
    "x_pos = [i for i, _ in enumerate(x)]\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (2,4)\n",
    "plt.bar(x, y)\n",
    "plt.xlabel(\"Food Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "title = (\"Food words in: %s\" % (doc_name))\n",
    "plt.title(title)\n",
    "plt.xticks(x_pos, x, rotation=90)\n",
    "plt.savefig((graphs_folder + 'bar_chart_%s.pdf' % (title)), \n",
    "            bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "x = ndc_climate_change\n",
    "y = ndc_climate_change_scores\n",
    "x_pos = [i for i, _ in enumerate(x)]\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (8,4)\n",
    "plt.bar(x, y, color='green') \n",
    "plt.xlabel(\"NDC Climate Change Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "title = \"NDC Climate Change words in: %s\" % (doc_name)\n",
    "plt.title(title)\n",
    "plt.xticks(x_pos, x, rotation=90)\n",
    "plt.savefig((graphs_folder + 'bar_chart_%s.pdf' % (title)), \n",
    "            bbox_inches='tight')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c7511c",
   "metadata": {},
   "source": [
    "### Parts of speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b40fc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T09:13:55.571094Z",
     "start_time": "2021-10-09T09:13:55.553856Z"
    }
   },
   "outputs": [],
   "source": [
    "for token in filtered_tokens[:50]:\n",
    "    print(token, token.tag_, token.pos_, spacy.explain(token.tag_))\n",
    "print('\\n')\n",
    "    \n",
    "nouns = []\n",
    "adjectives = []\n",
    "for token in filtered_tokens:\n",
    "    if token.pos_ == 'NOUN':\n",
    "        nouns.append(token)\n",
    "    if token.pos_ == 'ADJ':\n",
    "        adjectives.append(token)\n",
    "\n",
    "print('some nouns spacy called in the document include:', '\\n', nouns[:50], '\\n')\n",
    "print('some adjectives spacy called in the document include:', '\\n', adjectives[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc72129",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773f4581",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
