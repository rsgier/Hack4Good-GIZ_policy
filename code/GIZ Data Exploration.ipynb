{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "704be11d",
   "metadata": {},
   "source": [
    "### GIZ Initial Data Exploration\n",
    "#### author: Emily Robitschek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ca671e",
   "metadata": {},
   "source": [
    "Purpose: Before we build any model, we need to be able to take a look at the documents we have.\n",
    "\n",
    "Some resources: \n",
    "\n",
    "#### papers mentioned in project proposal: \n",
    "https://medium.com/fiscalnoteworthy/citing-your-sources-79062248f468\n",
    "https://www2.deloitte.com/us/en/insights/focus/cognitive-technologies/natural-language-processing-examples-in-government-data.html\n",
    "https://documents1.worldbank.org/curated/en/634591516387264234/pdf/WPS8310.pdf\n",
    "\n",
    "#### NLP related links: \n",
    "- https://spacy.io/usage/spacy-101\n",
    "- https://towardsdatascience.com/text-pre-processing-stop-words-removal-using-different-libraries-f20bac19929a\n",
    "- https://arunm8489.medium.com/getting-started-with-natural-language-processing-6e593e349675\n",
    "- https://towardsdatascience.com/natural-language-processing-pipeline-decoded-f97a4da5dbb7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a581e19b",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2d30a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T17:38:07.172675Z",
     "start_time": "2021-10-11T17:38:04.283957Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import json\n",
    "import codecs\n",
    "\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#set up packages for processing data types and for NLP analysis\n",
    "from collections import OrderedDict\n",
    "import contractions\n",
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm') #or the multi-language one: spacy.load('xx_ent_wiki_sm')\n",
    "\n",
    "#graphing/visualization packages: \n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334ff084",
   "metadata": {},
   "source": [
    "### Define helper functions for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbb2783",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T17:38:07.191447Z",
     "start_time": "2021-10-11T17:38:07.174978Z"
    }
   },
   "outputs": [],
   "source": [
    "from datahelper import *\n",
    "from nlppreprocess import *\n",
    "from nlpanalysis import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e16b60",
   "metadata": {},
   "source": [
    "### Import data: Keywords from NDCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6accc9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T17:38:07.234191Z",
     "start_time": "2021-10-11T17:38:07.192870Z"
    }
   },
   "outputs": [],
   "source": [
    "#keywords (just for testing purposes - from the policy proposal from GIZ)\n",
    "policy = [\"policy\", \"integrate\", \"implement\", \"committee\", \"consultation\"]\n",
    "food = [\"nutritions\", \"diets\", \"farm\", \"agriculture\", \"ecology\"]\n",
    "ndc_national_adaption_plan = [\"nap\", \"sector plan\", \"nccrp\", \"vulnerable sector\", \n",
    "                              \"geographic vulnerability\"]\n",
    "ndc_climate_change = [\"adaption\", \"program\", \"projects\", \"resilience\", \"institution\",\n",
    "                      \"capacity\", \"response\", \"budget\", \"reprioritisation\", \"development\", \n",
    "                      \"planner\", \"regulator\", \"practitioners\", \"geographical\", \n",
    "                      \"circumstances\", \"land\", \"scheme\", \"authorisation\", \"system\", \n",
    "                      \"spluma\"]\n",
    "ndc_early_warning = [\"system\", \"vulnerability\", \"needs\", \"assessment\", \"network\", \"weather\",\n",
    "   \"earth\", \"observation\", \"academic\", \"community\"]\n",
    "\n",
    "#keywords from json file\n",
    "json_keywords_SA_file = '../ndc_keywords/ndc_south_africa.json'\n",
    "keywords_SA_dict = None\n",
    "with open(json_keywords_SA_file, 'r') as f: \n",
    "    keywords_SA_dict = json.load(f)\n",
    "keywords_SA_dict\n",
    "\n",
    "ndc_dict = make_filtered_tokens_from_ndc(keywords_SA_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e306e11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T17:38:07.238856Z",
     "start_time": "2021-10-11T17:38:07.236926Z"
    }
   },
   "outputs": [],
   "source": [
    "### THIS IS A PLACEHOLDER ###\n",
    "#get words directly from NDCs (eventually want to get words from the NDC itself and group them by topics.)\n",
    "#\n",
    "#\n",
    "#\n",
    "ndc_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f87631",
   "metadata": {},
   "source": [
    "### Import data: Policy-related documents to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df47609",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T17:38:07.257528Z",
     "start_time": "2021-10-11T17:38:07.240432Z"
    }
   },
   "outputs": [],
   "source": [
    "#here is the general folder with the different types of policy documents\n",
    "policy_doc_folder = '../giz-policy_tracking_docs/SouthAfrica/Data'\n",
    "#get df of docs\n",
    "policy_doc_df = get_docs_df_from_folder(policy_doc_folder)\n",
    "policy_doc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57d7cae",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdd64a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T17:38:14.649941Z",
     "start_time": "2021-10-11T17:38:07.258667Z"
    }
   },
   "outputs": [],
   "source": [
    "##lets take a look at the document from South Africa used in the example in the proposal first:\n",
    "doc_name = '2nd National Biodiversity Strategy Action Plan 2015-2025.pdf_ocr.txt'\n",
    "doc_path = policy_doc_df.loc[doc_name]['policy_doc_paths']\n",
    "\n",
    "#can also simply specify the first or whichever document in the list by order: \n",
    "#(this will be helpful for future versions that involve more interrating over different documents)\n",
    "#i=53\n",
    "#doc_name = policy_doc_df.iloc[i]['policy_doc_name_clean']\n",
    "#doc_path = policy_doc_df.iloc[i]['policy_doc_paths']\n",
    "print(doc_name, doc_path)\n",
    "tokens, token_list, sentences = preprocess_doc(doc_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300b2e61",
   "metadata": {},
   "source": [
    "#### The token object: \n",
    "The tokens have all sorts of useful information association with them, for instance their positions (in token.idx) which we can use these later to define windows. See below for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a514aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T17:38:14.654813Z",
     "start_time": "2021-10-11T17:38:14.651307Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for token in token_list[:20]:\n",
    "    print (token, token.idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89452ae3",
   "metadata": {},
   "source": [
    "We can see from above that the tokens need to be filtered and it might be useful if the words are all made lowercase and the words are lemmatized so the different forms of a word are recognized as the same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e2e270",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T17:38:18.036372Z",
     "start_time": "2021-10-11T17:38:14.656132Z"
    }
   },
   "outputs": [],
   "source": [
    "filtered_tokens = filter_modify_tokens(tokens)\n",
    "print('These are some of the filtered tokens: ', filtered_tokens[200:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddeddce",
   "metadata": {},
   "source": [
    "### Find most common (and unique) words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bb1aa3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T17:38:18.057660Z",
     "start_time": "2021-10-11T17:38:18.038362Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# Remove stop words and punctuation symbols\n",
    "words = [token.text for token in filtered_tokens]\n",
    "word_freq = Counter(words)\n",
    "# 5 commonly occurring words with their frequencies\n",
    "common_words = word_freq.most_common(30)\n",
    "print(common_words)\n",
    "\n",
    "# Unique words\n",
    "#unique_words = [word for (word, freq) in word_freq.items() if freq == 1]\n",
    "#print (unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada47c6b",
   "metadata": {},
   "source": [
    "### Make some plots of the NDC/Thematic key words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f0ef52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T17:38:19.016475Z",
     "start_time": "2021-10-11T17:38:18.061908Z"
    }
   },
   "outputs": [],
   "source": [
    "#just to test - need to modularise/make more reproducible and tailor output to be useful across documents\n",
    "#could also link this back to the df of the documents to output a table with some summary metrics for keywords \n",
    "#in eaach document of interest\n",
    "\n",
    "graphs_folder = '../../outputs/test_plots/bar_charts/'    \n",
    "    \n",
    "for key in ndc_dict.keys(): \n",
    "    print(\"Graphing the occurences of %s words in the document\" % key)\n",
    "    topic_frequencies = topic_frequency_subset(word_freq, ndc_dict, str(key))\n",
    "    graph_word_freq_ndc(topic_frequencies, str(key), doc_name, graphs_folder) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900e8fb0",
   "metadata": {},
   "source": [
    "### Lets take a closer look at the climate change NDC keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462068c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T17:38:19.417780Z",
     "start_time": "2021-10-11T17:38:19.017600Z"
    }
   },
   "outputs": [],
   "source": [
    "for key in ndc_dict.keys(): \n",
    "    if key == 'climate change':\n",
    "        print(\"Graphing the occurences of %s words in the document\" % key)\n",
    "        topic_frequencies = topic_frequency_subset(word_freq, ndc_dict, str(key))\n",
    "        graph_word_freq_ndc(topic_frequencies, str(key), doc_name, graphs_folder) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0acbd5",
   "metadata": {},
   "source": [
    "### Where do these words appear in the document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcf73db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T17:38:19.460851Z",
     "start_time": "2021-10-11T17:38:19.419044Z"
    }
   },
   "outputs": [],
   "source": [
    "#[token.idx in tokens for token.text in words]\n",
    "ndc_climate_idxs = [token.idx for token in tokens if token.text in words]\n",
    "ndc_climate_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b427ef99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T14:10:58.274266Z",
     "start_time": "2021-10-09T14:10:58.266729Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80c7511c",
   "metadata": {},
   "source": [
    "### Parts of speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b40fc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T17:38:19.479231Z",
     "start_time": "2021-10-11T17:38:19.462030Z"
    }
   },
   "outputs": [],
   "source": [
    "for token in filtered_tokens[:50]:\n",
    "    print(token, token.tag_, token.pos_, spacy.explain(token.tag_))\n",
    "print('\\n')\n",
    "    \n",
    "nouns = []\n",
    "adjectives = []\n",
    "for token in filtered_tokens:\n",
    "    if token.pos_ == 'NOUN':\n",
    "        nouns.append(token)\n",
    "    if token.pos_ == 'ADJ':\n",
    "        adjectives.append(token)\n",
    "\n",
    "print('some nouns spacy called in the document include:', '\\n', nouns[:50], '\\n')\n",
    "print('some adjectives spacy called in the document include:', '\\n', adjectives[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b519308c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T11:56:38.400728Z",
     "start_time": "2021-10-09T11:56:38.397730Z"
    }
   },
   "source": [
    "### Entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773f4581",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T17:38:19.497291Z",
     "start_time": "2021-10-11T17:38:19.480454Z"
    }
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "#can display entity property for the tokens as well: \n",
    "entities=[(i, i.label_, i.label) for i in filtered_tokens[1400:1700].ents]\n",
    "print(entities[:10])\n",
    "\n",
    "token_subset = tokens[100:500]\n",
    "displacy.render(token_subset, style = \"ent\", jupyter = True) #use original tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d245d4",
   "metadata": {},
   "source": [
    "### Dependency visualization in sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24796c38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T17:38:19.536176Z",
     "start_time": "2021-10-11T17:38:19.498889Z"
    }
   },
   "outputs": [],
   "source": [
    "sent_example = nlp(\"For example, it is estimated that between 9 and 12 million DATE people in impoverished rural areas directly use natural resources such as fuel wood, wild fruits and wooden utensils as a source of energy, food and building material respectively (Shackleton ORG 2004)\")\n",
    "sentence_spans = list(sentences)\n",
    "sentence_spans[:10]\n",
    "displacy.render(sentence_spans[80], style=\"dep\", jupyter= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfcc675",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b467681",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T16:54:48.667689Z",
     "start_time": "2021-10-09T16:54:48.659224Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d940a0dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T14:10:59.123635Z",
     "start_time": "2021-10-09T14:10:58.811085Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c6da743",
   "metadata": {},
   "source": [
    "### Experimenting with gensim and LDA modelling \n",
    "Initial results dont look great at the document level for single documents and the corpus, and highlight the need for better preprocesing and whether its better to do LDA on snippets rather than whole documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58fbc745",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T17:38:28.933070Z",
     "start_time": "2021-10-11T17:38:19.537937Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_nlp_processing_framework(doc_name, doc_path):\n",
    "    \"\"\"Apply the NLP procesing functions to each document\"\"\"\n",
    "    tokens, token_list, sentences = preprocess_doc(doc_path)\n",
    "    filtered_tokens = filter_modify_tokens(tokens) #remember there is still a lot of weird stuff in here. \n",
    "    filtered_token_text = [token.text for token in filtered_tokens]\n",
    "    return filtered_token_text\n",
    "\n",
    "doc_list = []\n",
    "\n",
    "#for i in range(0, len(policy_doc_df)):\n",
    "for i in range(0, 2): \n",
    "    doc_name = policy_doc_df.iloc[i]['policy_doc_name_clean'] \n",
    "    doc_path = policy_doc_df.iloc[i]['policy_doc_paths']\n",
    "    print(i, doc_name, doc_path)\n",
    "    filtered_token_text = apply_nlp_processing_framework(doc_name, doc_path)\n",
    "    doc_list.append(filtered_token_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a74bdb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T17:38:29.752712Z",
     "start_time": "2021-10-11T17:38:28.934492Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21116/3771635705.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m###using this resouce: https://towardsdatascience.com/building-a-topic-modeling-pipeline-with-spacy-and-gensim-c5dc03ffc619\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m###and this resource: https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpora\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msimple_preprocess\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "###using this resouce: https://towardsdatascience.com/building-a-topic-modeling-pipeline-with-spacy-and-gensim-c5dc03ffc619\n",
    "###and this resource: https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import LdaModel\n",
    "from pprint import pprint\n",
    "\n",
    "#just perform LDA for a few documents for now:\n",
    "print(doc_list[0][:15], doc_list[1][:15], doc_list[2][:15])\n",
    "\n",
    "# Creates, which is a mapping of word IDs to words.\n",
    "dictionary_LDA = corpora.Dictionary(doc_list)\n",
    "# Turns each document into a bag of words.\n",
    "corpus = [dictionary_LDA.doc2bow(list_of_tokens) for list_of_tokens in doc_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66238c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T17:38:30.055599Z",
     "start_time": "2021-10-11T17:38:29.753925Z"
    }
   },
   "outputs": [],
   "source": [
    "num_topics = 20\n",
    "%time lda_model = LdaModel(corpus, num_topics=num_topics, \\\n",
    "                                  id2word=dictionary_LDA, \\\n",
    "                                  passes=4, alpha=[0.01]*num_topics, \\\n",
    "                                  eta=[0.01]*len(dictionary_LDA.keys()))\n",
    "    \n",
    "pprint(lda_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1138d50b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "75547552c603f22400c6f6e0e4ad2ade15359435e000e6746c53241e37331409"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
