{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "704be11d",
   "metadata": {},
   "source": [
    "### GIZ Initial Data Exploration\n",
    "#### author: Emily Robitschek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ca671e",
   "metadata": {},
   "source": [
    "Purpose: Before we build any model, we need to be able to take a look at the documents we have.\n",
    "\n",
    "Some resources: \n",
    "\n",
    "#### papers mentioned in project proposal: \n",
    "https://medium.com/fiscalnoteworthy/citing-your-sources-79062248f468\n",
    "https://www2.deloitte.com/us/en/insights/focus/cognitive-technologies/natural-language-processing-examples-in-government-data.html\n",
    "https://documents1.worldbank.org/curated/en/634591516387264234/pdf/WPS8310.pdf\n",
    "\n",
    "#### NLP related links: \n",
    "- https://spacy.io/usage/spacy-101\n",
    "- https://towardsdatascience.com/text-pre-processing-stop-words-removal-using-different-libraries-f20bac19929a\n",
    "- https://arunm8489.medium.com/getting-started-with-natural-language-processing-6e593e349675\n",
    "- https://towardsdatascience.com/natural-language-processing-pipeline-decoded-f97a4da5dbb7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a581e19b",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2d30a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T20:54:04.371774Z",
     "start_time": "2021-10-06T20:54:02.746738Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#set up packages for NLP analysis\n",
    "import contractions\n",
    "import nltk\n",
    "#nltk.download('punkt') \n",
    "#nltk.download('stopwords') #had to also download this\n",
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm') #or the multi-language one: spacy.load('xx_ent_wiki_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334ff084",
   "metadata": {},
   "source": [
    "### Define helper functions for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbb2783",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T20:54:04.380097Z",
     "start_time": "2021-10-06T20:54:04.374000Z"
    }
   },
   "outputs": [],
   "source": [
    "#input data helper functions\n",
    "def list_docs(folder):\n",
    "    \"\"\"Generates a list of document names for reference and tracking. \n",
    "    This command currently extracts the .txt documents from all the subfolders of a parent folder, \n",
    "    and filters out the ones containing source information, which we might not want to use in our analysis.\"\"\"\n",
    "    doc_names = []\n",
    "    doc_paths = []\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt') and (file not in ['Source.txt', 'Source Link.txt', 'Source Links.txt']):\n",
    "                doc_names.append(file)\n",
    "                doc_paths.append(os.path.join(root, file))  \n",
    "    return doc_names, doc_paths\n",
    "\n",
    "\n",
    "#NLP related helper functions\n",
    "#used these resources: \n",
    "#https://www.geeksforgeeks.org/nlp-expand-contractions-in-text-processing/)\n",
    "#https://realpython.com/natural-language-processing-spacy-python/#how-to-download-models-and-data\n",
    "\n",
    "def fix_contractions(document): \n",
    "    \"\"\"Switches contractions like can't to cannot so potentially important words/pieces of words are \n",
    "    removed with punctuation removal.\"\"\"\n",
    "    # original contracted text\n",
    "    text = open(document).read()\n",
    "    \n",
    "    #creating an empty list\n",
    "    expanded_words = []    \n",
    "    for word in text.split():\n",
    "        #using contractions.fix to expand the shortened words \n",
    "        expanded_words.append(contractions.fix(word))   \n",
    "\n",
    "    expanded_text = ' '.join(expanded_words)\n",
    "    print('Original text: ', len(text))\n",
    "    print('Expanded_text: ', len(expanded_text))\n",
    "    return expanded_text\n",
    "\n",
    "def is_token_allowed(token):\n",
    "    '''\n",
    "        Only allow valid tokens which are not stop words\n",
    "        and punctuation symbols.\n",
    "    '''\n",
    "    if (not token or not token.text.strip() or\n",
    "        token.is_stop or token.is_punct):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def preprocess_token(token):\n",
    "    # Reduce token to its lowercase lemma form\n",
    "    return token.lemma_.strip().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e16b60",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6accc9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T20:54:04.385193Z",
     "start_time": "2021-10-06T20:54:04.381450Z"
    }
   },
   "outputs": [],
   "source": [
    "#keywords (just for testing purposes - from the policy proposal from GIZ)\n",
    "#eventually want to import a file with keywords\n",
    "policy = [\"policy\", \"integrate\", \"implement\", \"committee\", \"consultation\"]\n",
    "food = [\"nutritions\", \"diets\", \"farm\", \"agriculture\", \"ecology\"]\n",
    "ndc_national_adaption_plan = [\"nap\", \"sector plan\", \"nccrp\", \"vulnerable sector\", \n",
    "                              \"geographic vulnerability\"]\n",
    "ndc_climate_change = [\"adaption\", \"program\", \"projects\", \"resilience\", \"institution\",\n",
    "                      \"capacity\", \"response\", \"budget\", \"reprioritisation\", \"development\", \n",
    "                      \"planner\", \"regulator\", \"practitioners\", \"geographical\", \n",
    "                      \"circumstances\", \"land\", \"scheme\", \"authorisation\", \"system\", \n",
    "                      \"spluma\"]\n",
    "ndc_early_warning = [\"system\", \"vulnerability\", \"needs\", \"assessment\", \"network\", \"weather\",\n",
    "   \"earth\", \"observation\", \"academic\", \"community\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df47609",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T20:54:04.395070Z",
     "start_time": "2021-10-06T20:54:04.387339Z"
    }
   },
   "outputs": [],
   "source": [
    "#here is the general folder with the different types of policy documents\n",
    "policy_doc_folder = '/Users/emilyrobitschek/git/giz-policy_tracking_docs/SouthAfrica/Data' #ultimately need to remove this path\n",
    "policy_doc_names, policy_doc_paths = list_docs(policy_doc_folder)\n",
    "print((\"There are %d policy docs\" % (len(policy_doc_names))),\n",
    "      \"Some of the policy docs include: \", policy_doc_names[:10])\n",
    "\n",
    "#If want to preserve names and paths of the documents and make them easily searchable, \n",
    "#it might be useful to make a dictionary (this could also easily be converted to a dataframe \n",
    "#to add more summary information about the document for instance)\n",
    "\n",
    "policy_doc_dict = {'policy_doc_names': policy_doc_names, 'policy_doc_paths': policy_doc_paths}\n",
    "policy_doc_df = pd.DataFrame(data=policy_doc_dict, dtype='string')\n",
    "policy_doc_df.index = policy_doc_df['policy_doc_names']\n",
    "policy_doc_df.head()\n",
    "\n",
    "##lets take a look at the document from South Africa used in the example in the proposal first:\n",
    "test_doc_name = '2nd National Biodiversity Strategy Action Plan 2015-2025.pdf_ocr.txt'\n",
    "##lets try with another document\n",
    "#test_doc_name = 'InSessionSpecialEditionBudget2021.pdf.ocr.txt'\n",
    "test_doc_path = policy_doc_df.loc[test_doc_name]['policy_doc_paths']\n",
    "\n",
    "#can also simply specify the first or whichever document in the list by order: \n",
    "#(this will be helpful for future versions that involve more interrating over different documents)\n",
    "i=40\n",
    "test_doc_name = policy_doc_df.iloc[i]['policy_doc_names']\n",
    "print(test_doc_name)\n",
    "test_doc_path = policy_doc_df.iloc[i]['policy_doc_paths']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57d7cae",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdd64a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T20:54:11.538338Z",
     "start_time": "2021-10-06T20:54:04.396799Z"
    }
   },
   "outputs": [],
   "source": [
    "#remove contracted words and tokenize the document\n",
    "expanded_text_doc = nlp(fix_contractions(test_doc_path))\n",
    "\n",
    "#extract list of word tokens\n",
    "tokens = [token for token in expanded_text_doc]\n",
    "print('These are some of the unfiltered tokens: ', tokens[0:50], '\\n')\n",
    "\n",
    "#find sentences\n",
    "sentences = list(expanded_text_doc.sents)\n",
    "for sentence in sentences[:10]:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300b2e61",
   "metadata": {},
   "source": [
    "#### The token object: \n",
    "The tokens have all sorts of useful information association with them, for instance their positions (in token.idx) which we can use these later to define windows. See below for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a514aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T20:54:11.544958Z",
     "start_time": "2021-10-06T20:54:11.540721Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for token in expanded_text_doc[:20]:\n",
    "    print (token, token.idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89452ae3",
   "metadata": {},
   "source": [
    "We can see from above that the tokens need to be filtered and it might be useful if the words are all made lowercase and the words are lemmatized so the different forms of a word are recognized as the same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e2e270",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T20:54:15.154317Z",
     "start_time": "2021-10-06T20:54:11.547316Z"
    }
   },
   "outputs": [],
   "source": [
    "#filter tokens, and make lowercase and lemmatize (with preprocess function): \n",
    "filtered_text_list = [preprocess_token(token) for token in \n",
    "                   expanded_text_doc if is_token_allowed(token)]\n",
    "\n",
    "filtered_text = ' '.join(filtered_text_list)\n",
    "filtered_tokens = nlp(filtered_text)\n",
    "\n",
    "#complete_filtered_tokens\n",
    "print('These are some of the filtered tokens: ', filtered_tokens[200:300])\n",
    "\n",
    "#need to filter out super weird non words and may want to filter numbers\n",
    "#may want to find some important accronyms too"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddeddce",
   "metadata": {},
   "source": [
    "### Find most common (and unique) words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bb1aa3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T20:54:15.170844Z",
     "start_time": "2021-10-06T20:54:15.155508Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# Remove stop words and punctuation symbols\n",
    "words = [token.text for token in filtered_tokens]\n",
    "word_freq = Counter(words)\n",
    "# 5 commonly occurring words with their frequencies\n",
    "common_words = word_freq.most_common(30)\n",
    "print(common_words)\n",
    "\n",
    "# Unique words\n",
    "#unique_words = [word for (word, freq) in word_freq.items() if freq == 1]\n",
    "#print (unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431501fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T20:54:15.176026Z",
     "start_time": "2021-10-06T20:54:15.172148Z"
    }
   },
   "outputs": [],
   "source": [
    "word_freq['environmental']\n",
    "ndc_climate_change_scores = []\n",
    "for word in ndc_climate_change:\n",
    "    #print(word, word_freq[word])\n",
    "    ndc_climate_change_scores.append(word_freq[word])\n",
    "print(\"This document has the following number of words related to climate change NDCs: \",\n",
    "      sum(ndc_climate_change_scores), '\\n')\n",
    "\n",
    "food_scores = []\n",
    "for word in food:\n",
    "    #print(word, word_freq[word])\n",
    "    food_scores.append(word_freq[word])\n",
    "print(\"This document has the following number of words related to food: \",\n",
    "      sum(food_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada47c6b",
   "metadata": {},
   "source": [
    "### Make some plots of the NDC/Thematic key words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcf73db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T20:55:00.001303Z",
     "start_time": "2021-10-06T20:54:59.480261Z"
    }
   },
   "outputs": [],
   "source": [
    "#just to test - need to modularise/make more reproducible and tailor output to be useful across documents\n",
    "#could also link this back to the df of the documents to output a table with some summary metrics for keywords \n",
    "#in eaach document of interest\n",
    "\n",
    "graphs_folder = '/Users/emilyrobitschek/git/outputs/test_plots/bar_charts/'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "x = food\n",
    "y = food_scores\n",
    "x_pos = [i for i, _ in enumerate(x)]\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (2,4)\n",
    "plt.bar(x, y)\n",
    "plt.xlabel(\"Food Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "title = (\"Food words in: %s\" % (test_doc_name))\n",
    "plt.title(title)\n",
    "plt.xticks(x_pos, x, rotation=90)\n",
    "plt.savefig((graphs_folder + 'bar_chart_%s.pdf' % (title)), \n",
    "            bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "x = ndc_climate_change\n",
    "y = ndc_climate_change_scores\n",
    "x_pos = [i for i, _ in enumerate(x)]\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (8,4)\n",
    "plt.bar(x, y, color='green') \n",
    "plt.xlabel(\"NDC Climate Change Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "title = \"NDC Climate Change words in: %s\" % (test_doc_name)\n",
    "plt.title(title)\n",
    "plt.xticks(x_pos, x, rotation=90)\n",
    "plt.savefig((graphs_folder + 'bar_chart_%s.pdf' % (title)), \n",
    "            bbox_inches='tight')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c7511c",
   "metadata": {},
   "source": [
    "### Parts of speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b40fc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T20:54:15.969960Z",
     "start_time": "2021-10-06T20:54:15.869867Z"
    }
   },
   "outputs": [],
   "source": [
    "for token in filtered_tokens[:50]:\n",
    "    print(token, token.tag_, token.pos_, spacy.explain(token.tag_))\n",
    "print('\\n')\n",
    "    \n",
    "nouns = []\n",
    "adjectives = []\n",
    "for token in filtered_tokens:\n",
    "    if token.pos_ == 'NOUN':\n",
    "        nouns.append(token)\n",
    "    if token.pos_ == 'ADJ':\n",
    "        adjectives.append(token)\n",
    "\n",
    "print('some nouns spacy called in the document include:', '\\n', nouns[:50], '\\n')\n",
    "print('some adjectives spacy called in the document include:', '\\n', adjectives[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc72129",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773f4581",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
