{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "704be11d",
   "metadata": {},
   "source": [
    "### GIZ Initial Data Exploration\n",
    "#### author: Emily Robitschek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ca671e",
   "metadata": {},
   "source": [
    "Purpose: Before we build any model, we need to be able to take a look at the documents we have.\n",
    "\n",
    "Some resources: \n",
    "\n",
    "#### papers mentioned in project proposal: \n",
    "https://medium.com/fiscalnoteworthy/citing-your-sources-79062248f468\n",
    "https://www2.deloitte.com/us/en/insights/focus/cognitive-technologies/natural-language-processing-examples-in-government-data.html\n",
    "https://documents1.worldbank.org/curated/en/634591516387264234/pdf/WPS8310.pdf\n",
    "\n",
    "#### NLP related links: \n",
    "- https://spacy.io/usage/spacy-101\n",
    "- https://towardsdatascience.com/text-pre-processing-stop-words-removal-using-different-libraries-f20bac19929a\n",
    "- https://arunm8489.medium.com/getting-started-with-natural-language-processing-6e593e349675\n",
    "- https://towardsdatascience.com/natural-language-processing-pipeline-decoded-f97a4da5dbb7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a581e19b",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2d30a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T20:08:58.241525Z",
     "start_time": "2021-10-06T20:08:56.509958Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#set up packages for NLP analysis\n",
    "import nltk\n",
    "#nltk.download('punkt') \n",
    "#nltk.download('stopwords') #had to also download this\n",
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm') #or the multi-language one: spacy.load('xx_ent_wiki_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334ff084",
   "metadata": {},
   "source": [
    "### Define helper functions for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbb2783",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T20:08:59.026177Z",
     "start_time": "2021-10-06T20:08:59.021348Z"
    }
   },
   "outputs": [],
   "source": [
    "#input data helper functions\n",
    "def list_docs(folder):\n",
    "    \"\"\"Generates a list of document names for reference and tracking. \n",
    "    This command currently extracts the .txt documents from all the subfolders of a parent folder, \n",
    "    and filters out the ones containing source information, which we might not want to use in our analysis.\"\"\"\n",
    "    doc_names = []\n",
    "    doc_paths = []\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt') and (file not in ['Source.txt', 'Source Link.txt', 'Source Links.txt']):\n",
    "                doc_names.append(file)\n",
    "                doc_paths.append(os.path.join(root, file))  \n",
    "    return doc_names, doc_paths\n",
    "\n",
    "\n",
    "#NLP related helper functions\n",
    "#from this resource: https://realpython.com/natural-language-processing-spacy-python/#how-to-download-models-and-data\n",
    "def is_token_allowed(token):\n",
    "    '''\n",
    "        Only allow valid tokens which are not stop words\n",
    "        and punctuation symbols.\n",
    "    '''\n",
    "    if (not token or not token.text.strip() or\n",
    "        token.is_stop or token.is_punct):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def preprocess_token(token):\n",
    "    # Reduce token to its lowercase lemma form\n",
    "    return token.lemma_.strip().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e16b60",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6accc9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T20:09:02.028319Z",
     "start_time": "2021-10-06T20:09:02.023869Z"
    }
   },
   "outputs": [],
   "source": [
    "#keywords (just for testing purposes - from the policy proposal from GIZ)\n",
    "#eventually want to import a file with keywords\n",
    "policy = [\"policy\", \"integrate\", \"implement\", \"committee\", \"consultation\"]\n",
    "food = [\"nutritions\", \"diets\", \"farm\", \"agriculture\", \"ecology\"]\n",
    "ndc_national_adaption_plan = [\"nap\", \"sector plan\", \"nccrp\", \"vulnerable sector\", \n",
    "                              \"geographic vulnerability\"]\n",
    "ndc_climate_change = [\"adaption\", \"program\", \"projects\", \"resilience\", \"institution\",\n",
    "                      \"capacity\", \"response\", \"budget\", \"reprioritisation\", \"development\", \n",
    "                      \"planner\", \"regulator\", \"practitioners\", \"geographical\", \n",
    "                      \"circumstances\", \"land\", \"scheme\", \"authorisation\", \"system\", \n",
    "                      \"spluma\"]\n",
    "ndc_early_warning = [\"system\", \"vulnerability\", \"needs\", \"assessment\", \"network\", \"weather\",\n",
    "   \"earth\", \"observation\", \"academic\", \"community\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd783fea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T20:12:01.574936Z",
     "start_time": "2021-10-06T20:12:01.567366Z"
    }
   },
   "outputs": [],
   "source": [
    "cd ../\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df47609",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T20:13:12.201156Z",
     "start_time": "2021-10-06T20:13:12.193018Z"
    }
   },
   "outputs": [],
   "source": [
    "#here is the general folder with the different types of policy documents\n",
    "policy_doc_folder = '/Users/emilyrobitschek/git/giz-policy/giz-policy_tracking_docs/SouthAfrica/Data'\n",
    "policy_doc_names, policy_doc_paths = list_docs(policy_doc_folder)\n",
    "print(\"Some of the policy docs include: \", policy_doc_names[:10])\n",
    "\n",
    "#If want to preserve names and paths of the documents and make them easily searchable, \n",
    "#it might be useful to make a dictionary (this could also easily be converted to a dataframe \n",
    "#to add more summary information about the document for instance)\n",
    "\n",
    "policy_doc_dict = {'policy_doc_names': policy_doc_names, 'policy_doc_paths': policy_doc_paths}\n",
    "policy_doc_df = pd.DataFrame(data=policy_doc_dict, dtype='string')\n",
    "policy_doc_df.index = policy_doc_df['policy_doc_names']\n",
    "policy_doc_df.head()\n",
    "\n",
    "##lets take a look at the document from South Africa used in the example in the proposal first:\n",
    "test_doc_name = '2nd National Biodiversity Strategy Action Plan 2015-2025.pdf_ocr.txt'\n",
    "test_doc_path = policy_doc_df.loc[test_doc_name]['policy_doc_paths']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57d7cae",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdd64a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T20:13:22.513055Z",
     "start_time": "2021-10-06T20:13:14.738215Z"
    }
   },
   "outputs": [],
   "source": [
    "### expand contractions (from this resource: https://www.geeksforgeeks.org/nlp-expand-contractions-in-text-processing/)\n",
    "# import library\n",
    "import contractions\n",
    "# contracted text\n",
    "text = open(test_doc_path).read()\n",
    "\n",
    "#creating an empty list\n",
    "expanded_words = []    \n",
    "for word in text.split():\n",
    "    #using contractions.fix to expand the shortened words \n",
    "    expanded_words.append(contractions.fix(word))   \n",
    "    \n",
    "expanded_text = ' '.join(expanded_words)\n",
    "print('Original text: ', len(text))\n",
    "print('Expanded_text: ', len(expanded_text))\n",
    "\n",
    "expanded_text_doc = nlp(expanded_text)\n",
    "sentences = list(expanded_text_doc.sents)\n",
    "len(sentences)\n",
    "\n",
    "for sentence in sentences[:20]:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a514aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T20:13:22.518887Z",
     "start_time": "2021-10-06T20:13:22.514979Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract tokens for the given doc (and their positions)\n",
    "for token in expanded_text_doc[:50]:\n",
    "    print (token, token.idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e2e270",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T20:13:25.996855Z",
     "start_time": "2021-10-06T20:13:22.520008Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens = [token for token in expanded_text_doc]\n",
    "print('These are some of the unfiltered tokens: ', tokens[200:300], '\\n')\n",
    "\n",
    "#filter tokens, and make lowercase and lemmatize (with preprocess function): \n",
    "filtered_text_list = [preprocess_token(token) for token in \n",
    "                   expanded_text_doc if is_token_allowed(token)]\n",
    "\n",
    "filtered_text = ' '.join(filtered_text_list)\n",
    "filtered_tokens = nlp(filtered_text)\n",
    "\n",
    "#complete_filtered_tokens\n",
    "print('These are some of the filtered tokens: ', filtered_tokens[200:300])\n",
    "\n",
    "#need to filter out super weird non words and may want to filter numbers\n",
    "#may want to find some important accronyms too"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddeddce",
   "metadata": {},
   "source": [
    "### Find most common (and unique) words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bb1aa3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T20:13:26.014231Z",
     "start_time": "2021-10-06T20:13:25.999152Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# Remove stop words and punctuation symbols\n",
    "words = [token.text for token in filtered_tokens]\n",
    "word_freq = Counter(words)\n",
    "# 5 commonly occurring words with their frequencies\n",
    "common_words = word_freq.most_common(30)\n",
    "print(common_words)\n",
    "\n",
    "# Unique words\n",
    "#unique_words = [word for (word, freq) in word_freq.items() if freq == 1]\n",
    "#print (unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431501fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T20:13:26.019418Z",
     "start_time": "2021-10-06T20:13:26.015990Z"
    }
   },
   "outputs": [],
   "source": [
    "word_freq['environmental']\n",
    "ndc_climate_change_scores = []\n",
    "for word in ndc_climate_change:\n",
    "    #print(word, word_freq[word])\n",
    "    ndc_climate_change_scores.append(word_freq[word])\n",
    "print(\"This document has the following number of words related to climate change NDCs: \",\n",
    "      sum(ndc_climate_change_scores), '\\n')\n",
    "\n",
    "food_scores = []\n",
    "for word in food:\n",
    "    #print(word, word_freq[word])\n",
    "    food_scores.append(word_freq[word])\n",
    "print(\"This document has the following number of words related to food: \",\n",
    "      sum(food_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada47c6b",
   "metadata": {},
   "source": [
    "### Make some plots of the NDC/Thematic key words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcf73db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T20:13:26.481534Z",
     "start_time": "2021-10-06T20:13:26.020737Z"
    }
   },
   "outputs": [],
   "source": [
    "#just to test - need to modularise/make more reproducible and tailor output to be useful across documents\n",
    "#could also link this back to the df of the documents to output a table with some summary metrics for keywords \n",
    "#in eaach document of interest\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "#plt.style.use('ggplot')\n",
    "\n",
    "x = food\n",
    "y = food_scores\n",
    "x_pos = [i for i, _ in enumerate(x)]\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (2,4)\n",
    "plt.bar(x, y)\n",
    "plt.xlabel(\"Food Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Food words in the documents\")\n",
    "plt.xticks(x_pos, x, rotation=90)\n",
    "plt.show()\n",
    "\n",
    "x = ndc_climate_change\n",
    "y = ndc_climate_change_scores\n",
    "x_pos = [i for i, _ in enumerate(x)]\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (8,4)\n",
    "plt.bar(x, y, color='green') \n",
    "plt.xlabel(\"NDC Climate Change Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"NDC Climate Change words in the document\")\n",
    "plt.xticks(x_pos, x, rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c7511c",
   "metadata": {},
   "source": [
    "### Parts of speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b40fc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T20:13:58.735270Z",
     "start_time": "2021-10-06T20:13:58.630021Z"
    }
   },
   "outputs": [],
   "source": [
    "for token in filtered_tokens[:50]:\n",
    "    print(token, token.tag_, token.pos_, spacy.explain(token.tag_))\n",
    "print('\\n')\n",
    "    \n",
    "nouns = []\n",
    "adjectives = []\n",
    "for token in filtered_tokens:\n",
    "    if token.pos_ == 'NOUN':\n",
    "        nouns.append(token)\n",
    "    if token.pos_ == 'ADJ':\n",
    "        adjectives.append(token)\n",
    "\n",
    "print('some nouns spacy called in the document include:', '\\n', nouns[:50], '\\n')\n",
    "print('some adjectives spacy called in the document include:', '\\n', adjectives[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc72129",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773f4581",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
