{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "704be11d",
   "metadata": {},
   "source": [
    "### GIZ Initial Data Exploration\n",
    "#### author: Emily Robitschek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ca671e",
   "metadata": {},
   "source": [
    "Purpose: Before we build any model, we need to be able to take a look at the documents we have.\n",
    "\n",
    "Some resources: \n",
    "\n",
    "#### papers mentioned in project proposal: \n",
    "https://medium.com/fiscalnoteworthy/citing-your-sources-79062248f468\n",
    "https://www2.deloitte.com/us/en/insights/focus/cognitive-technologies/natural-language-processing-examples-in-government-data.html\n",
    "https://documents1.worldbank.org/curated/en/634591516387264234/pdf/WPS8310.pdf\n",
    "\n",
    "#### NLP related links: \n",
    "- https://spacy.io/usage/spacy-101\n",
    "- https://towardsdatascience.com/text-pre-processing-stop-words-removal-using-different-libraries-f20bac19929a\n",
    "- https://arunm8489.medium.com/getting-started-with-natural-language-processing-6e593e349675\n",
    "- https://towardsdatascience.com/natural-language-processing-pipeline-decoded-f97a4da5dbb7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a581e19b",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2d30a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T10:26:01.420909Z",
     "start_time": "2021-10-27T10:25:58.718554Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import json\n",
    "import codecs\n",
    "\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#set up packages for processing data types and for NLP analysis\n",
    "from collections import OrderedDict, Counter\n",
    "import contractions\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_sm') #or the multi-language one: spacy.load('xx_ent_wiki_sm')\n",
    "\n",
    "#graphing/visualization packages: \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334ff084",
   "metadata": {},
   "source": [
    "### Define helper functions for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbb2783",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T10:26:01.817456Z",
     "start_time": "2021-10-27T10:26:01.422656Z"
    }
   },
   "outputs": [],
   "source": [
    "from datahelper import *\n",
    "from nlppreprocess import *\n",
    "from nlpanalysis import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e16b60",
   "metadata": {},
   "source": [
    "### Import data: Keywords from NDCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6accc9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T10:26:02.732674Z",
     "start_time": "2021-10-27T10:26:02.688748Z"
    }
   },
   "outputs": [],
   "source": [
    "#keywords (just for testing purposes - from the policy proposal from GIZ)\n",
    "policy = [\"policy\", \"integrate\", \"implement\", \"committee\", \"consultation\"]\n",
    "food = [\"nutritions\", \"diets\", \"farm\", \"agriculture\", \"ecology\"]\n",
    "ndc_national_adaption_plan = [\"nap\", \"sector plan\", \"nccrp\", \"vulnerable sector\", \n",
    "                              \"geographic vulnerability\"]\n",
    "ndc_climate_change = [\"adaption\", \"program\", \"projects\", \"resilience\", \"institution\",\n",
    "                      \"capacity\", \"response\", \"budget\", \"reprioritisation\", \"development\", \n",
    "                      \"planner\", \"regulator\", \"practitioners\", \"geographical\", \n",
    "                      \"circumstances\", \"land\", \"scheme\", \"authorisation\", \"system\", \n",
    "                      \"spluma\"]\n",
    "ndc_early_warning = [\"system\", \"vulnerability\", \"needs\", \"assessment\", \"network\", \"weather\",\n",
    "   \"earth\", \"observation\", \"academic\", \"community\"]\n",
    "\n",
    "#keywords from json file\n",
    "json_keywords_SA_file = '../ndc_keywords/ndc_south_africa.json'\n",
    "keywords_SA_dict = None\n",
    "with open(json_keywords_SA_file, 'r') as f: \n",
    "    keywords_SA_dict = json.load(f)\n",
    "keywords_SA_dict\n",
    "\n",
    "ndc_dict = make_filtered_tokens_from_ndc(keywords_SA_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e306e11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T10:26:04.587149Z",
     "start_time": "2021-10-27T10:26:04.581875Z"
    }
   },
   "outputs": [],
   "source": [
    "### THIS IS A PLACEHOLDER ###\n",
    "#get words directly from NDCs (eventually want to get words from the NDC itself and group them by topics.)\n",
    "#\n",
    "#\n",
    "#\n",
    "ndc_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f87631",
   "metadata": {},
   "source": [
    "### Import data: Policy-related documents to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df47609",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T10:26:07.514218Z",
     "start_time": "2021-10-27T10:26:07.502592Z"
    }
   },
   "outputs": [],
   "source": [
    "#here is the general folder with the different types of policy documents\n",
    "policy_doc_folder = '../test_resources'\n",
    "#get df of docs\n",
    "policy_doc_df = read_docs_to_df(policy_doc_folder)\n",
    "policy_doc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57d7cae",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdd64a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T10:26:21.127697Z",
     "start_time": "2021-10-27T10:26:13.265168Z"
    }
   },
   "outputs": [],
   "source": [
    "##lets take a look at the document from South Africa used in the example in the proposal first:\n",
    "doc_name = '2nd National Biodiversity Strategy Action Plan 2015-2025.pdf_ocr.txt'\n",
    "doc_path = policy_doc_df.loc[doc_name]['policy_doc_paths']\n",
    "\n",
    "#can also simply specify the first or whichever document in the list by order: \n",
    "#(this will be helpful for future versions that involve more interrating over different documents)\n",
    "#i=53\n",
    "#doc_name = policy_doc_df.iloc[i]['policy_doc_name_clean']\n",
    "#doc_path = policy_doc_df.iloc[i]['policy_doc_paths']\n",
    "print(doc_name, doc_path)\n",
    "tokens, token_list, sentences = preprocess_doc(doc_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300b2e61",
   "metadata": {},
   "source": [
    "#### The token object: \n",
    "The tokens have all sorts of useful information association with them, for instance their positions (in token.idx) which we can use these later to define windows. See below for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a514aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T10:26:25.407419Z",
     "start_time": "2021-10-27T10:26:25.403058Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for token in token_list[:20]:\n",
    "    print (token, token.idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89452ae3",
   "metadata": {},
   "source": [
    "We can see from above that the tokens need to be filtered and it might be useful if the words are all made lowercase and the words are lemmatized so the different forms of a word are recognized as the same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e2e270",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T10:26:30.917661Z",
     "start_time": "2021-10-27T10:26:27.336632Z"
    }
   },
   "outputs": [],
   "source": [
    "filtered_tokens = filter_modify_tokens(tokens)\n",
    "print('These are some of the filtered tokens: ', filtered_tokens[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddeddce",
   "metadata": {},
   "source": [
    "### Find most common (and unique) words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bb1aa3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T10:26:30.937427Z",
     "start_time": "2021-10-27T10:26:30.919624Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove stop words and punctuation symbols\n",
    "words = [token.text for token in filtered_tokens]\n",
    "word_freq = Counter(words)\n",
    "# 5 commonly occurring words with their frequencies\n",
    "common_words = word_freq.most_common(30)\n",
    "print(common_words)\n",
    "\n",
    "# Unique words\n",
    "#unique_words = [word for (word, freq) in word_freq.items() if freq == 1]\n",
    "#print (unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada47c6b",
   "metadata": {},
   "source": [
    "### Make some plots of the NDC/Thematic key words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f0ef52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T10:26:35.488760Z",
     "start_time": "2021-10-27T10:26:34.932822Z"
    }
   },
   "outputs": [],
   "source": [
    "#just to test - need to modularise/make more reproducible and tailor output to be useful across documents\n",
    "#could also link this back to the df of the documents to output a table with some summary metrics for keywords \n",
    "#in each document of interest\n",
    "\n",
    "graphs_folder = '../test_resources/bar_plots/'    \n",
    "    \n",
    "for key in ndc_dict.keys(): \n",
    "    print(\"Graphing the occurences of %s words in the document\" % key)\n",
    "    topic_frequencies =  calculate_topic_frequency_subset(word_freq, ndc_dict, str(key))\n",
    "    plot_word_freq_barchart_ndc(topic_frequencies, str(key), doc_name, graphs_folder) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900e8fb0",
   "metadata": {},
   "source": [
    "### Lets take a closer look at the climate change NDC keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462068c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T10:26:38.694642Z",
     "start_time": "2021-10-27T10:26:38.485892Z"
    }
   },
   "outputs": [],
   "source": [
    "for key in ndc_dict.keys(): \n",
    "    if key == 'climate change':\n",
    "        print(\"Graphing the occurences of %s words in the document\" % key)\n",
    "        topic_frequencies = calculate_topic_frequency_subset(word_freq, ndc_dict, str(key))\n",
    "        plot_word_freq_barchart_ndc(topic_frequencies, str(key), doc_name, graphs_folder) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0acbd5",
   "metadata": {},
   "source": [
    "### Where do these words appear in the document?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cc736f",
   "metadata": {},
   "source": [
    "#### Make dataframe for easy graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d067e20e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T11:09:58.891722Z",
     "start_time": "2021-10-27T11:09:58.790703Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_ndc_keyword_tidy_df_from_dict(key, col_group_name, ndc_dict):\n",
    "    ndc_df = pd.DataFrame({'keyword': ndc_dict[key], \n",
    "                       col_group_name: key})\n",
    "    return ndc_df\n",
    "\n",
    "def stack_tidy_ndc_dfs(key, col_group_name, ndc_dict):\n",
    "    ndc_df = pd.DataFrame()\n",
    "    for key in ndc_dict.keys():\n",
    "        ndc_df_add = make_ndc_keyword_tidy_df_from_dict(key, col_group_name, ndc_dict)\n",
    "        ndc_df = pd.concat([ndc_df, ndc_df_add], axis=0)\n",
    "    return ndc_df\n",
    "\n",
    "# lets apply: \n",
    "col_group_name = 'NDC'\n",
    "ndc_df = stack_tidy_ndc_dfs(key, col_group_name, ndc_dict)\n",
    "ndc_df.head()\n",
    "\n",
    "\n",
    "def make_ndc_idx_tidy_df(ndc_dict, topic_name, tokens):\n",
    "    ndc_idx_df = pd.DataFrame()\n",
    "    for key in ndc_dict.keys(): \n",
    "        ndc_idx_df_to_add = pd.DataFrame({topic_name: key,\n",
    "                                          #('%s word_index'%(key)): [token.idx for token in tokens if token.text in ndc_dict[key]],\n",
    "                                          'word_index': [token.idx for token in tokens if token.text in ndc_dict[key]]})\n",
    "        ndc_idx_df = pd.concat([ndc_idx_df, ndc_idx_df_to_add], axis=0)\n",
    "    return ndc_idx_df\n",
    "\n",
    "topic_name='NDC'\n",
    "ndc_idx_df = make_ndc_idx_tidy_df(ndc_dict, topic_name, tokens)\n",
    "ndc_idx_df.index = ndc_idx_df.NDC.copy()\n",
    "ndc_idx_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcf73db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T11:33:34.496794Z",
     "start_time": "2021-10-27T11:11:11.258603Z"
    }
   },
   "outputs": [],
   "source": [
    "### Graph with jointplot to see relationships based on the idx\n",
    "ax = sns.displot(x=ndc_idx_df[\"word_index\"], hue=ndc_idx_df[\"NDC\"], kind=\"kde\")\n",
    "#ax = sns.displot(x=ndc_early_warning_idxs, kind=\"kde\", bw_adjust=0.01)\n",
    "plt.xlabel(\"Location of NDC words in the document\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "title = (\"Distribution of NDC word locations in the document\")\n",
    "plt.title(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a91bd9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T11:04:10.430900Z",
     "start_time": "2021-10-27T11:04:10.367424Z"
    }
   },
   "outputs": [],
   "source": [
    "#try with two different variables: \n",
    "ndc_climate_idxs = [token.idx for token in tokens if token.text in ndc_dict['climate change']]\n",
    "ndc_early_warning_idxs = [token.idx for token in tokens if token.text in ndc_dict['early warning']]\n",
    "#ndc_climate_idxs\n",
    "for key in ndc_dict.keys():\n",
    "    print()\n",
    "ndc_dict['early warning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56500d72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T17:59:36.894522Z",
     "start_time": "2021-11-03T17:59:36.889784Z"
    }
   },
   "outputs": [],
   "source": [
    "ndc_climate_idxs\n",
    "\n",
    "def filter_idx_for_overlap(idxs, min_dist):\n",
    "    distance_btwn_idxs = [(idxs[i+1]-idxs[i]) for i in range(0, len(idxs)-1)]\n",
    "    print(distance_btwn_idxs[:20])\n",
    "    filtered_idxs = []\n",
    "    for index, distance in enumerate(distance_btwn_idxs):\n",
    "        if (distance >= min_dist):\n",
    "            filtered_idxs.append(idxs[index])\n",
    "        else:\n",
    "            pass\n",
    "    print(\"The number of times the idx words were found was: \", len(idxs), \"\\n\", \n",
    "          \"The number of idx words seperated by at least the min_distance was : \", len(filtered_idxs))\n",
    "    return filtered_idxs\n",
    "    \n",
    "idx_for_window = filter_idx_for_overlap(idxs=ndc_climate_idxs, min_dist=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83d6b37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T17:59:40.784469Z",
     "start_time": "2021-11-03T17:59:40.103012Z"
    }
   },
   "outputs": [],
   "source": [
    "#lets make a histogram of these to see where most of them fall in the document: \n",
    "## Plot distribution of where NDC words fall in the document\n",
    "print(len(ndc_climate_idxs))\n",
    "plt.rcParams[\"figure.figsize\"] = (20,4)\n",
    "plt.hist(x=ndc_climate_idxs, bins=200)\n",
    "plt.hist(x=idx_for_window, bins=200)\n",
    "#plt.hist(x=ndc_early_warning_idxs, bins=(round(len(ndc_early_warning_idxs))))\n",
    "plt.xlabel(\"Location of NDC words in the document\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "title = (\"Distribution of NDC word locations in the document\")\n",
    "plt.title(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af349eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T17:59:42.999776Z",
     "start_time": "2021-11-03T17:59:42.699869Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.displot(x=ndc_climate_idxs, kind=\"kde\", bw_adjust=0.1)\n",
    "ax = sns.displot(x=ndc_early_warning_idxs, kind=\"kde\", bw_adjust=0.1)\n",
    "plt.xlabel(\"Location of NDC words in the document\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "title = (\"Distribution of NDC word locations in the document\")\n",
    "plt.title(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acc810a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T10:20:29.197691Z",
     "start_time": "2021-11-03T10:20:28.858991Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,4)\n",
    "sns.displot(x=ndc_climate_idxs, kde=True, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98199873",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T17:58:45.351058Z",
     "start_time": "2021-11-03T17:58:38.288685Z"
    }
   },
   "outputs": [],
   "source": [
    "#lets filter out the long words: \n",
    "max_length=30\n",
    "def make_window_text(tokens, max_length):\n",
    "    filtered_for_length = [token.text.lower() for token in tokens if len(token) < max_length]\n",
    "    text_for_windows = ' '.join(filtered_for_length)\n",
    "    window_tokens = nlp(text_for_windows)\n",
    "    return window_tokens\n",
    "\n",
    "def return_window(ndc_word_index, tokens, size=50):\n",
    "    lower_limit = ndc_word_index - size\n",
    "    upper_limit = ndc_word_index + size\n",
    "    #print('The window is ', lower_limit, upper_limit)\n",
    "    return lower_limit, upper_limit, tokens[lower_limit:upper_limit]\n",
    "    #need to modify to return non-overlapping windows of text (maybe those with the most ndc keywords?)\n",
    "    \n",
    "window_tokens_overall = make_window_text(tokens, max_length=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86633073",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T18:00:06.298963Z",
     "start_time": "2021-11-03T17:59:54.413719Z"
    }
   },
   "outputs": [],
   "source": [
    "window_starts = []\n",
    "window_ends = []\n",
    "for index in idx_for_window: #ndc_climate_idxs:\n",
    "    lower_limit = return_window(index, tokens)[0]\n",
    "    upper_limit = return_window(index, tokens)[1]\n",
    "    window_starts.append(lower_limit)\n",
    "    window_ends.append(upper_limit)\n",
    "    print(lower_limit, upper_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b08a92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T11:57:46.578989Z",
     "start_time": "2021-11-03T11:57:46.559415Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1543ab5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T18:00:37.454406Z",
     "start_time": "2021-11-03T18:00:37.447697Z"
    }
   },
   "outputs": [],
   "source": [
    "#for index in ndc_climate_idxs[:10]:\n",
    "#    return_window(index, tokens)\n",
    "\n",
    "for index in idx_for_window[:40]:#ndc_climate_idxs[30:40]:\n",
    "    window_tokens = return_window(index, window_tokens_overall)\n",
    "    print(window_tokens[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7eaf80",
   "metadata": {},
   "source": [
    "### Using other resources:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b956e9",
   "metadata": {},
   "source": [
    "### NDC Ontology with SDG classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3ab981",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T11:37:10.654168Z",
     "start_time": "2021-11-03T11:37:10.642025Z"
    }
   },
   "outputs": [],
   "source": [
    "ndc_ontology = pd.read_csv('../additional_resources/Ontology_final.csv', sep=';')#, #skiprows=0)\n",
    "SDG1_keywords = list(ndc_ontology[ndc_ontology['clasification']=='SDG1']['keyword'])\n",
    "#print(SDG1_keywords)\n",
    "ndc_ontology.head()\n",
    "#print(list(ndc_ontology[ndc_ontology['clasification']=='SDG3']['keyword']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627eeb59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T19:28:34.180640Z",
     "start_time": "2021-11-03T19:28:31.964129Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "document_text = ' '.join([token.text for token in window_tokens_overall])\n",
    "patterns = [\"such as\", \"ecological\"]\n",
    "patterns = SDG1_keywords\n",
    "\n",
    "def find_patterns_df(pattern_list, text, topic_name):\n",
    "    pattern_locations = []\n",
    "    pattern_num = []\n",
    "    for pattern in pattern_list:\n",
    "        #print(pattern)\n",
    "        re.findall(pattern, text, flags=0)\n",
    "        #pattern_locations = [(m.start(0), m.end(0)) for m in re.finditer(pattern, text)] #if want start and end\n",
    "        locations = [m.start(0) for m in re.finditer(pattern, text)]\n",
    "        pattern_locations.append(locations)\n",
    "        pattern_num.append(len(locations))\n",
    "    #print(pattern_locations)\n",
    "        #if len(pattern_locations) > 0: \n",
    "        #    print(pattern, len(pattern_locations), pattern_locations)\n",
    "    return pd.DataFrame({'sdg_topic': sdg,\n",
    "                         'sdg_keywords': pattern_list,\n",
    "                         'sdg_keywords_num': pattern_num,\n",
    "                         'sdg_keyword_locations': pattern_locations})\n",
    "#look at SDGs across document/at document level\n",
    "sdg1_df = find_patterns_df(patterns, document_text, 'SDG1')\n",
    "sdg1_df[sdg1_df['sdg_keywords_num']>0] \n",
    "\n",
    "sdg_list = ['SDG1', 'SDG2', 'SDG3', 'SDG4', 'SDG5', 'SDG6', 'SDG7', 'SDG8', 'SDG9', \n",
    "            'SDG10', 'SDG11', 'SDG12', 'SDG13', 'SDG14', 'SDG15', 'SDG16', 'SDG17']\n",
    "\n",
    "df_sdg = pd.DataFrame()\n",
    "for sdg in list(sdg_list):\n",
    "    #print(sdg)\n",
    "    sdg_keywords = list(ndc_ontology[ndc_ontology['clasification']==sdg]['keyword'])\n",
    "    df_sdg_to_add = find_patterns_df(sdg_keywords, document_text, topic_name=sdg)\n",
    "    df_sdg = pd.concat([df_sdg, df_sdg_to_add])\n",
    "df_sdg[df_sdg['sdg_keywords_num']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1025ee95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T19:28:49.594434Z",
     "start_time": "2021-11-03T19:28:49.590946Z"
    }
   },
   "outputs": [],
   "source": [
    "#print a summary of the SDG words found: \n",
    "summary_sdg = df_sdg.groupby('sdg_topic')['sdg_keywords_num'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332f8af0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T18:01:54.904171Z",
     "start_time": "2021-11-03T18:01:52.581602Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690c4744",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T11:22:48.130308Z",
     "start_time": "2021-11-03T11:22:48.121846Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80c7511c",
   "metadata": {},
   "source": [
    "### Parts of speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b40fc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-26T17:23:14.651971Z",
     "start_time": "2021-10-26T17:23:14.634675Z"
    }
   },
   "outputs": [],
   "source": [
    "for token in filtered_tokens[:50]:\n",
    "    print(token, token.tag_, token.pos_, spacy.explain(token.tag_))\n",
    "print('\\n')\n",
    "    \n",
    "nouns = []\n",
    "adjectives = []\n",
    "for token in filtered_tokens:\n",
    "    if token.pos_ == 'NOUN':\n",
    "        nouns.append(token)\n",
    "    if token.pos_ == 'ADJ':\n",
    "        adjectives.append(token)\n",
    "\n",
    "print('some nouns spacy called in the document include:', '\\n', nouns[:50], '\\n')\n",
    "print('some adjectives spacy called in the document include:', '\\n', adjectives[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b519308c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T11:56:38.400728Z",
     "start_time": "2021-10-09T11:56:38.397730Z"
    }
   },
   "source": [
    "### Entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773f4581",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-26T17:23:14.744820Z",
     "start_time": "2021-10-26T17:23:14.654869Z"
    }
   },
   "outputs": [],
   "source": [
    "#can display entity property for the tokens as well: \n",
    "entities=[(i, i.label_, i.label) for i in filtered_tokens[1400:1700].ents]\n",
    "print(entities[:10])\n",
    "\n",
    "token_subset = tokens[100:500]\n",
    "displacy.render(token_subset, style = \"ent\", jupyter = True) #use original tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d245d4",
   "metadata": {},
   "source": [
    "### Dependency visualization in sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24796c38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-26T17:23:14.792742Z",
     "start_time": "2021-10-26T17:23:14.746941Z"
    }
   },
   "outputs": [],
   "source": [
    "sent_example = nlp(\"For example, it is estimated that between 9 and 12 million DATE people in impoverished rural areas directly use natural resources such as fuel wood, wild fruits and wooden utensils as a source of energy, food and building material respectively (Shackleton ORG 2004)\")\n",
    "sentence_spans = list(sentences)\n",
    "sentence_spans[:10]\n",
    "displacy.render(sentence_spans[80], style=\"dep\", jupyter= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c2f189",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b293e591",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14be276a",
   "metadata": {},
   "source": [
    "## Sandbox (extra code to be deleted if not of use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dc9dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "a_string = \"one two three\"\n",
    "word_list = a_string.split()\n",
    "number_of_words = len(word_list)\n",
    "print(number_of_words)\n",
    "pattern = \"ecological\"\n",
    "window_tokens\n",
    "print([(token.text, token.idx) for token in window_tokens if token.text == \"such\"][:])\n",
    "print([(token.text, token.idx) for token in window_tokens if token.text == \"as\"][:])\n",
    "#token.idx==\n",
    "\n",
    "#find first word\n",
    "patterns = [\"ecological\", \"such as\"]\n",
    "pattern = [\"ecological\"]\n",
    "window_tokens\n",
    "\n",
    "for pattern in patterns:\n",
    "    for token in window_tokens: \n",
    "        if token.text == pattern:\n",
    "            print(token.idx)\n",
    "#check if subsequent word matches\n",
    "\n",
    "def match_words_return_idx(token_list, pattern_list):\n",
    "    for pattern in pattern_list:\n",
    "        pattern_idxs = []\n",
    "        word_list = pattern.split()\n",
    "        number_of_words = len(word_list)\n",
    "        if number_of_words == 1: \n",
    "            #return list of matches to the single word (pattern)\n",
    "            pattern_idxs = [token.idx for token in window_tokens if token.text == pattern]\n",
    "            print(pattern, pattern_idxs)\n",
    "        if number_of_words > 1: \n",
    "            #return list of matches to the first word in the pattern\n",
    "            pattern_idxs = [token.idx for token in window_tokens if token.text == word_list[0]] \n",
    "            print(pattern, pattern_idxs) #print these matches\n",
    "            #check the following token to see if it matches the next word \n",
    "            for idx in pattern_idxs:\n",
    "                #print([token.text for token in window_tokens if token.idx==(idx+1)])\n",
    "            print(pattern)\n",
    "        \n",
    "        \n",
    "match_words_return_idx(window_tokens, patterns)\n",
    "list(pattern)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "75547552c603f22400c6f6e0e4ad2ade15359435e000e6746c53241e37331409"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
