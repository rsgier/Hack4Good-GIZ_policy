{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "704be11d",
   "metadata": {},
   "source": [
    "### GIZ Initial Data Exploration\n",
    "#### author: Emily Robitschek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ca671e",
   "metadata": {},
   "source": [
    "Purpose: Before we build any model, we need to be able to take a look at the documents we have.\n",
    "\n",
    "Some resources: \n",
    "\n",
    "#### papers mentioned in project proposal: \n",
    "https://medium.com/fiscalnoteworthy/citing-your-sources-79062248f468\n",
    "https://www2.deloitte.com/us/en/insights/focus/cognitive-technologies/natural-language-processing-examples-in-government-data.html\n",
    "https://documents1.worldbank.org/curated/en/634591516387264234/pdf/WPS8310.pdf\n",
    "\n",
    "#### NLP related links: \n",
    "- https://spacy.io/usage/spacy-101\n",
    "- https://towardsdatascience.com/text-pre-processing-stop-words-removal-using-different-libraries-f20bac19929a\n",
    "- https://arunm8489.medium.com/getting-started-with-natural-language-processing-6e593e349675\n",
    "- https://towardsdatascience.com/natural-language-processing-pipeline-decoded-f97a4da5dbb7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a581e19b",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2d30a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:27.718385Z",
     "start_time": "2021-11-04T22:28:25.061763Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import json\n",
    "import codecs\n",
    "import re\n",
    "\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#set up packages for processing data types and for NLP analysis\n",
    "from collections import OrderedDict, Counter\n",
    "import contractions\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm') #or the multi-language one: spacy.load('xx_ent_wiki_sm')\n",
    "\n",
    "from n_gram_correlation import NGramCorrelateSpacy\n",
    "\n",
    "#graphing/visualization packages: \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334ff084",
   "metadata": {},
   "source": [
    "### Define helper functions for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbb2783",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:28.114906Z",
     "start_time": "2021-11-04T22:28:27.720531Z"
    }
   },
   "outputs": [],
   "source": [
    "from datahelper import *\n",
    "from nlppreprocess import *\n",
    "from nlpanalysis import *\n",
    "from correlation import *\n",
    "from textutils import importer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e16b60",
   "metadata": {},
   "source": [
    "### Import data: Keywords from NDCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6accc9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:28.158053Z",
     "start_time": "2021-11-04T22:28:28.116076Z"
    }
   },
   "outputs": [],
   "source": [
    "#keywords (just for testing purposes - from the policy proposal from GIZ)\n",
    "policy = [\"policy\", \"integrate\", \"implement\", \"committee\", \"consultation\"]\n",
    "food = [\"nutritions\", \"diets\", \"farm\", \"agriculture\", \"ecology\"]\n",
    "ndc_national_adaption_plan = [\"nap\", \"sector plan\", \"nccrp\", \"vulnerable sector\", \n",
    "                              \"geographic vulnerability\"]\n",
    "ndc_climate_change = [\"adaption\", \"program\", \"projects\", \"resilience\", \"institution\",\n",
    "                      \"capacity\", \"response\", \"budget\", \"reprioritisation\", \"development\", \n",
    "                      \"planner\", \"regulator\", \"practitioners\", \"geographical\", \n",
    "                      \"circumstances\", \"land\", \"scheme\", \"authorisation\", \"system\", \n",
    "                      \"spluma\"]\n",
    "ndc_early_warning = [\"system\", \"vulnerability\", \"needs\", \"assessment\", \"network\", \"weather\",\n",
    "   \"earth\", \"observation\", \"academic\", \"community\"]\n",
    "\n",
    "#keywords from json file\n",
    "json_keywords_SA_file = '../ndc_keywords/ndc_south_africa.json'\n",
    "keywords_SA_dict = None\n",
    "with open(json_keywords_SA_file, 'r') as f: \n",
    "    keywords_SA_dict = json.load(f)\n",
    "    \n",
    "    \n",
    "json_keywords_Ethiopia_file = '../ndc_keywords/ndc_ethiopia.json'\n",
    "keywords_Ethiopia_dict = None\n",
    "with open(json_keywords_Ethiopia_file, 'r') as f: \n",
    "    keywords_Ethiopia_dict = json.load(f)\n",
    "    \n",
    "print(keywords_SA_dict)\n",
    "print(keywords_Ethiopia_dict)\n",
    "#ndc_dict = make_filtered_tokens_from_ndc(keywords_SA_dict) #if want to use filtered, lemmatized tokens\n",
    "ndc_dict = keywords_SA_dict\n",
    "#ndc_dict = keywords_Ethiopia_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc7f429",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extra functions\n",
    "def make_ndc_keyword_tidy_df_from_dict(key, col_group_name, ndc_dict):\n",
    "    ndc_df = pd.DataFrame({'keyword': ndc_dict[key], \n",
    "                       col_group_name: key})\n",
    "    return ndc_df\n",
    "\n",
    "def stack_tidy_ndc_dfs(col_group_name, ndc_dict):\n",
    "    ndc_df = pd.DataFrame()\n",
    "    for key in ndc_dict.keys():\n",
    "        ndc_df_add = make_ndc_keyword_tidy_df_from_dict(key, col_group_name, ndc_dict)\n",
    "        ndc_df = pd.concat([ndc_df, ndc_df_add], axis=0)\n",
    "    return ndc_df\n",
    "\n",
    "# lets apply: \n",
    "col_group_name = 'NDC'\n",
    "ndc_df = stack_tidy_ndc_dfs(col_group_name, ndc_dict)\n",
    "ndc_df.head()\n",
    "\n",
    "\n",
    "def make_ndc_idx_tidy_df(ndc_dict, topic_name, tokens):\n",
    "    ndc_idx_df = pd.DataFrame()\n",
    "    for key in ndc_dict.keys(): \n",
    "        ndc_idx_df_to_add = pd.DataFrame({topic_name: key,\n",
    "                                          #('%s word_index'%(key)): [token.idx for token in tokens if token.text in ndc_dict[key]],\n",
    "                                          'word_index': [token.idx for token in tokens if token.text in ndc_dict[key]]})\n",
    "        ndc_idx_df = pd.concat([ndc_idx_df, ndc_idx_df_to_add], axis=0)\n",
    "    return ndc_idx_df\n",
    "\n",
    "def filter_idx_for_overlap(idxs, min_dist):\n",
    "    distance_btwn_idxs = [(idxs[i+1]-idxs[i]) for i in range(0, len(idxs)-1)]\n",
    "    print(distance_btwn_idxs[:20])\n",
    "    filtered_idxs = []\n",
    "    for index, distance in enumerate(distance_btwn_idxs):\n",
    "        if (distance >= min_dist):\n",
    "            filtered_idxs.append(idxs[index])\n",
    "        else:\n",
    "            pass\n",
    "    print(\"The number of times the idx words were found was: \", len(idxs), \"\\n\", \n",
    "          \"The number of idx words seperated by at least the min_distance was : \", len(filtered_idxs))\n",
    "    return filtered_idxs\n",
    "    \n",
    "def make_window_text(tokens, max_length):\n",
    "    filtered_for_length = [token.text.lower() for token in tokens if len(token) < max_length]\n",
    "    text_for_windows = ' '.join(filtered_for_length)\n",
    "    window_tokens = nlp(text_for_windows)\n",
    "    return window_tokens\n",
    "\n",
    "def return_window(ndc_word_index, tokens, size=100):\n",
    "    lower_limit = ndc_word_index - size\n",
    "    upper_limit = ndc_word_index + size\n",
    "    token_idxs = [token.idx for token in tokens]\n",
    "    window_token_list = []\n",
    "    #print('The window is ', lower_limit, upper_limit)\n",
    "    for index, idx in enumerate(token_idxs):\n",
    "        if (idx >= lower_limit) and (idx <= upper_limit):\n",
    "            window_token_list.append(tokens[index])\n",
    "        else:\n",
    "            pass\n",
    "    text_for_windows = ' '.join(list(token.text for token in window_token_list))\n",
    "    window_tokens = nlp(text_for_windows)\n",
    "    return lower_limit, upper_limit, window_tokens\n",
    "\n",
    "def run_nlp_pipeline(doc_name, doc_path, ndc_dict, max_word_length):\n",
    "    print(doc_name, doc_path)\n",
    "    tokens, token_list, sentences = preprocess_doc(doc_path)\n",
    "    filtered_tokens = filter_modify_tokens(tokens)\n",
    "    print('These are some of the filtered tokens: ', filtered_tokens[0:10])\n",
    "    #words = [token.text for token in filtered_tokens]\n",
    "    #word_freq = Counter(words)\n",
    "    #for key in ndc_dict.keys():\n",
    "    #    topic_frequencies =  calculate_topic_frequency_subset(word_freq, ndc_dict, str(key))\n",
    "    #ndc_climate_idxs = [token.idx for token in tokens if token.text in ndc_dict['climate change']]\n",
    "    document_text = ' '.join([token.text for token in tokens if len(token.text)<=max_word_length])\n",
    "    topic_name='NDC'\n",
    "    ndc_idx_df = make_ndc_idx_tidy_df(ndc_dict, topic_name, tokens)\n",
    "    ndc_idx_df.index = ndc_idx_df.NDC.copy()\n",
    "    df_sdg = make_sdg_df(sdg_list, sdg_ontology, document_text)\n",
    "    doc_summary_sdg = df_sdg.groupby('sdg_topic')['sdg_keywords_num'].sum()\n",
    "    doc_summary_sdg_df = doc_summary_sdg.to_frame().rename(columns={\"sdg_keywords_num\": doc_name})\n",
    "    return tokens, token_list, filtered_tokens, ndc_idx_df, doc_summary_sdg, doc_summary_sdg_df #topic_frequencies, sentences\n",
    "\n",
    "def run_nlp_pipeline_no_lemma_ndc_corr(doc_name, doc_path, ndc_dict, max_word_length):\n",
    "    print(doc_name, doc_path)\n",
    "    tokens, token_list, sentences = preprocess_doc(doc_path)\n",
    "    filtered_tokens = filter_tokens(tokens)\n",
    "    print('These are some of the filtered tokens: ', filtered_tokens[0:10])\n",
    "    document_text = ' '.join([token.text for token in filtered_tokens if len(token.text)<=max_word_length])\n",
    "    doc = nlp(document_text)\n",
    "    ndc_labels, ndc_dict_corr, labelled_doc = label_ndc_spans_correlated(ndc_dict, doc, span_length=2, corr_thresh=0.7)\n",
    "    ndc_idx_df = make_ndc_idx_df_from_spans(ndc_dict_corr, labelled_doc)\n",
    "    ndc_idx_df.index = ndc_idx_df.NDC.copy()\n",
    "    ndc_summary_df = ndc_idx_df['NDC'].value_counts().to_frame().rename(columns={\"NDC\": (doc_name)})\n",
    "    df_sdg = make_sdg_df(sdg_list, sdg_ontology, document_text)\n",
    "    doc_summary_sdg = df_sdg.groupby('sdg_topic')['sdg_keywords_num'].sum()\n",
    "    doc_summary_sdg_df = doc_summary_sdg.to_frame().rename(columns={\"sdg_keywords_num\": doc_name})\n",
    "    doc_summary_df = pd.concat([doc_summary_sdg_df, ndc_summary_df]) #combine the SDG and NDC information into one df\n",
    "    return tokens, token_list, sentences, filtered_tokens, document_text, ndc_dict_corr, ndc_idx_df, ndc_summary_df, df_sdg, doc_summary_sdg_df, doc_summary_df #topic_frequencies, sentences\n",
    "\n",
    "\n",
    "def make_ndc_idx_df_from_spans(ndc_keywords, labelled_doc): \n",
    "    \"\"\"ndc_keywords is the ndc keyword dictionary and the labelled_doc is the one labelled with the spans of NDC keywords\"\"\"\n",
    "    ndc_idx_df = pd.DataFrame()\n",
    "    for entity_reference in [key for key in ndc_keywords.keys()]:\n",
    "        entity_label = entity_reference + ' NDC'\n",
    "        #print(entity_label) \n",
    "        ndc_idx_df_to_add = pd.DataFrame({'NDC': entity_reference,\n",
    "                                          ('word'): [ent.text for ent in labelled_doc.ents if ent.label_ == entity_label],\n",
    "                                          'word_index': [ent.start for ent in labelled_doc.ents if ent.label_ == entity_label]})\n",
    "        ndc_idx_df = pd.concat([ndc_idx_df, ndc_idx_df_to_add], axis=0)\n",
    "    return ndc_idx_df \n",
    "\n",
    "def label_ndc_spans(ndc_keywords, doc):\n",
    "    \"\"\"ndc_keywords is a dictionary, doc is the document text with nlp run on it that is the filtered but not lemmatized document text.\"\"\"\n",
    "    matcher = PhraseMatcher(nlp.vocab)\n",
    "    #iterate through NDC keys in NDC dictionary to create seperate label categories for the matching\n",
    "    entity_labels = []\n",
    "    for entity_reference in [key for key in ndc_keywords.keys()]:\n",
    "        entity_label = entity_reference + ' NDC'\n",
    "        #print(entity_label)\n",
    "        entity_labels.append(entity_label)\n",
    "        keywords = ndc_keywords[entity_reference]\n",
    "        patterns = [nlp(i) for i in keywords]\n",
    "        #print(keywords, patterns)\n",
    "        matcher.add(entity_label, None, *patterns)\n",
    "    matches = matcher(doc)\n",
    "        #label spans\n",
    "    for match_id, start, end in matches:\n",
    "        try:\n",
    "            span = Span(doc, start, end, label=match_id)\n",
    "            doc.ents = list(doc.ents) + [span]  # add span to doc.ents\n",
    "        except: \n",
    "            pass\n",
    "    #print([(ent.text, ent.start, ent.label_) for ent in doc.ents])\n",
    "    return entity_labels, doc\n",
    "\n",
    "def label_ndc_spans_correlated(ndc_keywords, doc, span_length=2, corr_thresh=0.7):\n",
    "    \"\"\"ndc_keywords is a dictionary, doc is the document text with nlp run on it that is the filtered but not lemmatized document text.\"\"\"\n",
    "    matcher = PhraseMatcher(nlp.vocab)\n",
    "    #iterate through NDC keys in NDC dictionary to create seperate label categories for the matching\n",
    "    entity_labels = []\n",
    "    ndc_dict_corr = dict()\n",
    "    for entity_reference in [key for key in ndc_keywords.keys()]:\n",
    "        entity_label = entity_reference + ' NDC'\n",
    "        #print(entity_label)\n",
    "        entity_labels.append(entity_label)\n",
    "        keywords = ndc_keywords[entity_reference]\n",
    "        n_gram_cor = NGramCorrelateSpacy(keywords, corr_thresh, entity_label)\n",
    "        doc.ents = []\n",
    "        n_gram_cor.correlate_spans(doc, span_length)\n",
    "        print(len(doc.ents), len(set([ent.text for ent in list(doc.ents)])))\n",
    "        unique_keywords_from_corr = list(set([ent.text for ent in list(doc.ents)]))\n",
    "        ndc_dict_corr[entity_reference] = unique_keywords_from_corr\n",
    "        patterns = [nlp(i) for i in unique_keywords_from_corr]\n",
    "        #print(keywords, patterns)\n",
    "        matcher.add(entity_label, None, *patterns)\n",
    "    matches = matcher(doc)\n",
    "        #label spans\n",
    "    for match_id, start, end in matches:\n",
    "        try:\n",
    "            span = Span(doc, start, end, label=match_id)\n",
    "            doc.ents = list(doc.ents) + [span]  # add span to doc.ents\n",
    "        except: \n",
    "            pass\n",
    "    #print([(ent.text, ent.start, ent.label_) for ent in doc.ents])\n",
    "    return entity_labels, ndc_dict_corr, doc\n",
    "\n",
    "def find_patterns_df(pattern_list, text, topic_name):\n",
    "    pattern_locations = []\n",
    "    pattern_num = []\n",
    "    for pattern in pattern_list:\n",
    "        #print(pattern)\n",
    "        re.findall(pattern, text, flags=0)\n",
    "        #pattern_locations = [(m.start(0), m.end(0)) for m in re.finditer(pattern, text)] #if want start and end\n",
    "        locations = [m.start(0) for m in re.finditer(pattern, text)]\n",
    "        pattern_locations.append(locations)\n",
    "        pattern_num.append(int(len(locations)))\n",
    "    #print(pattern_locations)\n",
    "        #if len(pattern_locations) > 0: \n",
    "        #    print(pattern, len(pattern_locations), pattern_locations)\n",
    "    return pd.DataFrame({'sdg_topic': topic_name,\n",
    "                         'sdg_keywords': pattern_list,\n",
    "                         'sdg_keywords_num': pattern_num,\n",
    "                         'sdg_keyword_locations': pattern_locations})\n",
    "\n",
    "def make_sdg_df(sdg_list, sdg_ontology, text):\n",
    "    df_sdg = pd.DataFrame()\n",
    "    for sdg in list(sdg_list):\n",
    "        sdg_keywords = list(sdg_ontology[sdg_ontology['clasification']==sdg]['keyword'])\n",
    "        #print(sdg)\n",
    "        df_sdg_to_add = find_patterns_df(sdg_keywords, text, topic_name=sdg)\n",
    "        df_sdg = pd.concat([df_sdg, df_sdg_to_add])\n",
    "    return df_sdg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4857372",
   "metadata": {},
   "source": [
    "### SDG and other topic keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf61b2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdg_ontology = pd.read_csv('../additional_resources/Ontology_final_modified.csv', sep=';')#, #skiprows=0)\n",
    "sdg_list = ['SDG1', 'SDG2', 'SDG3', 'SDG4', 'SDG5', 'SDG6', 'SDG7', 'SDG8', 'SDG9', \n",
    "            'SDG10', 'SDG11', 'SDG12', 'SDG13', 'SDG14', 'SDG15', 'SDG16', 'SDG17', \n",
    "            \"mention_money\"]\n",
    "\n",
    "sdg_ontology.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f87631",
   "metadata": {},
   "source": [
    "### Import data: Policy-related documents to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df47609",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:28.176885Z",
     "start_time": "2021-11-04T22:28:28.167255Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#here is the general folder with the different types of policy documents\n",
    "policy_doc_folder = '../test_resources/data'\n",
    "#policy_doc_folder = '../../selected_policy_tracking_docs/Ethiopia/'\n",
    "#get df of docs\n",
    "policy_doc_df = read_docs_to_df(policy_doc_folder)\n",
    "print(len(policy_doc_df))\n",
    "#policy_doc_df['policy_doc_name_clean'] = (policy_doc_df.index.apply(lambda x: x.split('.txt')[0].split('.pdf.ocr')[0]))\n",
    "policy_doc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57d7cae",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a4fe1a-4e4f-4af1-8074-67a7bd315d86",
   "metadata": {},
   "source": [
    "### Analyise Data on Document Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85135848",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_word_length=25\n",
    "corpus_summary_df = pd.DataFrame()\n",
    "count = 1\n",
    "for i in range(0, len(policy_doc_df[:2])):\n",
    "    doc_name = policy_doc_df.iloc[i]['policy_doc_name_clean']\n",
    "    doc_path = policy_doc_df.iloc[i]['policy_doc_paths']\n",
    "    tokens, token_list, sentences, filtered_tokens, document_text, ndc_dict_corr, ndc_idx_df, ndc_summary_df, df_sdg, doc_summary_sdg_df, doc_summary_df = run_nlp_pipeline_no_lemma_ndc_corr(doc_name, doc_path, ndc_dict, max_word_length) \n",
    "    print(ndc_idx_df.NDC.value_counts()) \n",
    "    if count == 1: \n",
    "        corpus_summary_df = doc_summary_df\n",
    "        total = doc_summary_df[doc_name].sum()\n",
    "        print(doc_name, len(filtered_tokens), total)\n",
    "        count += 1\n",
    "    else: \n",
    "        corpus_summary_df_for_merge = doc_summary_df\n",
    "        total = doc_summary_df[doc_name].sum()\n",
    "        print(doc_name, len(filtered_tokens), total)\n",
    "        corpus_summary_df = corpus_summary_df.merge(corpus_summary_df_for_merge, how=\"outer\", left_index=True, right_index=True)\n",
    "        count += 1\n",
    "        \n",
    "corpus_summary_df = corpus_summary_df.fillna(0)\n",
    "corpus_summary_df_corr = corpus_summary_df\n",
    "corpus_summary_df_corr.head()\n",
    "\n",
    "# output_name = '../../outputs/Ethiopia_corr_matching_NDC_SDGs_10_doc_corpus_df.txt'\n",
    "# corpus_summary_df.to_csv(output_name, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216ba862",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_summary_df_corr.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0590c2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Make document level heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce07493",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make heatmap of plot above\n",
    "# plot_folder = '../../outputs/heatmaps/'\n",
    "plt.figure(figsize=(20, 6)) #16, 6\n",
    "ax = sns.heatmap(corpus_summary_df,\n",
    "                 #annot=True, fmt=\"d\", \n",
    "                 cmap=\"YlGnBu\")\n",
    "plt.xlabel(\"Documents\")\n",
    "plt.ylabel(\"Topics\")\n",
    "title = (\"Topic keywords in the documents (counts)\")\n",
    "plt.title(title)\n",
    "file_name=(title + (' all docs Ethiopia v1.png'))\n",
    "# plt.savefig((plot_folder+file_name), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04790794",
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_by_col_sum = corpus_summary_df.div(corpus_summary_df.sum(axis=0), axis=1).round(3)\n",
    "sorted_df = normed_by_col_sum.T.sort_values(by=['climate change'], ascending=False)\n",
    "normed_by_col_sum.head()\n",
    "\n",
    "#normed_by_col_sum_corr = corpus_summary_df_corr.div(corpus_summary_df_corr.sum(axis=0), axis=1).round(3)\n",
    "#sorted_df_corr = normed_by_col_sum_corr.T.sort_values(by=['climate change'], ascending=False)\n",
    "#normed_by_col_sum_corr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c525acff",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_summary_df = corpus_summary_df.fillna(0)\n",
    "corpus_summary_df_corr = corpus_summary_df\n",
    "corpus_summary_df_corr.head()\n",
    "\n",
    "normed_by_col_sum_corr = corpus_summary_df_corr.div(corpus_summary_df_corr.sum(axis=0), axis=1).round(3)\n",
    "sorted_df_corr = normed_by_col_sum_corr.T.sort_values(by=['climate change'], ascending=False)\n",
    "normed_by_col_sum_corr.head()\n",
    "\n",
    "plt.figure(figsize=(20, 6)) #16, 6\n",
    "ax = sns.heatmap(sorted_df_corr.T,\n",
    "                 #annot=True, #fmt=\"d\", \n",
    "                 cmap=\"YlGnBu\")\n",
    "#plt.xlabel(\"Documents\")\n",
    "#plt.ylabel(\"Topics\")\n",
    "#ax.yaxis.tick_right()\n",
    "#ax.yaxis.set_label_position(\"right\")\n",
    "title = (\"NDC and custom topic keywords in the sorted documents\")\n",
    "plt.title(title)\n",
    "file_name=(title + (' (with correlated NDCs) first 10 docs Ethiopia v3.png'))\n",
    "# plt.savefig((plot_folder+file_name), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e4f456",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make heatmap of plot above\n",
    "plt.figure(figsize=(20, 6)) #16, 6\n",
    "ax = sns.heatmap(sorted_df.T,\n",
    "                 #annot=True, #fmt=\"d\", \n",
    "                 cmap=\"YlGnBu\")\n",
    "plt.xlabel(\"Documents\")\n",
    "plt.ylabel(\"Topics\")\n",
    "#ax.yaxis.tick_right()\n",
    "#ax.yaxis.set_label_position(\"right\")\n",
    "title = (\"Topic keywords in the sorted documents (normalized)\")\n",
    "plt.title(title)\n",
    "file_name=(title + (' all doc Ethiopia v1.png'))\n",
    "# plt.savefig((plot_folder+file_name), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e2e270",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:39.197908Z",
     "start_time": "2021-11-04T22:28:35.779411Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24,6)) #16, 6\n",
    "ax = sns.heatmap(normed_by_col_sum,\n",
    "                 #annot=True, fmt=\"d\", \n",
    "                 cmap=\"YlGnBu\")\n",
    "plt.xlabel(\"Documents\")\n",
    "plt.ylabel(\"Topics\")\n",
    "title = (\"Topic keywords in the documents (normalized)\")\n",
    "plt.title(title)\n",
    "file_name=(title + (' all docs Ethiopia v1.png'))\n",
    "# plt.savefig((plot_folder+file_name), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee269af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24,6)) #16, 6\n",
    "ax = sns.heatmap(normed_by_col_sum,\n",
    "                 #annot=True, fmt=\"d\", \n",
    "                 cmap=\"YlGnBu\")\n",
    "plt.xlabel(\"Documents\")\n",
    "plt.ylabel(\"Topics\")\n",
    "title = (\"Topic keywords in the documents (normalized)\")\n",
    "plt.title(title)\n",
    "file_name=(title + (' all docs SA v2.png.png'))\n",
    "# plt.savefig((plot_folder+file_name), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487f3e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "ax = sns.clustermap(normed_by_col_sum.T,\n",
    "                 #annot=True,\n",
    "                 cbar_pos=(0, .45, .03, .2),\n",
    "                 cmap=\"YlGnBu\", \n",
    "                 xticklabels=1, \n",
    "                 yticklabels=1, \n",
    "                 figsize=(5, 15))\n",
    "#plt.xlabel(\"Documents\")\n",
    "#plt.ylabel(\"SDG Topics\")\n",
    "title = (\"Distribution of topic keywords in the documents (normalized)\")\n",
    "#plt.title(title)\n",
    "file_name=(title + (' clustermap all docs SA v2.png'))\n",
    "# plt.savefig((plot_folder+file_name), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227bb8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.clustermap(normed_by_col_sum_corr.T,\n",
    "                 #annot=True,\n",
    "                 cbar_pos=(0, .45, .03, .2),\n",
    "                 cmap=\"YlGnBu\", \n",
    "                 xticklabels=1, \n",
    "                 figsize=(12, 6))\n",
    "#plt.xlabel(\"Documents\")\n",
    "#plt.ylabel(\"SDG Topics\")\n",
    "title = (\"Distribution of topic keywords (with correlated NDC words) in the documents (normalized)\")\n",
    "#plt.title(title)\n",
    "file_name=(title + (' clustermap 10 docs SA v2.png'))\n",
    "# plt.savefig((plot_folder+file_name), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6869a1e-e494-4543-aa07-c30c9ee39fe5",
   "metadata": {},
   "source": [
    "### Now we look closer at one specific document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18cf6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if using Johnathan's fuzzy search:\n",
    "doc_name = '2nd National Biodiversity Strategy Action Plan 2015-2025.txt' #'Climate_Change_Bill (2018).txt' \n",
    "doc_path = policy_doc_df.loc[doc_name]['policy_doc_paths']\n",
    "\n",
    "max_word_length=25\n",
    "tokens, token_list, sentences, filtered_tokens, document_text, ndc_dict_corr, ndc_idx_df, ndc_summary_df, df_sdg, doc_summary_sdg_df, doc_summary_df = run_nlp_pipeline_no_lemma_ndc_corr(doc_name, doc_path, ndc_dict, max_word_length) \n",
    "print(ndc_idx_df.NDC.value_counts()) \n",
    "total = doc_summary_df[doc_name].sum()\n",
    "print(doc_name, len(filtered_tokens), total)\n",
    "#output_name = '../../outputs/SA_corr_matching_NDC_SDGs_climate_change_bill_2018_df.txt'\n",
    "#corpus_summary_df.to_csv(output_name, sep='\\t')\n",
    "ndc_dict_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e98666",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_tokens_overall = tokens\n",
    "window_size = 200\n",
    "ndc_climate_idxs = list(ndc_idx_df[ndc_idx_df['NDC']=='climate change']['word_index'])\n",
    "idx_for_window = ndc_climate_idxs\n",
    "idx_for_window = filter_idx_for_overlap(idxs=ndc_climate_idxs, min_dist=50)\n",
    "\n",
    "count = 0\n",
    "for index in idx_for_window[0:len(idx_for_window)]:\n",
    "    count += 1\n",
    "    print(index)\n",
    "    window_tokens = return_window(index, window_tokens_overall, size=window_size)[2]\n",
    "    window_text = ' '.join([token.text for token in window_tokens])\n",
    "    #print(window_text)\n",
    "    df_sdg = make_sdg_df(sdg_list, sdg_ontology, window_text) #may want to keep this for the windows for a more granular analysis\n",
    "    df_sdg_sorted = df_sdg.sort_values(by=['sdg_keywords_num'], ascending=False)\n",
    "    #print(df_sdg_sorted.head())\n",
    "    print(list(df_sdg_sorted[df_sdg_sorted['sdg_keywords_num'] > 0]['sdg_keywords'][:10])) #print top 10 positive valued keywords\n",
    "    #print a summary of the SDG words found: \n",
    "    summary_sdg = df_sdg.groupby('sdg_topic')['sdg_keywords_num'].sum()\n",
    "    if count == 1: \n",
    "        summary_sdg_df = summary_sdg.to_frame().rename(columns={\"sdg_keywords_num\": (\"sdg_kw_%d\"% index)})\n",
    "    else: \n",
    "        summary_sdg_df_for_merge = summary_sdg.to_frame().rename(columns={\"sdg_keywords_num\": (\"sdg_kw_%d\"% index)})\n",
    "        summary_sdg_df = summary_sdg_df.merge(summary_sdg_df_for_merge, left_index=True, right_index=True)\n",
    "summary_sdg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531c1de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_sdg_df_filtered = summary_sdg_df.loc[:, (summary_sdg_df.sum() >= 10)]\n",
    "#make heatmap of plot above\n",
    "plt.figure(figsize=(16, 6))\n",
    "ax = sns.heatmap(summary_sdg_df_filtered,\n",
    "                 annot=True, #fmt=\"d\", \n",
    "                 cmap=\"YlGnBu\")\n",
    "plt.xlabel(\"climate change NDC-associated windows\")\n",
    "plt.ylabel(\"SDG and Custom Topics\")\n",
    "title = (\"Distribution of topic keywords in climate change NDC-associated windows in %s\" % (doc_name))\n",
    "plt.title(title)\n",
    "file_name=(title + (' across windows from corr matching and above 10 topic words.png'))\n",
    "# plt.savefig((plot_folder+file_name), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9acea96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(ndc_idx_df[ndc_idx_df['word_index']==23262])\n",
    "#index=2875\n",
    "#index=9693\n",
    "index=23262\n",
    "print(ndc_idx_df[ndc_idx_df['word_index']==index])\n",
    "print(return_window(index, tokens, size=window_size)[2]) #print(window_tokens[2])\n",
    "summary_sdg_df[('sdg_kw_%d' % index)]\n",
    "test_text = return_window(index, window_tokens_overall, size=window_size)[2]\n",
    "test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e12f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make heatmap of plot above\n",
    "summary_sdg_df_filtered = summary_sdg_df.loc[:, (summary_sdg_df.sum() >= 6)]\n",
    "plt.figure(figsize=(16, 6))\n",
    "ax = sns.heatmap(summary_sdg_df_filtered,\n",
    "                 #annot=True, #fmt=\"d\", \n",
    "                 cmap=\"YlGnBu\")\n",
    "plt.xlabel(\"climate change NDC-associated windows\")\n",
    "plt.ylabel(\"SDG and Custom Topics\")\n",
    "title = (\"Distribution of topic keywords in climate change NDC-associated windows in %s\" % (doc_name))\n",
    "plt.title(title)\n",
    "file_name=(title + (' across windows from corr matching and above 6 topic words.png'))\n",
    "# plt.savefig((plot_folder+file_name), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6213a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make heatmap of plot above\n",
    "plt.figure(figsize=(20, 6))\n",
    "ax = sns.heatmap(summary_sdg_df.T,\n",
    "                 annot=True, #fmt=\"d\", \n",
    "                 cmap=\"YlGnBu\")\n",
    "plt.xlabel(\"climate change NDC-associated windows\")\n",
    "plt.ylabel(\"SDG and Custom Topics\")\n",
    "title = (\"Distribution of topic keywords in climate change NDC-associated windows in %s\" % (doc_name))\n",
    "plt.title(title)\n",
    "file_name=(title + (' across windows from corr matching.png'))\n",
    "# plt.savefig((plot_folder+file_name), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a74b55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_ndc_sdg_spans_in_windows(ndc_keywords, doc):\n",
    "    \"\"\"ndc_keywords is a dictionary, doc is the document text with nlp run on it that is the filtered but not lemmatized document text.\"\"\"\n",
    "    matcher = PhraseMatcher(nlp.vocab)\n",
    "    #iterate through NDC keys in NDC dictionary to create seperate label categories for the matching\n",
    "    entity_labels = []\n",
    "    for entity_reference in [key for key in ndc_keywords.keys()]:\n",
    "        entity_label = entity_reference + ' NDC'\n",
    "        #print(entity_label)\n",
    "        entity_labels.append(entity_label)\n",
    "        keywords = ndc_keywords[entity_reference]\n",
    "        patterns = [nlp(i) for i in keywords]\n",
    "        #print(keywords, patterns)\n",
    "        matcher.add(entity_label, None, *patterns)\n",
    "    matches = matcher(doc)\n",
    "        #label spans\n",
    "    for match_id, start, end in matches:\n",
    "        try:\n",
    "            span = Span(doc, start, end, label=match_id)\n",
    "            doc.ents = list(doc.ents) + [span]  # add span to doc.ents\n",
    "        except: \n",
    "            pass\n",
    "    #print([(ent.text, ent.start, ent.label_) for ent in doc.ents])\n",
    "    return entity_labels, doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07141693",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndc_idx_df.NDC.value_counts()#head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec35dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at SDGs across document/at document level\n",
    "max_length=25\n",
    "document_text = ' '.join([token.text for token in tokens if len(token.text)<=max_length])\n",
    "\n",
    "df_sdg = make_sdg_df(sdg_list, sdg_ontology, document_text)\n",
    "doc_summary_sdg = df_sdg.groupby('sdg_topic')['sdg_keywords_num'].sum()\n",
    "doc_summary_sdg_df = doc_summary_sdg.to_frame().rename(columns={\"sdg_keywords_num\": (\"%s\"% doc_name)})\n",
    "doc_summary_sdg_df \n",
    "\n",
    "#lets see the 50 words that occur the most often\n",
    "df_sdg.sort_values(by=['sdg_keywords_num'], ascending=False)[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe608f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = doc_summary_sdg_df[doc_name].sum()\n",
    "print(doc_name, len(filtered_tokens), total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78e683f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Apply Jonathan's fuzzy search method to find NDC related words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7703f6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sdg_ontology[sdg_ontology['clasification']=='SDG13']\n",
    "climate_keywords = ndc_dict['climate change']\n",
    "climate_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74a41a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndc_keywords = climate_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c588a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_ndc_spans(ndc_keywords, doc):\n",
    "    \"\"\"ndc_keywords is a dictionary, doc is the document text with nlp run on it that is the filtered but not lemmatized document text.\"\"\"\n",
    "    matcher = PhraseMatcher(nlp.vocab)\n",
    "    #iterate through NDC keys in NDC dictionary to create seperate label categories for the matching\n",
    "    entity_labels = []\n",
    "    for entity_reference in [key for key in ndc_keywords.keys()]:\n",
    "        entity_label = entity_reference + ' NDC'\n",
    "        print(entity_label)\n",
    "        entity_labels.append(entity_label)\n",
    "        keywords = ndc_keywords[entity_reference]\n",
    "        patterns = [nlp(i) for i in keywords]\n",
    "        #print(keywords, patterns)\n",
    "        matcher.add(entity_label, None, *patterns)\n",
    "    matches = matcher(doc)\n",
    "        #label spans\n",
    "    for match_id, start, end in matches:\n",
    "        try:\n",
    "            span = Span(doc, start, end, label=match_id)\n",
    "            doc.ents = list(doc.ents) + [span]  # add span to doc.ents\n",
    "        except: \n",
    "            pass\n",
    "    #print([(ent.text, ent.start, ent.label_) for ent in doc.ents])\n",
    "    return entity_labels, doc\n",
    "\n",
    "test_text = document_text\n",
    "test_doc = nlp(test_text)\n",
    "ndc_labels, labelled_doc = label_ndc_spans(ndc_dict, test_doc)\n",
    "ndc_idx_df = make_ndc_idx_df_from_spans(ndc_dict, labelled_doc)\n",
    "#displacy.render(labelled_doc, style = \"ent\", jupyter = True)\n",
    "#labelled_doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1b90a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndc_idx_df = make_ndc_idx_df_from_spans(ndc_dict, labelled_doc)\n",
    "ndc_idx_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f566feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if count == 1: \n",
    "    summary_sdg_df_doc = doc_summary_sdg_df\n",
    "    total = doc_summary_sdg_df[doc_name].sum()\n",
    "    print(doc_name, len(filtered_tokens), total)\n",
    "else: \n",
    "    summary_sdg_df_doc_for_merge = doc_summary_sdg_df\n",
    "    total = summary_sdg_df_doc_for_merge[doc_name].sum()\n",
    "    print(doc_name, len(filtered_tokens), total)\n",
    "    summary_sdg_df_doc = summary_sdg_df_doc.merge(summary_sdg_df_doc_for_merge, left_index=True, right_index=True)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b276326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"helps to address poverty and unemployment . operation phakisa is initially implemented in two sectors , the ocean economy and health , and will be rolled out in other sectors . in the oceans economy four priority areas for unlocking the oceans economy through inclusive economic growth have been identified , one of which is marine protection services and ocean governance . other biodiversity and\"\n",
    "sdg_ontology[sdg_ontology['clasification']=='SDG8']['keyword']\n",
    "#list(ndc_dict['climate change'])\n",
    "\n",
    "def label_ndc_spans_return_index(entity_reference, ndc_keywords, document_text):\n",
    "    \"\"\"ndc_keywords is a dictionary, document_text is the filtered but not lemmatized document text.\"\"\"\n",
    "    entity_label = entity_reference + ' NDC'\n",
    "    keywords = ndc_keywords[entity_reference]\n",
    "    patterns = [nlp(i) for i in keywords]\n",
    "    matcher = PhraseMatcher(nlp.vocab)\n",
    "    matcher.add(entity_label, None, *patterns)\n",
    "    doc = nlp(document_text)\n",
    "    matches = matcher(doc)\n",
    "    #label spans\n",
    "    for match_id, start, end in matches:\n",
    "        try:\n",
    "            span = Span(doc, start, end, label=match_id)\n",
    "            doc.ents = list(doc.ents) + [span]  # add span to doc.ents\n",
    "        except: \n",
    "            pass\n",
    "    #get list of spans related to the ndc\n",
    "    idxs_from_matcher = [ent.start for ent in doc.ents if ent.label_ == entity_label]\n",
    "    return doc, idxs_from_matcher\n",
    "    \n",
    "entity_reference = 'climate change'\n",
    "#print([(ent.text, ent.start, ent.label_) for ent in doc.ents]) #there are some cool default entitites as well\n",
    "climate_ndc_idxs_from_matcher = label_ndc_spans_return_index(entity_reference, ndc_keywords, document_text)[1]\n",
    "climate_ndc_idxs_from_matcher\n",
    "\n",
    "new_doc = label_ndc_spans_return_index(entity_reference, ndc_keywords, document_text)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b639213",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets make another function to label categories:\n",
    "\n",
    "def label_keywords(entity_reference, keyword_list, text):\n",
    "    \"\"\"ndc_keywords is a dictionary, document_text is the filtered but not lemmatized document text.\"\"\"\n",
    "    entity_label = entity_reference\n",
    "    patterns = [nlp(i) for i in keyword_list]\n",
    "    matcher = PhraseMatcher(nlp.vocab)\n",
    "    matcher.add(entity_label, None, *patterns)\n",
    "    doc = nlp(document_text)\n",
    "    matches = matcher(doc)\n",
    "    #label spans\n",
    "    for match_id, start, end in matches:\n",
    "        try:\n",
    "            span = Span(doc, start, end, label=match_id)\n",
    "            doc.ents = list(doc.ents) + [span]  # add span to doc.ents\n",
    "        except: \n",
    "            pass\n",
    "    #get list of spans related to the ndc\n",
    "    idxs_from_matcher = [ent.start for ent in doc.ents if ent.label_ == entity_label]\n",
    "    return doc, idxs_from_matcher\n",
    "\n",
    "sdg_list = ['SDG1', 'SDG2', 'SDG3', 'SDG4', 'SDG5', 'SDG6', 'SDG7', 'SDG8', 'SDG9', \n",
    "            'SDG10', 'SDG11', 'SDG12', 'SDG13', 'SDG14', 'SDG15', 'SDG16', 'SDG17', \n",
    "            \"mention_money\"]\n",
    "\n",
    "index=361\n",
    "index=5868\n",
    "index=9693\n",
    "print(return_window(index, window_tokens_overall, size=window_size)[2]) #print(window_tokens[2])\n",
    "summary_sdg_df[('sdg_kw_%d' % index)]\n",
    "\n",
    "test_text = return_window(index, window_tokens_overall, size=window_size)[2]\n",
    "test_text\n",
    "\n",
    "text = test_text\n",
    "sdg_list = ['SDG15']\n",
    "for i in sdg_list:\n",
    "    entity_reference = i\n",
    "    keyword_list = list(sdg_ontology[sdg_ontology['clasification']==entity_reference]['keyword'])\n",
    "    text = label_keywords(entity_reference, keyword_list, text)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655ffff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "colors = {\"climate change NDC\" :'#3A9C75', \n",
    "          \"SDG3\": \"#3A619C\"\n",
    "          \"SDG14\": \"#85C1E9\", \n",
    "          \"SDG8\": \"#ff6961\", \n",
    "          \"SDG15\": \"#3A619C\"}\n",
    "#colors = {\"SDG14\": \"#85C1E9\", \"SDG8\": \"#ff6961\"}\n",
    "options = {\"ents\": ['SDG14', 'SDG8', \"SDG15\"], \"colors\": colors}\n",
    "displacy.render(doc, style='ent', options=options) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61dcc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets see if we can find ndc phrases with spacy matcher\n",
    "#document_text\n",
    "\n",
    "keywords = list(ndc_keyword_dict['climate cha'])\n",
    "sdg14_keywords = list(sdg_ontology[sdg_ontology['clasification']=='SDG14']['keyword'])\n",
    "sdg8_keywords = list(sdg_ontology[sdg_ontology['clasification']=='SDG8']['keyword'])\n",
    "\n",
    "patterns14 = [nlp(i) for i in sdg14_keywords]\n",
    "patterns8 = [nlp(i) for i in sdg8_keywords]\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add('SDG14', None, *patterns14)\n",
    "matcher.add('SDG8', None, *patterns8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f20ca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ndc_climate_idxs\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "good = ['bacon', 'chicken', 'lamb','hot dog']\n",
    "bad = ['apple', 'carrot']\n",
    "patterns1 = [nlp(good) for good in good]\n",
    "patterns2 = [nlp(bad) for bad in bad]\n",
    "\n",
    "sdg14_keywords = list(sdg_ontology[sdg_ontology['clasification']=='SDG14']['keyword'])\n",
    "sdg8_keywords = list(sdg_ontology[sdg_ontology['clasification']=='SDG8']['keyword'])\n",
    "\n",
    "patterns14 = [nlp(i) for i in sdg14_keywords]\n",
    "patterns8 = [nlp(i) for i in sdg8_keywords]\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add('SDG14', None, *patterns14)\n",
    "matcher.add('SDG8', None, *patterns8)\n",
    "\n",
    "#doc = nlp(\"I like bacon and chicken but unfortunately I only had an apple and a carrot in the fridge\")\n",
    "doc = nlp(test_text)\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    try:\n",
    "        span = Span(doc, start, end, label=match_id)\n",
    "        doc.ents = list(doc.ents) + [span]  # add span to doc.ents\n",
    "    except: \n",
    "        pass\n",
    "\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])\n",
    "\n",
    "from spacy import displacy\n",
    "colors = {\"SDG14\": \"#85C1E9\", \"SDG8\": \"#ff6961\"}\n",
    "options = {\"ents\": ['SDG14', 'SDG8'], \"colors\": colors}\n",
    "displacy.render(doc, style='ent', options=options) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb172494",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ndc_climate_idxs\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "good = ['bacon', 'chicken', 'lamb','hot dog']\n",
    "bad = ['apple', 'carrot']\n",
    "patterns1 = [nlp(good) for good in good]\n",
    "patterns2 = [nlp(bad) for bad in bad]\n",
    "\n",
    "sdg14_keywords = list(sdg_ontology[sdg_ontology['clasification']=='SDG14']['keyword'])\n",
    "sdg8_keywords = list(sdg_ontology[sdg_ontology['clasification']=='SDG8']['keyword'])\n",
    "\n",
    "patterns14 = [nlp(i) for i in sdg14_keywords]\n",
    "patterns8 = [nlp(i) for i in sdg8_keywords]\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add('SDG14', None, *patterns14)\n",
    "matcher.add('SDG8', None, *patterns8)\n",
    "\n",
    "#doc = nlp(\"I like bacon and chicken but unfortunately I only had an apple and a carrot in the fridge\")\n",
    "doc = nlp(test_text)\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    try:\n",
    "        span = Span(doc, start, end, label=match_id)\n",
    "        doc.ents = list(doc.ents) + [span]  # add span to doc.ents\n",
    "    except: \n",
    "        pass\n",
    "\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])\n",
    "\n",
    "from spacy import displacy\n",
    "colors = {\"SDG14\": \"#85C1E9\", \"SDG8\": \"#ff6961\"}\n",
    "options = {\"ents\": ['SDG14', 'SDG8'], \"colors\": colors}\n",
    "displacy.render(doc, style='ent', options=options) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4e808d",
   "metadata": {},
   "source": [
    "### Window functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503efa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndc_climate_idxs = [token.idx for token in tokens if token.text in ndc_dict['climate change']]\n",
    "ndc_climate_idxs = [ent.start for ent in doc.ents]\n",
    "#ndc_early_warning_idxs = [token.idx for token in tokens if token.text in ndc_dict['early warning']]\n",
    "#ndc_nap_idxs = [token.idx for token in tokens if token.text in ndc_dict['national adaptation plan']]\n",
    "idx_for_window = filter_idx_for_overlap(idxs=ndc_climate_idxs, min_dist=200)\n",
    "window_tokens_overall = make_window_text(tokens, max_length=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddeddce",
   "metadata": {},
   "source": [
    "### Find most common (and unique) words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bb1aa3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:39.216487Z",
     "start_time": "2021-11-04T22:28:39.199518Z"
    }
   },
   "outputs": [],
   "source": [
    "print(doc_name, doc_path)\n",
    "tokens, token_list, sentences = preprocess_doc(doc_path)\n",
    "filtered_tokens = filter_modify_tokens(tokens)\n",
    "print('These are some of the filtered tokens: ', filtered_tokens[0:10])\n",
    "words = [token.text for token in filtered_tokens]\n",
    "word_freq = Counter(words)\n",
    "for key in ndc_dict.keys():\n",
    "    topic_frequencies =  calculate_topic_frequency_subset(word_freq, ndc_dict, str(key))\n",
    "    print(topic_frequencies)\n",
    "    \n",
    "ndc_climate_idxs = #[token.idx for token in tokens if token.text in ndc_dict['climate change']]\n",
    "\n",
    "\n",
    "# Remove stop words and punctuation symbols\n",
    "words = [token.text for token in filtered_tokens]\n",
    "word_freq = Counter(words)\n",
    "# 5 commonly occurring words with their frequencies\n",
    "common_words = word_freq.most_common(30)\n",
    "print(common_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada47c6b",
   "metadata": {},
   "source": [
    "### Make some plots of the NDC/Thematic key words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f0ef52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:39.785606Z",
     "start_time": "2021-11-04T22:28:39.220235Z"
    }
   },
   "outputs": [],
   "source": [
    "#just to test - need to modularise/make more reproducible and tailor output to be useful across documents\n",
    "#could also link this back to the df of the documents to output a table with some summary metrics for keywords \n",
    "#in each document of interest\n",
    "graphs_folder = '../../outputs/bar_charts/'    \n",
    "    \n",
    "for key in ndc_dict.keys(): \n",
    "    print(\"Graphing the occurences of %s words in the document\" % key)\n",
    "    topic_frequencies =  calculate_topic_frequency_subset(word_freq, ndc_dict, str(key))\n",
    "    plot_word_freq_barchart_ndc(topic_frequencies, str(key), doc_name, graphs_folder) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838af122",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndc_climate_idxs = [ent.start for ent in doc.ents]\n",
    "if key == 'climate change':\n",
    "        print(\"Graphing the occurences of %s words in the document\" % key)\n",
    "        topic_frequencies = calculate_topic_frequency_subset(word_freq, ndc_dict, str(key))\n",
    "        plot_word_freq_barchart_ndc(topic_frequencies, str(key), doc_name, graphs_folder) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900e8fb0",
   "metadata": {},
   "source": [
    "### Lets take a closer look at the climate change NDC keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462068c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:40.061691Z",
     "start_time": "2021-11-04T22:28:39.788052Z"
    }
   },
   "outputs": [],
   "source": [
    "#for key in ndc_dict.keys(): \n",
    "#    if key == 'climate change':\n",
    "#        print(\"Graphing the occurences of %s words in the document\" % key)\n",
    "#        topic_frequencies = calculate_topic_frequency_subset(word_freq, ndc_dict, str(key))\n",
    "#        plot_word_freq_barchart_ndc(topic_frequencies, str(key), doc_name, graphs_folder) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0acbd5",
   "metadata": {},
   "source": [
    "### Where do these words appear in the document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8f3377",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_tokens_overall = make_window_text(tokens, max_length=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f19d46",
   "metadata": {},
   "source": [
    "#### Use indexes from spacy matcher "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d067e20e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T08:26:52.345345Z",
     "start_time": "2021-11-05T08:26:52.146105Z"
    }
   },
   "outputs": [],
   "source": [
    "#may want to increase the min_distance/set it as a function of window size\n",
    "window_size = 200\n",
    "min_dist = 200\n",
    "idx_for_window = filter_idx_for_overlap(idxs=climate_ndc_idxs_from_matcher, min_dist=min_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcf73db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:40.217870Z",
     "start_time": "2021-11-04T22:28:40.215870Z"
    }
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for index in idx_for_window[0:len(idx_for_window)]:\n",
    "    count += 1\n",
    "    print(index)\n",
    "    window_tokens = return_window(index, window_tokens_overall, size=window_size)[2]\n",
    "    window_text = ' '.join([token.text for token in window_tokens])\n",
    "    #print(window_text)\n",
    "    df_sdg = make_sdg_df(sdg_list, sdg_ontology, window_text) #may want to keep this for the windows for a more granular analysis\n",
    "    df_sdg_sorted = df_sdg.sort_values(by=['sdg_keywords_num'], ascending=False)\n",
    "    #print(df_sdg_sorted.head())\n",
    "    print(list(df_sdg_sorted[df_sdg_sorted['sdg_keywords_num'] > 0]['sdg_keywords'][:10])) #print top 10 positive valued keywords\n",
    "    #print a summary of the SDG words found: \n",
    "    summary_sdg = df_sdg.groupby('sdg_topic')['sdg_keywords_num'].sum()\n",
    "    if count == 1: \n",
    "        summary_sdg_df = summary_sdg.to_frame().rename(columns={\"sdg_keywords_num\": (\"sdg_kw_%d\"% index)})\n",
    "    else: \n",
    "        summary_sdg_df_for_merge = summary_sdg.to_frame().rename(columns={\"sdg_keywords_num\": (\"sdg_kw_%d\"% index)})\n",
    "        summary_sdg_df = summary_sdg_df.merge(summary_sdg_df_for_merge, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015bf4a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:40.279908Z",
     "start_time": "2021-11-04T22:28:40.219372Z"
    }
   },
   "outputs": [],
   "source": [
    "summary_sdg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8c9acb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:40.286182Z",
     "start_time": "2021-11-04T22:28:40.281225Z"
    }
   },
   "outputs": [],
   "source": [
    "#make heatmap of plot above\n",
    "plt.figure(figsize=(16, 6))\n",
    "ax = sns.heatmap(summary_sdg_df,\n",
    "                 annot=True, #fmt=\"d\", \n",
    "                 cmap=\"YlGnBu\")\n",
    "plt.xlabel(\"climate change NDC-associated windows\")\n",
    "plt.ylabel(\"SDG Topics\")\n",
    "title = (\"Distribution of topic keywords in climate change NDC-associated windows in %s\" % (doc_name))\n",
    "plt.title(title)\n",
    "file_name=(title + (' across windows from spacy matching.png'))\n",
    "plt.savefig((plot_folder+file_name), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83d6b37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:40.871444Z",
     "start_time": "2021-11-04T22:28:40.287814Z"
    }
   },
   "outputs": [],
   "source": [
    "index=361\n",
    "print(return_window(index, window_tokens_overall, size=window_size)[2]) #print(window_tokens[2])\n",
    "summary_sdg_df[('sdg_kw_%d' % index)]\n",
    "print(\"\")\n",
    "\n",
    "index=5868\n",
    "print(return_window(index, window_tokens_overall, size=window_size)[2]) #print(window_tokens[2])\n",
    "summary_sdg_df[('sdg_kw_%d' % index)]\n",
    "print(\"\")\n",
    "\n",
    "index=9693\n",
    "print(return_window(index, window_tokens_overall, size=window_size)[2]) #print(window_tokens[2])\n",
    "summary_sdg_df[('sdg_kw_%d' % index)]\n",
    "print(\"\")\n",
    "\n",
    "test_text = return_window(index, window_tokens_overall, size=window_size)[2]\n",
    "test_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9db410",
   "metadata": {},
   "source": [
    "### Using other resources:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dbc823",
   "metadata": {},
   "source": [
    "### NDC Ontology with SDG classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7560567",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T08:39:50.566673Z",
     "start_time": "2021-11-05T08:39:50.555850Z"
    }
   },
   "outputs": [],
   "source": [
    "sdg_ontology = pd.read_csv('../additional_resources/Ontology_final_modified.csv', sep=';')#, #skiprows=0)\n",
    "SDG1_keywords = list(sdg_ontology[sdg_ontology['clasification']=='SDG1']['keyword'])\n",
    "#print(SDG1_keywords)\n",
    "sdg_ontology.head(20)\n",
    "#print(list(ndc_ontology[ndc_ontology['clasification']=='SDG3']['keyword']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c13e34b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T08:11:20.768901Z",
     "start_time": "2021-11-05T08:11:18.591192Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "document_text = ' '.join([token.text for token in window_tokens_overall])\n",
    "\n",
    "def find_patterns_df(pattern_list, text, topic_name):\n",
    "    pattern_locations = []\n",
    "    pattern_num = []\n",
    "    for pattern in pattern_list:\n",
    "        #print(pattern)\n",
    "        re.findall(pattern, text, flags=0)\n",
    "        #pattern_locations = [(m.start(0), m.end(0)) for m in re.finditer(pattern, text)] #if want start and end\n",
    "        locations = [m.start(0) for m in re.finditer(pattern, text)]\n",
    "        pattern_locations.append(locations)\n",
    "        pattern_num.append(int(len(locations)))\n",
    "    #print(pattern_locations)\n",
    "        #if len(pattern_locations) > 0: \n",
    "        #    print(pattern, len(pattern_locations), pattern_locations)\n",
    "    return pd.DataFrame({'sdg_topic': topic_name,\n",
    "                         'sdg_keywords': pattern_list,\n",
    "                         'sdg_keywords_num': pattern_num,\n",
    "                         'sdg_keyword_locations': pattern_locations})\n",
    "\n",
    "\n",
    "#look at SDGs across document/at document level\n",
    "sdg_list = ['SDG1', 'SDG2', 'SDG3', 'SDG4', 'SDG5', 'SDG6', 'SDG7', 'SDG8', 'SDG9', \n",
    "            'SDG10', 'SDG11', 'SDG12', 'SDG13', 'SDG14', 'SDG15', 'SDG16', 'SDG17', \n",
    "            \"mention_money\"]\n",
    "\n",
    "def make_sdg_df(sdg_list, sdg_ontology, text):\n",
    "    df_sdg = pd.DataFrame()\n",
    "    for sdg in list(sdg_list):\n",
    "        sdg_keywords = list(sdg_ontology[sdg_ontology['clasification']==sdg]['keyword'])\n",
    "        #print(sdg)\n",
    "        df_sdg_to_add = find_patterns_df(sdg_keywords, text, topic_name=sdg)\n",
    "        df_sdg = pd.concat([df_sdg, df_sdg_to_add])\n",
    "    return df_sdg\n",
    "\n",
    "df_sdg = make_sdg_df(sdg_list, sdg_ontology, document_text)\n",
    "\n",
    "#lets see the 50 words that occur the most often\n",
    "df_sdg.sort_values(by=['sdg_keywords_num'], ascending=False)[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b5d262",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(): \n",
    "    print('Processing doc: ', doc_name)\n",
    "    \n",
    "document_text = ' '.join([token.text for token in window_tokens_overall])\n",
    "df_sdg = make_sdg_df(sdg_list, sdg_ontology, document_text)\n",
    "doc_summary_sdg = df_sdg.groupby('sdg_topic')['sdg_keywords_num'].sum()\n",
    "doc_summary_sdg.to_frame().rename(columns={\"sdg_keywords_num\": (\"%s\"% doc_name)})\n",
    "\n",
    "#if count == 1: \n",
    "#    summary_sdg_df = summary_sdg.to_frame().rename(columns={\"sdg_keywords_num\": (\"sdg_kw_%d\"% index)})\n",
    "#else: \n",
    "#    summary_sdg_df_for_merge = summary_sdg.to_frame().rename(columns={\"sdg_keywords_num\": (\"sdg_kw_%d\"% index)})\n",
    "#    summary_sdg_df = summary_sdg_df.merge(summary_sdg_df_for_merge, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f229ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T08:11:22.665452Z",
     "start_time": "2021-11-05T08:11:22.153909Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_folder = '../../outputs/heatmaps/'\n",
    "#print a summary of the SDG words found: \n",
    "doc_summary_sdg = df_sdg.groupby('sdg_topic')['sdg_keywords_num'].sum()\n",
    "doc_summary_sdg.to_frame()\n",
    "#doc_summary_sdg.to_frame()\n",
    "plt.figure(figsize=(2, 6))\n",
    "ax = sns.heatmap(doc_summary_sdg.to_frame(), \n",
    "                 annot=True, fmt=\"d\", \n",
    "                 cmap=\"YlGnBu\")\n",
    "\n",
    "#plt.xlabel(\"climate change NDC-associated windows\")\n",
    "plt.ylabel(\"Topics\")\n",
    "title = (\"Topic keywords in %s\" % (doc_name))\n",
    "plt.title(title)\n",
    "file_name=(title + (' across document v1.png'))\n",
    "plt.savefig((plot_folder+file_name), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05c9600",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:48:11.802550Z",
     "start_time": "2021-11-04T22:48:11.748704Z"
    }
   },
   "outputs": [],
   "source": [
    "len(idx_for_window)\n",
    "for index in idx_for_window[60:63]:\n",
    "    print(return_window(index, window_tokens_overall)[2])\n",
    "idx_for_window[60:63]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253bc7f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:48:16.297411Z",
     "start_time": "2021-11-04T22:48:16.291014Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sdg_sorted  = df_sdg.sort_values(by=['sdg_keywords_num'], ascending=False)\n",
    "list(df_sdg_sorted[df_sdg_sorted['sdg_keywords_num'] > 0]['sdg_keywords'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2951206",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:48:36.716699Z",
     "start_time": "2021-11-04T22:48:35.936147Z"
    }
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for index in idx_for_window[60:63]:\n",
    "    count += 1\n",
    "    print(index)\n",
    "    window_tokens = return_window(index, window_tokens_overall, size=200)[2]\n",
    "    window_text = ' '.join([token.text for token in window_tokens])\n",
    "    print(window_text)\n",
    "    df_sdg = make_sdg_df(sdg_list, sdg_ontology, window_text)\n",
    "    df_sdg_sorted = df_sdg.sort_values(by=['sdg_keywords_num'], ascending=False)\n",
    "    #print(df_sdg_sorted.head())\n",
    "    print(list(df_sdg_sorted[df_sdg_sorted['sdg_keywords_num'] > 0]['sdg_keywords'][:10])) #print top 10 positive valued keywords\n",
    "    #print a summary of the SDG words found: \n",
    "    summary_sdg = df_sdg.groupby('sdg_topic')['sdg_keywords_num'].sum()\n",
    "    if count == 1: \n",
    "        summary_sdg_df = summary_sdg.to_frame().rename(columns={\"sdg_keywords_num\": (\"sdg_kw_%d\"% index)})\n",
    "    else: \n",
    "        summary_sdg_df_for_merge = summary_sdg.to_frame().rename(columns={\"sdg_keywords_num\": (\"sdg_kw_%d\"% index)})\n",
    "        summary_sdg_df = summary_sdg_df.merge(summary_sdg_df_for_merge, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc8169c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:48:44.482914Z",
     "start_time": "2021-11-04T22:48:44.476742Z"
    }
   },
   "outputs": [],
   "source": [
    "#may want to increase the min_distance/set it as a function of window size\n",
    "window_size = 200\n",
    "min_dist = 200\n",
    "idx_for_window = filter_idx_for_overlap(idxs=ndc_climate_idxs, min_dist=min_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c59104",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:50:38.264461Z",
     "start_time": "2021-11-04T22:49:02.540867Z"
    }
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for index in idx_for_window[0:len(idx_for_window)]:\n",
    "    count += 1\n",
    "    print(index)\n",
    "    window_tokens = return_window(index, window_tokens_overall, size=window_size)[2]\n",
    "    window_text = ' '.join([token.text for token in window_tokens])\n",
    "    #print(window_text)\n",
    "    df_sdg = make_sdg_df(sdg_list, sdg_ontology, window_text) #may want to keep this for the windows for a more granular analysis\n",
    "    df_sdg_sorted = df_sdg.sort_values(by=['sdg_keywords_num'], ascending=False)\n",
    "    #print(df_sdg_sorted.head())\n",
    "    print(list(df_sdg_sorted[df_sdg_sorted['sdg_keywords_num'] > 0]['sdg_keywords'][:10])) #print top 10 positive valued keywords\n",
    "    #print a summary of the SDG words found: \n",
    "    summary_sdg = df_sdg.groupby('sdg_topic')['sdg_keywords_num'].sum()\n",
    "    if count == 1: \n",
    "        summary_sdg_df = summary_sdg.to_frame().rename(columns={\"sdg_keywords_num\": (\"sdg_kw_%d\"% index)})\n",
    "    else: \n",
    "        summary_sdg_df_for_merge = summary_sdg.to_frame().rename(columns={\"sdg_keywords_num\": (\"sdg_kw_%d\"% index)})\n",
    "        summary_sdg_df = summary_sdg_df.merge(summary_sdg_df_for_merge, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc621d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:50:38.288112Z",
     "start_time": "2021-11-04T22:50:38.266666Z"
    }
   },
   "outputs": [],
   "source": [
    "summary_sdg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5012a507",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T08:08:44.447055Z",
     "start_time": "2021-11-05T08:08:42.366958Z"
    }
   },
   "outputs": [],
   "source": [
    "summary_sdg_df\n",
    "\n",
    "#make heatmap of plot above\n",
    "plt.figure(figsize=(16, 6))\n",
    "ax = sns.heatmap(summary_sdg_df,\n",
    "                 #annot=True, fmt=\"d\", \n",
    "                 cmap=\"YlGnBu\")\n",
    "plt.xlabel(\"climate change NDC-associated windows\")\n",
    "plt.ylabel(\"SDG Topics\")\n",
    "title = (\"Distribution of topic keywords in climate change NDC-associated windows in %s\" % (doc_name))\n",
    "plt.title(title)\n",
    "file_name=(title + (' across all windows v1.png'))\n",
    "plt.savefig((plot_folder+file_name), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19355a2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:29:03.169869Z",
     "start_time": "2021-11-04T22:29:03.169858Z"
    }
   },
   "outputs": [],
   "source": [
    "#make heatmap of plot above\n",
    "plt.figure(figsize=(16, 6))\n",
    "ax = sns.heatmap(summary_sdg_df,\n",
    "                 #annot=True, fmt=\"d\", \n",
    "                 cmap=\"YlGnBu\")\n",
    "plt.xlabel(\"Windows with NDC words in the document\")\n",
    "plt.ylabel(\"SDG Topics\")\n",
    "title = (\"Distribution of keywords related to different SDGs in NDC-associated windows in the document\")\n",
    "plt.title(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc43662",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T08:50:35.824422Z",
     "start_time": "2021-11-05T08:50:30.550327Z"
    }
   },
   "outputs": [],
   "source": [
    "#make heatmap of plot above\n",
    "plt.figure(figsize=(16, 6))\n",
    "ax = sns.heatmap(summary_sdg_df.iloc[:, 40:100], #can show all windows with summary_sdg_df\n",
    "                 annot=True, fmt=\"d\", \n",
    "                 cmap=\"YlGnBu\")\n",
    "plt.xlabel(\"climate change NDC-associated windows\")\n",
    "plt.ylabel(\"SDG Topics\")\n",
    "title = (\"Distribution of topic keywords in climate change NDC-associated windows in %s\" % (doc_name))\n",
    "plt.title(title)\n",
    "file_name=(title + (' across windows 40-100 v1.png'))\n",
    "plt.savefig((plot_folder+file_name), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734824f6",
   "metadata": {},
   "source": [
    "### Lets see some examples: \n",
    "#### SDG15: \"Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss\"\n",
    "https://sdgs.un.org/goals/goal15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220a4d58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T08:54:34.555993Z",
     "start_time": "2021-11-05T08:54:34.501786Z"
    }
   },
   "outputs": [],
   "source": [
    "index=41020\n",
    "print(return_window(index, window_tokens_overall, size=window_size)[2]) #print(window_tokens[2])\n",
    "summary_sdg_df[('sdg_kw_%d' % index)]\n",
    "print(\"\")\n",
    "index=41340\n",
    "print(return_window(index, window_tokens_overall, size=window_size)[2]) #print(window_tokens[2])\n",
    "summary_sdg_df[('sdg_kw_%d' % index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6b4630",
   "metadata": {},
   "source": [
    "#### SDG14: \"Conserve and sustainably use the oceans, seas and marine resources for sustainable development\"\n",
    "https://sdgs.un.org/goals/goal14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b03bf3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:50:41.267008Z",
     "start_time": "2021-11-04T22:50:41.239511Z"
    }
   },
   "outputs": [],
   "source": [
    "index=55289\n",
    "print(return_window(index, window_tokens_overall, size=window_size)[2]) #print(window_tokens[2])\n",
    "summary_sdg_df[('sdg_kw_%d' % index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a427fa",
   "metadata": {},
   "source": [
    "#### SDG08: \"Promote sustained, inclusive and sustainable economic growth, full and productive employment and decent work for all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf04d556",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:50:41.300930Z",
     "start_time": "2021-11-04T22:50:41.269142Z"
    }
   },
   "outputs": [],
   "source": [
    "index=72837\n",
    "print(return_window(index, window_tokens_overall, size=window_size)[2]) #print(window_tokens[2])\n",
    "summary_sdg_df[('sdg_kw_%d' % index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c691fa",
   "metadata": {},
   "source": [
    "#### SDG11: \"Make cities and human settlements inclusive, safe, resilient and sustainable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6018424d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:50:41.324337Z",
     "start_time": "2021-11-04T22:50:41.302317Z"
    }
   },
   "outputs": [],
   "source": [
    "index=74881\n",
    "print(return_window(index, window_tokens_overall, size=window_size)[2]) #print(window_tokens[2])\n",
    "summary_sdg_df[('sdg_kw_%d' % index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8a3549",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:50:41.355159Z",
     "start_time": "2021-11-04T22:50:41.326148Z"
    }
   },
   "outputs": [],
   "source": [
    "index=241379\n",
    "print(return_window(index, window_tokens_overall, size=window_size)[2]) #print(window_tokens[2])\n",
    "summary_sdg_df[('sdg_kw_%d' % index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6243292e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:51:48.692277Z",
     "start_time": "2021-11-04T22:51:48.668640Z"
    }
   },
   "outputs": [],
   "source": [
    "index=67819\n",
    "print(return_window(index, window_tokens_overall, size=window_size)[2]) #print(window_tokens[2])\n",
    "summary_sdg_df[('sdg_kw_%d' % index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c7511c",
   "metadata": {},
   "source": [
    "### Parts of speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b40fc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:29:03.177541Z",
     "start_time": "2021-11-04T22:29:03.177528Z"
    }
   },
   "outputs": [],
   "source": [
    "for token in filtered_tokens[:50]:\n",
    "    print(token, token.tag_, token.pos_, spacy.explain(token.tag_))\n",
    "print('\\n')\n",
    "    \n",
    "nouns = []\n",
    "adjectives = []\n",
    "for token in filtered_tokens:\n",
    "    if token.pos_ == 'NOUN':\n",
    "        nouns.append(token)\n",
    "    if token.pos_ == 'ADJ':\n",
    "        adjectives.append(token)\n",
    "\n",
    "print('some nouns spacy called in the document include:', '\\n', nouns[:50], '\\n')\n",
    "print('some adjectives spacy called in the document include:', '\\n', adjectives[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b519308c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T11:56:38.400728Z",
     "start_time": "2021-10-09T11:56:38.397730Z"
    }
   },
   "source": [
    "### Entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773f4581",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:29:03.178541Z",
     "start_time": "2021-11-04T22:29:03.178529Z"
    }
   },
   "outputs": [],
   "source": [
    "#can display entity property for the tokens as well: \n",
    "entities=[(i, i.label_, i.label) for i in filtered_tokens[1400:1700].ents]\n",
    "print(entities[:10])\n",
    "\n",
    "token_subset = tokens[100:500]\n",
    "displacy.render(token_subset, style = \"ent\", jupyter = True) #use original tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d245d4",
   "metadata": {},
   "source": [
    "### Dependency visualization in sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24796c38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:29:03.179664Z",
     "start_time": "2021-11-04T22:29:03.179653Z"
    }
   },
   "outputs": [],
   "source": [
    "sent_example = nlp(\"For example, it is estimated that between 9 and 12 million DATE people in impoverished rural areas directly use natural resources such as fuel wood, wild fruits and wooden utensils as a source of energy, food and building material respectively (Shackleton ORG 2004)\")\n",
    "sentence_spans = list(sentences)\n",
    "sentence_spans[:10]\n",
    "displacy.render(sentence_spans[80], style=\"dep\", jupyter= True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "75547552c603f22400c6f6e0e4ad2ade15359435e000e6746c53241e37331409"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
