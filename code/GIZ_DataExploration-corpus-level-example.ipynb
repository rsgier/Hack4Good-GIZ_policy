{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "704be11d",
   "metadata": {},
   "source": [
    "### GIZ Initial Data Exploration\n",
    "#### author: Emily Robitschek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ca671e",
   "metadata": {},
   "source": [
    "Purpose: Before we build any model, we need to be able to take a look at the documents we have.\n",
    "\n",
    "Some resources: \n",
    "\n",
    "#### papers mentioned in project proposal: \n",
    "https://medium.com/fiscalnoteworthy/citing-your-sources-79062248f468\n",
    "https://www2.deloitte.com/us/en/insights/focus/cognitive-technologies/natural-language-processing-examples-in-government-data.html\n",
    "https://documents1.worldbank.org/curated/en/634591516387264234/pdf/WPS8310.pdf\n",
    "\n",
    "#### NLP related links: \n",
    "- https://spacy.io/usage/spacy-101\n",
    "- https://towardsdatascience.com/text-pre-processing-stop-words-removal-using-different-libraries-f20bac19929a\n",
    "- https://arunm8489.medium.com/getting-started-with-natural-language-processing-6e593e349675\n",
    "- https://towardsdatascience.com/natural-language-processing-pipeline-decoded-f97a4da5dbb7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a581e19b",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2d30a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:27.718385Z",
     "start_time": "2021-11-04T22:28:25.061763Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import json\n",
    "import codecs\n",
    "import re\n",
    "\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#set up packages for processing data types and for NLP analysis\n",
    "from collections import OrderedDict, Counter\n",
    "import contractions\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm') #or the multi-language one: spacy.load('xx_ent_wiki_sm')\n",
    "\n",
    "#from n_gram_correlation import NGramCorrelateSpacy\n",
    "\n",
    "#graphing/visualization packages: \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334ff084",
   "metadata": {},
   "source": [
    "### Define helper functions for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbb2783",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:28.114906Z",
     "start_time": "2021-11-04T22:28:27.720531Z"
    }
   },
   "outputs": [],
   "source": [
    "from datahelper import *\n",
    "from nlppreprocess import *\n",
    "from nlpanalysis import *\n",
    "from n_gram_correlation import *\n",
    "from textutils import importer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e16b60",
   "metadata": {},
   "source": [
    "### Import data: Keywords from NDCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6accc9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:28.158053Z",
     "start_time": "2021-11-04T22:28:28.116076Z"
    }
   },
   "outputs": [],
   "source": [
    "#keywords (just for testing purposes - from the policy proposal from GIZ)\n",
    "policy = [\"policy\", \"integrate\", \"implement\", \"committee\", \"consultation\"]\n",
    "food = [\"nutritions\", \"diets\", \"farm\", \"agriculture\", \"ecology\"]\n",
    "\n",
    "#keywords from json file\n",
    "json_keywords_SA_file = '../ndc_keywords/ndc_south_africa.json'\n",
    "keywords_SA_dict = None\n",
    "with open(json_keywords_SA_file, 'r') as f: \n",
    "    keywords_SA_dict = json.load(f)\n",
    "    \n",
    "    \n",
    "json_keywords_Ethiopia_file = '../ndc_keywords/ndc_ethiopia.json'\n",
    "keywords_Ethiopia_dict = None\n",
    "with open(json_keywords_Ethiopia_file, 'r') as f: \n",
    "    keywords_Ethiopia_dict = json.load(f)\n",
    "    \n",
    "print(keywords_SA_dict)\n",
    "print(keywords_Ethiopia_dict)\n",
    "#ndc_dict = make_filtered_tokens_from_ndc(keywords_SA_dict) #if want to use filtered, lemmatized tokens\n",
    "ndc_dict = keywords_SA_dict\n",
    "#ndc_dict = keywords_Ethiopia_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc7f429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets apply these functions to change the format of the NDC dictionary to a dataframe: \n",
    "def make_ndc_keyword_tidy_df_from_dict(key, col_group_name, ndc_dict):\n",
    "    ndc_df = pd.DataFrame({'keyword': ndc_dict[key], \n",
    "                       col_group_name: key})\n",
    "    return ndc_df\n",
    "\n",
    "def stack_tidy_ndc_dfs(col_group_name, ndc_dict):\n",
    "    ndc_df = pd.DataFrame()\n",
    "    for key in ndc_dict.keys():\n",
    "        ndc_df_add = make_ndc_keyword_tidy_df_from_dict(key, col_group_name, ndc_dict)\n",
    "        ndc_df = pd.concat([ndc_df, ndc_df_add], axis=0)\n",
    "    return ndc_df\n",
    "\n",
    "col_group_name = 'NDC'\n",
    "ndc_df = stack_tidy_ndc_dfs(col_group_name, ndc_dict)\n",
    "ndc_df.head()\n",
    "\n",
    "\n",
    "\n",
    "def return_window(ndc_word_index, tokens, size=100):\n",
    "    \"\"\"Returns a section of text around an NDC word, but also strips out some of the document information \n",
    "    because of the conversion of tokens to text and back. This window is used ot  \n",
    "    so that non-overlapping windows are generated to look for the keyword topics\"\"\"\n",
    "    lower_limit = ndc_word_index - size\n",
    "    upper_limit = ndc_word_index + size\n",
    "    token_idxs = [token.idx for token in tokens]\n",
    "    window_token_list = []\n",
    "    #print('The window is ', lower_limit, upper_limit)\n",
    "    for index, idx in enumerate(token_idxs):\n",
    "        if (idx >= lower_limit) and (idx <= upper_limit):\n",
    "            window_token_list.append(tokens[index])\n",
    "        else:\n",
    "            pass\n",
    "    text_for_windows = ' '.join(list(token.text for token in window_token_list))\n",
    "    window_tokens = nlp(text_for_windows)\n",
    "    return lower_limit, upper_limit, window_tokens\n",
    "\n",
    "\n",
    "def make_ndc_idx_df_from_spans(ndc_keywords, labelled_doc): \n",
    "    \"\"\"ndc_keywords is the ndc keyword dictionary and the labelled_doc is the one labelled with the spans of NDC keywords\"\"\"\n",
    "    ndc_idx_df = pd.DataFrame()\n",
    "    for entity_reference in [key for key in ndc_keywords.keys()]:\n",
    "        entity_label = entity_reference + ' NDC'\n",
    "        #print(entity_label) \n",
    "        ndc_idx_df_to_add = pd.DataFrame({'NDC': entity_reference,\n",
    "                                          ('word'): [ent.text for ent in labelled_doc.ents if ent.label_ == entity_label],\n",
    "                                          'word_index': [ent.start for ent in labelled_doc.ents if ent.label_ == entity_label]})\n",
    "        ndc_idx_df = pd.concat([ndc_idx_df, ndc_idx_df_to_add], axis=0)\n",
    "    return ndc_idx_df \n",
    "\n",
    "def label_ndc_spans(ndc_keywords, doc):\n",
    "    \"\"\"ndc_keywords is a dictionary, doc is the document text with nlp run on it that is the filtered but not lemmatized document text.\"\"\"\n",
    "    matcher = PhraseMatcher(nlp.vocab)\n",
    "    #iterate through NDC keys in NDC dictionary to create seperate label categories for the matching\n",
    "    entity_labels = []\n",
    "    for entity_reference in [key for key in ndc_keywords.keys()]:\n",
    "        entity_label = entity_reference + ' NDC'\n",
    "        #print(entity_label)\n",
    "        entity_labels.append(entity_label)\n",
    "        keywords = ndc_keywords[entity_reference]\n",
    "        patterns = [nlp(i) for i in keywords]\n",
    "        #print(keywords, patterns)\n",
    "        matcher.add(entity_label, None, *patterns)\n",
    "    matches = matcher(doc)\n",
    "        #label spans\n",
    "    for match_id, start, end in matches:\n",
    "        try:\n",
    "            span = Span(doc, start, end, label=match_id)\n",
    "            doc.ents = list(doc.ents) + [span]  # add span to doc.ents\n",
    "        except: \n",
    "            pass\n",
    "    #print([(ent.text, ent.start, ent.label_) for ent in doc.ents])\n",
    "    return entity_labels, doc\n",
    "\n",
    "def label_ndc_spans_correlated(ndc_keywords, doc, span_length=2, corr_thresh=0.7):\n",
    "    \"\"\"ndc_keywords is a dictionary, doc is the document text with nlp run on it that is the filtered but not lemmatized document text.\"\"\"\n",
    "    matcher = PhraseMatcher(nlp.vocab)\n",
    "    #iterate through NDC keys in NDC dictionary to create seperate label categories for the matching\n",
    "    entity_labels = []\n",
    "    ndc_dict_corr = dict()\n",
    "    for entity_reference in [key for key in ndc_keywords.keys()]:\n",
    "        entity_label = entity_reference + ' NDC'\n",
    "        #print(entity_label)\n",
    "        entity_labels.append(entity_label)\n",
    "        keywords = ndc_keywords[entity_reference]\n",
    "        n_gram_cor = NGramCorrelateSpacy(keywords, corr_thresh, entity_label)\n",
    "        doc.ents = []\n",
    "        n_gram_cor.correlate_spans(doc, span_length)\n",
    "        print(len(doc.ents), len(set([ent.text for ent in list(doc.ents)])))\n",
    "        unique_keywords_from_corr = list(set([ent.text for ent in list(doc.ents)]))\n",
    "        ndc_dict_corr[entity_reference] = unique_keywords_from_corr\n",
    "        patterns = [nlp(i) for i in unique_keywords_from_corr]\n",
    "        #print(keywords, patterns)\n",
    "        matcher.add(entity_label, None, *patterns)\n",
    "    matches = matcher(doc)\n",
    "        #label spans\n",
    "    for match_id, start, end in matches:\n",
    "        try:\n",
    "            span = Span(doc, start, end, label=match_id)\n",
    "            doc.ents = list(doc.ents) + [span]  # add span to doc.ents\n",
    "        except: \n",
    "            pass\n",
    "    #print([(ent.text, ent.start, ent.label_) for ent in doc.ents])\n",
    "    return entity_labels, ndc_dict_corr, doc\n",
    "\n",
    "def find_patterns_df(pattern_list, text, topic_name):\n",
    "    \"\"\"This function takes in a list of keywords/terms and uses re exact matching to find them in the text \n",
    "    and returns a dataframe of their start locations, topics, and how many times they were found\"\"\"\n",
    "    pattern_locations = []\n",
    "    pattern_num = []\n",
    "    for pattern in pattern_list:\n",
    "        #print(pattern)\n",
    "        re.findall(pattern, text, flags=0)\n",
    "        #pattern_locations = [(m.start(0), m.end(0)) for m in re.finditer(pattern, text)] #if want start and end\n",
    "        locations = [m.start(0) for m in re.finditer(pattern, text)]\n",
    "        pattern_locations.append(locations)\n",
    "        pattern_num.append(int(len(locations)))\n",
    "    #print(pattern_locations)\n",
    "        #if len(pattern_locations) > 0: \n",
    "        #    print(pattern, len(pattern_locations), pattern_locations)\n",
    "    return pd.DataFrame({'sdg_topic': topic_name,\n",
    "                         'sdg_keywords': pattern_list,\n",
    "                         'sdg_keywords_num': pattern_num,\n",
    "                         'sdg_keyword_locations': pattern_locations})\n",
    "\n",
    "def make_sdg_df(sdg_list, sdg_ontology, text):\n",
    "    df_sdg = pd.DataFrame()\n",
    "    for sdg in list(sdg_list):\n",
    "        sdg_keywords = list(sdg_ontology[sdg_ontology['clasification']==sdg]['keyword'])\n",
    "        #print(sdg)\n",
    "        df_sdg_to_add = find_patterns_df(sdg_keywords, text, topic_name=sdg)\n",
    "        df_sdg = pd.concat([df_sdg, df_sdg_to_add])\n",
    "    return df_sdg\n",
    "\n",
    "\n",
    "def run_nlp_pipeline_no_lemma(doc_name, doc_path, ndc_dict, max_word_length):\n",
    "    \"\"\"This version of the pipeline employs the simple single word matching for finding NDC keywords, \n",
    "    and re based matching for the keyword topic keywords. \"\"\"\n",
    "    print(doc_name, doc_path)\n",
    "    tokens, token_list, sentences = preprocess_doc(doc_path)\n",
    "    filtered_tokens = filter_tokens(tokens)\n",
    "    print('These are some of the filtered tokens: ', filtered_tokens[0:10])\n",
    "    document_text = ' '.join([token.text for token in filtered_tokens if len(token.text)<=max_word_length])\n",
    "    doc = nlp(document_text)\n",
    "    ndc_labels, labelled_doc = label_ndc_spans(ndc_dict, doc)\n",
    "    ndc_idx_df = make_ndc_idx_df_from_spans(ndc_dict, labelled_doc)\n",
    "    ndc_idx_df.index = ndc_idx_df.NDC.copy()\n",
    "    ndc_summary_df = ndc_idx_df['NDC'].value_counts().to_frame().rename(columns={\"NDC\": (doc_name)})\n",
    "    df_sdg = make_sdg_df(sdg_list, sdg_ontology, document_text)\n",
    "    doc_summary_sdg = df_sdg.groupby('sdg_topic')['sdg_keywords_num'].sum()\n",
    "    doc_summary_sdg_df = doc_summary_sdg.to_frame().rename(columns={\"sdg_keywords_num\": doc_name})\n",
    "    doc_summary_df = pd.concat([doc_summary_sdg_df, ndc_summary_df]) #combine the SDG and NDC information into one df\n",
    "    return tokens, token_list, sentences, filtered_tokens, document_text, ndc_idx_df, ndc_summary_df, df_sdg, doc_summary_sdg_df, doc_summary_df #topic_frequencies, sentences\n",
    "\n",
    "\n",
    "def run_nlp_pipeline_no_lemma_ndc_corr(doc_name, doc_path, ndc_dict, max_word_length):\n",
    "    \"\"\"This version of the pipeline incorporates the fuzzy search method that Johnathan wrote, \n",
    "    and re based matching for the keyword topic keywords.\"\"\"\n",
    "    print(doc_name, doc_path)\n",
    "    tokens, token_list, sentences = preprocess_doc(doc_path)\n",
    "    filtered_tokens = filter_tokens(tokens)\n",
    "    print('These are some of the filtered tokens: ', filtered_tokens[0:10])\n",
    "    document_text = ' '.join([token.text for token in filtered_tokens if len(token.text)<=max_word_length])\n",
    "    doc = nlp(document_text)\n",
    "    ndc_labels, ndc_dict_corr, labelled_doc = label_ndc_spans_correlated(ndc_dict, doc, span_length=2, corr_thresh=0.7)\n",
    "    ndc_idx_df = make_ndc_idx_df_from_spans(ndc_dict_corr, labelled_doc)\n",
    "    ndc_idx_df.index = ndc_idx_df.NDC.copy()\n",
    "    ndc_summary_df = ndc_idx_df['NDC'].value_counts().to_frame().rename(columns={\"NDC\": (doc_name)})\n",
    "    df_sdg = make_sdg_df(sdg_list, sdg_ontology, document_text)\n",
    "    doc_summary_sdg = df_sdg.groupby('sdg_topic')['sdg_keywords_num'].sum()\n",
    "    doc_summary_sdg_df = doc_summary_sdg.to_frame().rename(columns={\"sdg_keywords_num\": doc_name})\n",
    "    doc_summary_df = pd.concat([doc_summary_sdg_df, ndc_summary_df]) #combine the SDG and NDC information into one df\n",
    "    return tokens, token_list, sentences, filtered_tokens, document_text, ndc_dict_corr, ndc_idx_df, ndc_summary_df, df_sdg, doc_summary_sdg_df, doc_summary_df #topic_frequencies, sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4857372",
   "metadata": {},
   "source": [
    "### SDG and other topic keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf61b2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdg_ontology = pd.read_csv('../additional_resources/Ontology_final_modified.csv', sep=';')#, #skiprows=0)\n",
    "sdg_list = ['SDG1', 'SDG2', 'SDG3', 'SDG4', 'SDG5', 'SDG6', 'SDG7', 'SDG8', 'SDG9', \n",
    "            'SDG10', 'SDG11', 'SDG12', 'SDG13', 'SDG14', 'SDG15', 'SDG16', 'SDG17', \n",
    "            \"mention_money\"]\n",
    "\n",
    "sdg_ontology.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f87631",
   "metadata": {},
   "source": [
    "### Import data: Policy-related documents to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df47609",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:28.176885Z",
     "start_time": "2021-11-04T22:28:28.167255Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#here is the general folder with the different types of policy documents\n",
    "policy_doc_folder = '../test_resources/data'\n",
    "#policy_doc_folder = '../../selected_policy_tracking_docs/Ethiopia/'\n",
    "#get df of docs\n",
    "policy_doc_df = read_docs_to_df(policy_doc_folder)\n",
    "print(len(policy_doc_df))\n",
    "#policy_doc_df['policy_doc_name_clean'] = (policy_doc_df.index.apply(lambda x: x.split('.txt')[0].split('.pdf.ocr')[0]))\n",
    "policy_doc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57d7cae",
   "metadata": {},
   "source": [
    "### Preprocess data and run pipeline on a set of documents in series of folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdd64a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:35.772740Z",
     "start_time": "2021-11-04T22:28:28.178107Z"
    }
   },
   "outputs": [],
   "source": [
    "##lets take a look at the document from South Africa used in the example in the proposal first:\n",
    "#doc_name = '2nd National Biodiversity Strategy Action Plan 2015-2025.txt'\n",
    "#doc_path = policy_doc_df.loc[doc_name]['policy_doc_paths']\n",
    "\n",
    "max_word_length=25\n",
    "corpus_summary_df = pd.DataFrame()\n",
    "count = 1\n",
    "for i in range(0, 2): #len(policy_doc_df)\n",
    "    doc_name = policy_doc_df.iloc[i]['policy_doc_name_clean']\n",
    "    doc_path = policy_doc_df.iloc[i]['policy_doc_paths']\n",
    "    tokens, token_list, sentences, filtered_tokens, document_text, ndc_idx_df, ndc_summary_df, df_sdg, doc_summary_sdg_df, doc_summary_df = run_nlp_pipeline_no_lemma(doc_name, doc_path, ndc_dict, max_word_length) \n",
    "    print(ndc_idx_df.NDC.value_counts()) \n",
    "    if count == 1: \n",
    "        corpus_summary_df = doc_summary_df\n",
    "        total = doc_summary_df[doc_name].sum()\n",
    "        print(doc_name, len(filtered_tokens), total)\n",
    "        count += 1\n",
    "    else: \n",
    "        corpus_summary_df_for_merge = doc_summary_df\n",
    "        total = doc_summary_df[doc_name].sum()\n",
    "        print(doc_name, len(filtered_tokens), total)\n",
    "        corpus_summary_df = corpus_summary_df.merge(corpus_summary_df_for_merge, how='outer', left_index=True, right_index=True)\n",
    "        count += 1\n",
    "corpus_summary_df.head()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67054128",
   "metadata": {},
   "source": [
    "### Can fill na values and export the corpus summary as a txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8766b4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example: \n",
    "corpus_summary_df = corpus_summary_df.fillna(0)\n",
    "#output_name = '../../outputs/SA_spacy_matching_NDC_SDGs_df.txt'\n",
    "#corpus_summary_df.to_csv('../../outputs/SouthAfrica_spacy_matching_NDC_SDGs_corpus_df.txt', sep='\\t')\n",
    "\n",
    "#test_df = pd.read_csv('../../outputs/SouthAfrica_spacy_matching_NDC_SDGs_corpus_df.txt', \n",
    "#                      index_col='Unnamed: 0', sep='\\t')\n",
    "#test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e38b1b",
   "metadata": {},
   "source": [
    "### Make document level heatmaps to visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6805f8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this normalization method has drawbacks as it depends on the prevalence of other categories which may not be desired.\n",
    "#It would be better to normalize by number of filtered tokens or number of tokens per document.\n",
    "normed_by_col_sum = corpus_summary_df.div(corpus_summary_df.sum(axis=0), axis=1).round(3) \n",
    "sorted_df = normed_by_col_sum.T.sort_values(by=['climate change'], ascending=False) #ndc south africa example\n",
    "#sorted_df = normed_by_col_sum.T.sort_values(by=['Fairness, equity, ambition'], ascending=False) #ndc ethiopia example\n",
    "normed_by_col_sum.head()\n",
    "\n",
    "normed_by_col_sum_corr = corpus_summary_df.div(corpus_summary_df.sum(axis=0), axis=1).round(3)\n",
    "sorted_df_corr = normed_by_col_sum_corr.T.sort_values(by=['climate change'], ascending=False)\n",
    "#normed_by_col_sum_corr.head()\n",
    "\n",
    "corpus_summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390f3290",
   "metadata": {},
   "source": [
    "## Can look at multiple documents at once and see what topics are prevalent and prioritize them with a heatmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1890b8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6)) #16, 6\n",
    "ax = sns.heatmap(normed_by_col_sum,\n",
    "                 #annot=True, fmt=\"d\", \n",
    "                 cmap=\"YlGnBu\")\n",
    "plt.xlabel(\"Documents\")\n",
    "plt.ylabel(\"Topics\")\n",
    "title = (\"Topic keywords in the documents (normalized)\")\n",
    "plt.title(title)\n",
    "file_name=(title + (' some docs SA.png'))\n",
    "#plt.savefig((plot_folder+file_name), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e395b1",
   "metadata": {},
   "source": [
    "## Can use clustering of the heatmap results to see how documents that are related by source or time cluster by the topics and ndcs present within them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5bed92",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "ax = sns.clustermap(normed_by_col_sum,\n",
    "                 #annot=True,\n",
    "                 cbar_pos=(1.0, .45, .03, .2),\n",
    "                 cmap=\"YlGnBu\", \n",
    "                 xticklabels=1, \n",
    "                 yticklabels=1, \n",
    "                 figsize=(5, 15))\n",
    "#plt.xlabel(\"Documents\")\n",
    "#plt.ylabel(\"SDG Topics\")\n",
    "title = (\"Distribution of topic and NDC keywords in the documents (normalized)\")\n",
    "#plt.title(title)\n",
    "file_name=(title + (' clustermap 10 docs SA v2.png'))\n",
    "#plt.savefig((plot_folder+file_name), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b894bf9",
   "metadata": {},
   "source": [
    "## If using the embedding/correlator on the documents, a slightly different version of the pipeline is used: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75a5452",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if using Johnathan's fuzzy search:\n",
    "#doc_name = '2nd National Biodiversity Strategy Action Plan 2015-2025.txt'\n",
    "#doc_path = policy_doc_df.loc[doc_name]['policy_doc_paths']\n",
    "\n",
    "max_word_length=25\n",
    "corpus_summary_df = pd.DataFrame()\n",
    "count = 1\n",
    "for i in range(0, len(policy_doc_df[:2])):\n",
    "    doc_name = policy_doc_df.iloc[i]['policy_doc_name_clean']\n",
    "    doc_path = policy_doc_df.iloc[i]['policy_doc_paths']\n",
    "    tokens, token_list, sentences, filtered_tokens, document_text, ndc_dict_corr, ndc_idx_df, ndc_summary_df, df_sdg, doc_summary_sdg_df, doc_summary_df = run_nlp_pipeline_no_lemma_ndc_corr(doc_name, doc_path, ndc_dict, max_word_length) \n",
    "    print(ndc_idx_df.NDC.value_counts()) \n",
    "    if count == 1: \n",
    "        corpus_summary_df = doc_summary_df\n",
    "        total = doc_summary_df[doc_name].sum()\n",
    "        print(doc_name, len(filtered_tokens), total)\n",
    "        count += 1\n",
    "    else: \n",
    "        corpus_summary_df_for_merge = doc_summary_df\n",
    "        total = doc_summary_df[doc_name].sum()\n",
    "        print(doc_name, len(filtered_tokens), total)\n",
    "        corpus_summary_df = corpus_summary_df.merge(corpus_summary_df_for_merge, how=\"outer\", left_index=True, right_index=True)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa4d702",
   "metadata": {},
   "source": [
    "### Make a df and initial representation of the data with the correlation/ngram method for finding NDC words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3953d5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_summary_df = corpus_summary_df.fillna(0)\n",
    "corpus_summary_df_corr = corpus_summary_df\n",
    "corpus_summary_df_corr.head()\n",
    "\n",
    "#output_name = '../../outputs/SA_corr_matching_NDC_SDGs_10_doc_corpus_df.txt'\n",
    "#corpus_summary_df.to_csv(output_name, sep='\\t')\n",
    "\n",
    "normed_by_col_sum_corr = corpus_summary_df_corr.div(corpus_summary_df_corr.sum(axis=0), axis=1).round(3)\n",
    "sorted_df_corr = normed_by_col_sum_corr.T.sort_values(by=['climate change'], ascending=False)\n",
    "normed_by_col_sum_corr.head()\n",
    "\n",
    "plt.figure(figsize=(8, 6)) #16, 6\n",
    "ax = sns.heatmap(sorted_df_corr,\n",
    "                 #annot=True, #fmt=\"d\", \n",
    "                 cmap=\"YlGnBu\")\n",
    "#plt.xlabel(\"Documents\")\n",
    "#plt.ylabel(\"Topics\")\n",
    "#ax.yaxis.tick_right()\n",
    "#ax.yaxis.set_label_position(\"right\")\n",
    "title = (\"NDC and custom topic keywords in the sorted documents\")\n",
    "plt.title(title)\n",
    "file_name=(title + (' (with correlated NDCs).png'))\n",
    "#plt.savefig((plot_folder+file_name), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "75547552c603f22400c6f6e0e4ad2ade15359435e000e6746c53241e37331409"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
