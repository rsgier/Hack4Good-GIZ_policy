{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "704be11d",
   "metadata": {},
   "source": [
    "### GIZ Initial Data Exploration\n",
    "#### author: Emily Robitschek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ca671e",
   "metadata": {},
   "source": [
    "Purpose: Before we build any model, we need to be able to take a look at the documents we have.\n",
    "\n",
    "Some resources: \n",
    "\n",
    "#### papers mentioned in project proposal: \n",
    "https://medium.com/fiscalnoteworthy/citing-your-sources-79062248f468\n",
    "https://www2.deloitte.com/us/en/insights/focus/cognitive-technologies/natural-language-processing-examples-in-government-data.html\n",
    "https://documents1.worldbank.org/curated/en/634591516387264234/pdf/WPS8310.pdf\n",
    "\n",
    "#### NLP related links: \n",
    "- https://spacy.io/usage/spacy-101\n",
    "- https://towardsdatascience.com/text-pre-processing-stop-words-removal-using-different-libraries-f20bac19929a\n",
    "- https://arunm8489.medium.com/getting-started-with-natural-language-processing-6e593e349675\n",
    "- https://towardsdatascience.com/natural-language-processing-pipeline-decoded-f97a4da5dbb7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a581e19b",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2d30a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:27.718385Z",
     "start_time": "2021-11-04T22:28:25.061763Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import json\n",
    "import codecs\n",
    "import re\n",
    "\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#set up packages for processing data types and for NLP analysis\n",
    "from collections import OrderedDict, Counter\n",
    "import contractions\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_sm') #or the multi-language one: spacy.load('xx_ent_wiki_sm')\n",
    "\n",
    "#graphing/visualization packages: \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334ff084",
   "metadata": {},
   "source": [
    "### Define helper functions for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbb2783",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:28.114906Z",
     "start_time": "2021-11-04T22:28:27.720531Z"
    }
   },
   "outputs": [],
   "source": [
    "from datahelper import *\n",
    "from nlppreprocess import *\n",
    "from nlpanalysis import *\n",
    "from n_gram_correlation import *\n",
    "from correlation import *\n",
    "from textutils import importer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e16b60",
   "metadata": {},
   "source": [
    "### Import data: Keywords from NDCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6accc9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:28.158053Z",
     "start_time": "2021-11-04T22:28:28.116076Z"
    }
   },
   "outputs": [],
   "source": [
    "#keywords (just for testing purposes - from the policy proposal from GIZ)\n",
    "policy = [\"policy\", \"integrate\", \"implement\", \"committee\", \"consultation\"]\n",
    "food = [\"nutritions\", \"diets\", \"farm\", \"agriculture\", \"ecology\"]\n",
    "ndc_national_adaption_plan = [\"nap\", \"sector plan\", \"nccrp\", \"vulnerable sector\", \n",
    "                              \"geographic vulnerability\"]\n",
    "ndc_climate_change = [\"adaption\", \"program\", \"projects\", \"resilience\", \"institution\",\n",
    "                      \"capacity\", \"response\", \"budget\", \"reprioritisation\", \"development\", \n",
    "                      \"planner\", \"regulator\", \"practitioners\", \"geographical\", \n",
    "                      \"circumstances\", \"land\", \"scheme\", \"authorisation\", \"system\", \n",
    "                      \"spluma\"]\n",
    "ndc_early_warning = [\"system\", \"vulnerability\", \"needs\", \"assessment\", \"network\", \"weather\",\n",
    "   \"earth\", \"observation\", \"academic\", \"community\"]\n",
    "\n",
    "#keywords from json file\n",
    "json_keywords_SA_file = '../ndc_keywords/ndc_south_africa.json'\n",
    "keywords_SA_dict = None\n",
    "with open(json_keywords_SA_file, 'r') as f: \n",
    "    keywords_SA_dict = json.load(f)\n",
    "keywords_SA_dict\n",
    "\n",
    "ndc_dict = make_filtered_tokens_from_ndc(keywords_SA_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e306e11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:28.165958Z",
     "start_time": "2021-11-04T22:28:28.160530Z"
    }
   },
   "outputs": [],
   "source": [
    "### THIS IS A PLACEHOLDER ###\n",
    "#get words directly from NDCs (eventually want to get words from the NDC itself and group them by topics.)\n",
    "#\n",
    "#\n",
    "#\n",
    "ndc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc7f429",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extra functions\n",
    "def make_ndc_keyword_tidy_df_from_dict(key, col_group_name, ndc_dict):\n",
    "    ndc_df = pd.DataFrame({'keyword': ndc_dict[key], \n",
    "                       col_group_name: key})\n",
    "    return ndc_df\n",
    "\n",
    "def stack_tidy_ndc_dfs(col_group_name, ndc_dict):\n",
    "    ndc_df = pd.DataFrame()\n",
    "    for key in ndc_dict.keys():\n",
    "        ndc_df_add = make_ndc_keyword_tidy_df_from_dict(key, col_group_name, ndc_dict)\n",
    "        ndc_df = pd.concat([ndc_df, ndc_df_add], axis=0)\n",
    "    return ndc_df\n",
    "\n",
    "# lets apply: \n",
    "col_group_name = 'NDC'\n",
    "ndc_df = stack_tidy_ndc_dfs(col_group_name, ndc_dict)\n",
    "ndc_df.head()\n",
    "\n",
    "\n",
    "def make_ndc_idx_tidy_df(ndc_dict, topic_name, tokens):\n",
    "    ndc_idx_df = pd.DataFrame()\n",
    "    for key in ndc_dict.keys(): \n",
    "        ndc_idx_df_to_add = pd.DataFrame({topic_name: key,\n",
    "                                          #('%s word_index'%(key)): [token.idx for token in tokens if token.text in ndc_dict[key]],\n",
    "                                          'word_index': [token.idx for token in tokens if token.text in ndc_dict[key]]})\n",
    "        ndc_idx_df = pd.concat([ndc_idx_df, ndc_idx_df_to_add], axis=0)\n",
    "    return ndc_idx_df\n",
    "\n",
    "def filter_idx_for_overlap(idxs, min_dist):\n",
    "    distance_btwn_idxs = [(idxs[i+1]-idxs[i]) for i in range(0, len(idxs)-1)]\n",
    "    print(distance_btwn_idxs[:20])\n",
    "    filtered_idxs = []\n",
    "    for index, distance in enumerate(distance_btwn_idxs):\n",
    "        if (distance >= min_dist):\n",
    "            filtered_idxs.append(idxs[index])\n",
    "        else:\n",
    "            pass\n",
    "    print(\"The number of times the idx words were found was: \", len(idxs), \"\\n\", \n",
    "          \"The number of idx words seperated by at least the min_distance was : \", len(filtered_idxs))\n",
    "    return filtered_idxs\n",
    "    \n",
    "def make_window_text(tokens, max_length):\n",
    "    filtered_for_length = [token.text.lower() for token in tokens if len(token) < max_length]\n",
    "    text_for_windows = ' '.join(filtered_for_length)\n",
    "    window_tokens = nlp(text_for_windows)\n",
    "    return window_tokens\n",
    "\n",
    "def return_window(ndc_word_index, tokens, size=100):\n",
    "    lower_limit = ndc_word_index - size\n",
    "    upper_limit = ndc_word_index + size\n",
    "    token_idxs = [token.idx for token in tokens]\n",
    "    window_token_list = []\n",
    "    #print('The window is ', lower_limit, upper_limit)\n",
    "    for index, idx in enumerate(token_idxs):\n",
    "        if (idx >= lower_limit) and (idx <= upper_limit):\n",
    "            window_token_list.append(tokens[index])\n",
    "        else:\n",
    "            pass\n",
    "    text_for_windows = ' '.join(list(token.text for token in window_token_list))\n",
    "    window_tokens = nlp(text_for_windows)\n",
    "    return lower_limit, upper_limit, window_tokens\n",
    "\n",
    "def find_patterns_df(pattern_list, text, topic_name):\n",
    "    pattern_locations = []\n",
    "    pattern_num = []\n",
    "    for pattern in pattern_list:\n",
    "        #print(pattern)\n",
    "        re.findall(pattern, text, flags=0)\n",
    "        #pattern_locations = [(m.start(0), m.end(0)) for m in re.finditer(pattern, text)] #if want start and end\n",
    "        locations = [m.start(0) for m in re.finditer(pattern, text)]\n",
    "        pattern_locations.append(locations)\n",
    "        pattern_num.append(int(len(locations)))\n",
    "    #print(pattern_locations)\n",
    "        #if len(pattern_locations) > 0: \n",
    "        #    print(pattern, len(pattern_locations), pattern_locations)\n",
    "    return pd.DataFrame({'sdg_topic': topic_name,\n",
    "                         'sdg_keywords': pattern_list,\n",
    "                         'sdg_keywords_num': pattern_num,\n",
    "                         'sdg_keyword_locations': pattern_locations})\n",
    "\n",
    "def make_sdg_df(sdg_list, sdg_ontology, text):\n",
    "    df_sdg = pd.DataFrame()\n",
    "    for sdg in list(sdg_list):\n",
    "        sdg_keywords = list(sdg_ontology[sdg_ontology['clasification']==sdg]['keyword'])\n",
    "        #print(sdg)\n",
    "        df_sdg_to_add = find_patterns_df(sdg_keywords, text, topic_name=sdg)\n",
    "        df_sdg = pd.concat([df_sdg, df_sdg_to_add])\n",
    "    return df_sdg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4857372",
   "metadata": {},
   "source": [
    "### SDG and other topic keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf61b2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdg_ontology = pd.read_csv('../additional_resources/Ontology_final_modified.csv', sep=';')#, #skiprows=0)\n",
    "\n",
    "sdg_list = ['SDG1', 'SDG2', 'SDG3', 'SDG4', 'SDG5', 'SDG6', 'SDG7', 'SDG8', 'SDG9', \n",
    "            'SDG10', 'SDG11', 'SDG12', 'SDG13', 'SDG14', 'SDG15', 'SDG16', 'SDG17', \n",
    "            \"mention_money\"]\n",
    "\n",
    "sdg_ontology.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f87631",
   "metadata": {},
   "source": [
    "### Import data: Policy-related documents to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df47609",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:28.176885Z",
     "start_time": "2021-11-04T22:28:28.167255Z"
    }
   },
   "outputs": [],
   "source": [
    "#here is the general folder with the different types of policy documents\n",
    "policy_doc_folder = '../test_resources/data'\n",
    "#get df of docs\n",
    "policy_doc_df = read_docs_to_df(policy_doc_folder)\n",
    "print(len(policy_doc_df))\n",
    "policy_doc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57d7cae",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdd64a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:35.772740Z",
     "start_time": "2021-11-04T22:28:28.178107Z"
    }
   },
   "outputs": [],
   "source": [
    "#can also simply specify the first or whichever document in the list by order: \n",
    "#(this will be helpful for future versions that involve more interrating over different documents)\n",
    "i=53\n",
    "#doc_name = policy_doc_df.iloc[i]['policy_doc_name_clean']\n",
    "#doc_path = policy_doc_df.iloc[i]['policy_doc_paths']\n",
    "\n",
    "def run_nlp_pipeline(doc_name, doc_path, ndc_dict, max_word_length):\n",
    "    print(doc_name, doc_path)\n",
    "    tokens, token_list, sentences = preprocess_doc(doc_path)\n",
    "    filtered_tokens = filter_modify_tokens(tokens)\n",
    "    print('These are some of the filtered tokens: ', filtered_tokens[0:10])\n",
    "    #words = [token.text for token in filtered_tokens]\n",
    "    #word_freq = Counter(words)\n",
    "    #for key in ndc_dict.keys():\n",
    "    #    topic_frequencies =  calculate_topic_frequency_subset(word_freq, ndc_dict, str(key))\n",
    "    #ndc_climate_idxs = [token.idx for token in tokens if token.text in ndc_dict['climate change']]\n",
    "    document_text = ' '.join([token.text for token in tokens if len(token.text)<=max_word_length])\n",
    "    topic_name='NDC'\n",
    "    ndc_idx_df = make_ndc_idx_tidy_df(ndc_dict, topic_name, tokens)\n",
    "    ndc_idx_df.index = ndc_idx_df.NDC.copy()\n",
    "    df_sdg = make_sdg_df(sdg_list, sdg_ontology, document_text)\n",
    "    doc_summary_sdg = df_sdg.groupby('sdg_topic')['sdg_keywords_num'].sum()\n",
    "    doc_summary_sdg_df = doc_summary_sdg.to_frame().rename(columns={\"sdg_keywords_num\": doc_name})\n",
    "    return tokens, token_list, filtered_tokens, ndc_idx_df, doc_summary_sdg, doc_summary_sdg_df #topic_frequencies, sentences\n",
    "\n",
    "\n",
    "max_word_length=25\n",
    "summary_sdg_df_doc = pd.DataFrame()\n",
    "count = 1\n",
    "for i in range(0, len(policy_doc_df[:3])):\n",
    "    doc_name = policy_doc_df.iloc[i]['policy_doc_name_clean']\n",
    "    doc_path = policy_doc_df.iloc[i]['policy_doc_paths']\n",
    "    tokens, token_list, filtered_tokens, ndc_idx_df, doc_summary_sdg, doc_summary_sdg_df = run_nlp_pipeline(doc_name, doc_path, ndc_dict, max_word_length)   \n",
    "    print(ndc_idx_df.NDC.value_counts())\n",
    "    if count == 1: \n",
    "        summary_sdg_df_doc = doc_summary_sdg_df\n",
    "        total = doc_summary_sdg_df[doc_name].sum()\n",
    "        print(doc_name, len(filtered_tokens), total)\n",
    "    else: \n",
    "        summary_sdg_df_doc_for_merge = doc_summary_sdg_df\n",
    "        total = summary_sdg_df_doc_for_merge[doc_name].sum()\n",
    "        print(doc_name, len(filtered_tokens), total)\n",
    "        summary_sdg_df_doc = summary_sdg_df_doc.merge(summary_sdg_df_doc_for_merge, left_index=True, right_index=True)\n",
    "    count += 1\n",
    "    #print(ndc_idx_df.head())\n",
    "summary_sdg_df_doc.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e2e270",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:39.197908Z",
     "start_time": "2021-11-04T22:28:35.779411Z"
    }
   },
   "outputs": [],
   "source": [
    "summary_sdg_df_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227bb8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make heatmap of plot above\n",
    "plot_folder = '../../outputs/heatmaps/'\n",
    "plt.figure(figsize=(30, 6)) #16, 6\n",
    "ax = sns.heatmap(summary_sdg_df_doc,\n",
    "                 #annot=True, fmt=\"d\", \n",
    "                 cmap=\"YlGnBu\")\n",
    "plt.xlabel(\"Documents\")\n",
    "plt.ylabel(\"SDG Topics\")\n",
    "title = (\"Distribution of topic keywords in the documents (counts)\")\n",
    "plt.title(title)\n",
    "file_name=(title + (' all docs SA v1.png'))\n",
    "# plt.savefig((plot_folder+file_name), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18cf6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary_sdg_df_doc.sum(axis=0)\n",
    "#print(summary_sdg_df_doc.div(summary_sdg_df_doc.sum(axis=0), axis=1))\n",
    "normed_by_col_sum = summary_sdg_df_doc.div(summary_sdg_df_doc.sum(axis=0), axis=1).round(3)\n",
    "normed_by_col_sum.head()\n",
    "#normed_by_col_sum.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e98666",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 6)) #16, 6\n",
    "ax = sns.heatmap(normed_by_col_sum,\n",
    "                 #annot=True, fmt=\"d\", \n",
    "                 cmap=\"YlGnBu\")\n",
    "plt.xlabel(\"Documents\")\n",
    "plt.ylabel(\"SDG Topics\")\n",
    "title = (\"Distribution of topic keywords in the documents (normalized)\")\n",
    "plt.title(title)\n",
    "file_name=(title + (' all docs SA v1.png'))\n",
    "# plt.savefig((plot_folder+file_name), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6213a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 6))\n",
    "ax = sns.heatmap(normed_by_col_sum,\n",
    "                 #annot=True, #fmt=\"f\", \n",
    "                 cmap=\"YlGnBu\")\n",
    "plt.xlabel(\"Documents\")\n",
    "plt.ylabel(\"SDG Topics\")\n",
    "title = (\"Distribution of topic keywords in the documents (normalized)\")\n",
    "plt.title(title)\n",
    "file_name=(title + (' all docs SA v1.png'))\n",
    "# plt.savefig((plot_folder+file_name), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a74b55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 30))\n",
    "ax = sns.clustermap(normed_by_col_sum,\n",
    "                 #annot=True,\n",
    "                 cbar_pos=(0, .45, .03, .2),\n",
    "                 cmap=\"YlGnBu\", \n",
    "                 xticklabels=1, \n",
    "                 figsize=(30, 12))\n",
    "#plt.xlabel(\"Documents\")\n",
    "#plt.ylabel(\"SDG Topics\")\n",
    "title = (\"Distribution of topic keywords in the documents (normalized)\")\n",
    "#plt.title(title)\n",
    "file_name=(title + (' clustermap all docs SA v2.png'))\n",
    "# plt.savefig((plot_folder+file_name), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07141693",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndc_idx_df.NDC.value_counts()#head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec35dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at SDGs across document/at document level\n",
    "max_length=25\n",
    "document_text = ' '.join([token.text for token in tokens if len(token.text)<=max_length])\n",
    "\n",
    "df_sdg = make_sdg_df(sdg_list, sdg_ontology, document_text)\n",
    "doc_summary_sdg = df_sdg.groupby('sdg_topic')['sdg_keywords_num'].sum()\n",
    "doc_summary_sdg_df = doc_summary_sdg.to_frame().rename(columns={\"sdg_keywords_num\": (\"%s\"% doc_name)})\n",
    "doc_summary_sdg_df \n",
    "\n",
    "#lets see the 50 words that occur the most often\n",
    "df_sdg.sort_values(by=['sdg_keywords_num'], ascending=False)[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210c1ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##for i in range(): \n",
    "#    print('Processing doc: ', doc_name)\n",
    "\n",
    "doc_summary_sdg_df \n",
    "#if count == 1: \n",
    "#    summary_sdg_df = summary_sdg.to_frame().rename(columns={\"sdg_keywords_num\": (\"sdg_kw_%d\"% index)})\n",
    "#else: \n",
    "#    summary_sdg_df_for_merge = summary_sdg.to_frame().rename(columns={\"sdg_keywords_num\": (\"sdg_kw_%d\"% index)})\n",
    "#    summary_sdg_df = summary_sdg_df.merge(summary_sdg_df_for_merge, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe608f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = doc_summary_sdg_df[doc_name].sum()\n",
    "print(doc_name, len(filtered_tokens), total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b5ac75",
   "metadata": {},
   "source": [
    "### Apply Jonathan's fuzzy search method to find NDC related words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca26673",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample to work with \n",
    "doc_name = '2nd National Biodiversity Strategy Action Plan 2015-2025.pdf_ocr.txt'\n",
    "doc_path = policy_doc_df.loc[doc_name]['policy_doc_paths']\n",
    "\n",
    "def run_nlp_pipeline_no_lemma(doc_name, doc_path, ndc_dict, max_word_length):\n",
    "    print(doc_name, doc_path)\n",
    "    tokens, token_list, sentences = preprocess_doc(doc_path)\n",
    "    filtered_tokens = filter_tokens(tokens)\n",
    "    print('These are some of the filtered tokens: ', filtered_tokens[0:10])\n",
    "    #words = [token.text for token in filtered_tokens]\n",
    "    #word_freq = Counter(words)\n",
    "    #for key in ndc_dict.keys():\n",
    "    #    topic_frequencies =  calculate_topic_frequency_subset(word_freq, ndc_dict, str(key))\n",
    "    #ndc_climate_idxs = [token.idx for token in tokens if token.text in ndc_dict['climate change']]\n",
    "    document_text = ' '.join([token.text for token in filtered_tokens if len(token.text)<=max_word_length])\n",
    "    topic_name='NDC'\n",
    "    ndc_idx_df = make_ndc_idx_tidy_df(ndc_dict, topic_name, tokens)\n",
    "    ndc_idx_df.index = ndc_idx_df.NDC.copy()\n",
    "    df_sdg = make_sdg_df(sdg_list, sdg_ontology, document_text)\n",
    "    doc_summary_sdg = df_sdg.groupby('sdg_topic')['sdg_keywords_num'].sum()\n",
    "    doc_summary_sdg_df = doc_summary_sdg.to_frame().rename(columns={\"sdg_keywords_num\": doc_name})\n",
    "    return tokens, token_list, sentences, filtered_tokens, document_text, ndc_idx_df, doc_summary_sdg, doc_summary_sdg_df #topic_frequencies, sentences\n",
    "\n",
    "count=1\n",
    "max_word_length=25\n",
    "tokens, token_list, sentences, filtered_tokens, document_text, ndc_idx_df, doc_summary_sdg, doc_summary_sdg_df = run_nlp_pipeline_no_lemma(doc_name, doc_path, ndc_dict, max_word_length)   \n",
    "print(ndc_idx_df.NDC.value_counts())\n",
    "if count == 1: \n",
    "    summary_sdg_df_doc = doc_summary_sdg_df\n",
    "    total = doc_summary_sdg_df[doc_name].sum()\n",
    "    print(doc_name, len(filtered_tokens), total)\n",
    "else: \n",
    "    summary_sdg_df_doc_for_merge = doc_summary_sdg_df\n",
    "    total = summary_sdg_df_doc_for_merge[doc_name].sum()\n",
    "    print(doc_name, len(filtered_tokens), total)\n",
    "    summary_sdg_df_doc = summary_sdg_df_doc.merge(summary_sdg_df_doc_for_merge, left_index=True, right_index=True)\n",
    "    count += 1\n",
    "    #print(ndc_idx_df.head())\n",
    "summary_sdg_df_doc.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0309a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_text[100:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05e1b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "##lets take a look at the document from South Africa used in the example in the proposal first:\n",
    "doc_name = '2nd National Biodiversity Strategy Action Plan 2015-2025.pdf_ocr.txt'\n",
    "doc_path = policy_doc_df.loc[doc_name]['policy_doc_paths']\n",
    "\n",
    "annual_report = importer.TextImporter(doc_path)\n",
    "with open(\"../ndc_keywords/ndc_south_africa.json\") as f:\n",
    "    ndc_keywords = json.load(f)\n",
    "\n",
    "climate_keywords = ndc_keywords['climate change']\n",
    "#climate_keywords = list(sdg_ontology[sdg_ontology['clasification']=='SDG13']['keyword'])\n",
    "doc = nlp(document_text) #modified to use filtered version of text #nlp(annual_report.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60024d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sdg_ontology[sdg_ontology['clasification']=='SDG13']\n",
    "climate_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee3dae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_cor = NGramCorrelateSpacy(climate_keywords, 0.7, \"CLIMATE_N\")\n",
    "doc.ents = []\n",
    "n_gram_cor.correlate_spans(doc, 2)\n",
    "print(len(doc.ents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3c104d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in doc.ents[0:10]:\n",
    "    displacy.render(doc[e.start-20:e.end+20], style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2a1f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc.ents[:20])\n",
    "#idx = [ent.start for ent in doc.ents]\n",
    "ndc_climate_idxs = [ent.start for ent in doc.ents]\n",
    "print(len(ndc_climate_idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd8d6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_correlator = TokenArrayCorrelator(climate_keywords, 0.4, \"CLIMATE_TOKEN\")\n",
    "span_correlator = SpanCorrelator(climate_keywords, 0.4, \"CLIMATE_SPAN\")\n",
    "generic_correlator = KeywordCorrelator(climate_keywords)\n",
    "\n",
    "sentence_to_correlate = \"We need to adapt our project to be more resillient to geographical circumstances.\"\n",
    "unrelated_sentence = \"The next time the leaders will meet in paris\"\n",
    "\n",
    "#print(generic_correlator([sent for sent in sentences]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef59350",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndc_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fad622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_ndc_spans(ndc_keywords, doc):\n",
    "    \"\"\"ndc_keywords is a dictionary, doc is the document text with nlp run on it that is the filtered but not lemmatized document text.\"\"\"\n",
    "    matcher = PhraseMatcher(nlp.vocab)\n",
    "    #iterate through NDC keys in NDC dictionary to create seperate label categories for the matching\n",
    "    entity_labels = []\n",
    "    for entity_reference in [key for key in ndc_keywords.keys()]:\n",
    "        entity_label = entity_reference + ' NDC'\n",
    "        print(entity_label)\n",
    "        entity_labels.append(entity_label)\n",
    "        keywords = ndc_keywords[entity_reference]\n",
    "        patterns = [nlp(i) for i in keywords]\n",
    "        #print(keywords, patterns)\n",
    "        matcher.add(entity_label, None, *patterns)\n",
    "    matches = matcher(doc)\n",
    "        #label spans\n",
    "    for match_id, start, end in matches:\n",
    "        try:\n",
    "            span = Span(doc, start, end, label=match_id)\n",
    "            doc.ents = list(doc.ents) + [span]  # add span to doc.ents\n",
    "        except: \n",
    "            pass\n",
    "    #print([(ent.text, ent.start, ent.label_) for ent in doc.ents])\n",
    "    return entity_labels, doc\n",
    "\n",
    "def label_ndc_spans_return_index(entity_reference, ndc_keywords, document_text):\n",
    "    \"\"\"ndc_keywords is a dictionary, document_text is the filtered but not lemmatized document text.\"\"\"\n",
    "    entity_label = entity_reference + ' NDC'\n",
    "    keywords = ndc_keywords[entity_reference]\n",
    "    patterns = [nlp(i) for i in keywords]\n",
    "    matcher = PhraseMatcher(nlp.vocab)\n",
    "    matcher.add(entity_label, None, *patterns)\n",
    "    doc = nlp(document_text)\n",
    "    matches = matcher(doc)\n",
    "    #label spans\n",
    "    for match_id, start, end in matches:\n",
    "        try:\n",
    "            span = Span(doc, start, end, label=match_id)\n",
    "            doc.ents = list(doc.ents) + [span]  # add span to doc.ents\n",
    "        except: \n",
    "            pass\n",
    "    #get list of spans related to the ndc\n",
    "    idxs_from_matcher = [ent.start for ent in doc.ents if ent.label_ == entity_label]\n",
    "    return doc, idxs_from_matcher\n",
    "\n",
    "test_text = document_text #\"natural nationa ationa organisi association of conservancies of south africa action plan biodiversity assessment biodiversity and business network biodiversity economy strategy biodiversity framework biodiversity strategy and action plan rimental finding development plan department of tourism environmental advisory forum environmental management act environmental skills planning forum implementing\"\n",
    "test_doc = nlp(test_text)\n",
    "ndc_labels, labelled_doc = label_ndc_spans(ndc_keywords, test_doc)\n",
    "\n",
    "#displacy.render(labelled_doc, style = \"ent\", jupyter = True)\n",
    "#labelled_doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2546ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ndc_num_dict(ndc_keywords, labelled_doc): \n",
    "    \"\"\"ndc_keywords is the ndc keyword dictionary and the labelled_doc is the one labelled with the spans of NDC keywords\"\"\"\n",
    "    ndc_doc_idx_dict = dict()\n",
    "    ndc_doc_num = dict()\n",
    "    for entity_reference in [key for key in ndc_keywords.keys()]:\n",
    "            entity_label = entity_reference + ' NDC'\n",
    "            print(entity_label) \n",
    "            ents_from_matcher = [ent for ent in labelled_doc.ents if ent.label_ == entity_label]\n",
    "            idxs_from_matcher = [ent.start for ent in labelled_doc.ents if ent.label_ == entity_label]\n",
    "            ndc_doc_idx_dict[entity_label] = idxs_from_matcher \n",
    "            if len(ents_from_matcher)==0:\n",
    "                ndc_doc_num[entity_label] = 0\n",
    "            else: \n",
    "                ndc_doc_num[entity_label] = len(ents_from_matcher)\n",
    "    ndc_doc_num_df = pd.DataFrame.from_dict(ndc_doc_num, orient='index').rename(columns={0: 'NDC_word_count_%s' % doc_name})\n",
    "    return ndc_doc_idx_dict, ndc_doc_num_df\n",
    "\n",
    "ndc_doc_idx_dict, ndc_doc_num_df = make_ndc_num_dict(ndc_keywords, labelled_doc)\n",
    "ndc_doc_num_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1801f2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if count == 1: \n",
    "    summary_sdg_df_doc = doc_summary_sdg_df\n",
    "    total = doc_summary_sdg_df[doc_name].sum()\n",
    "    print(doc_name, len(filtered_tokens), total)\n",
    "else: \n",
    "    summary_sdg_df_doc_for_merge = doc_summary_sdg_df\n",
    "    total = summary_sdg_df_doc_for_merge[doc_name].sum()\n",
    "    print(doc_name, len(filtered_tokens), total)\n",
    "    summary_sdg_df_doc = summary_sdg_df_doc.merge(summary_sdg_df_doc_for_merge, left_index=True, right_index=True)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319581e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"helps to address poverty and unemployment . operation phakisa is initially implemented in two sectors , the ocean economy and health , and will be rolled out in other sectors . in the oceans economy four priority areas for unlocking the oceans economy through inclusive economic growth have been identified , one of which is marine protection services and ocean governance . other biodiversity and\"\n",
    "sdg_ontology[sdg_ontology['clasification']=='SDG8']['keyword']\n",
    "#list(ndc_dict['climate change'])\n",
    "    \n",
    "entity_reference = 'climate change'\n",
    "#print([(ent.text, ent.start, ent.label_) for ent in doc.ents]) #there are some cool default entitites as well\n",
    "climate_ndc_idxs_from_matcher = label_ndc_spans_return_index(entity_reference, ndc_keywords, document_text)[1]\n",
    "climate_ndc_idxs_from_matcher\n",
    "\n",
    "new_doc = label_ndc_spans_return_index(entity_reference, ndc_keywords, document_text)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83ee2d6-8659-4dcc-a6cd-1b9e18deb191",
   "metadata": {},
   "source": [
    "### Window functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf2c1da-b22c-434d-bf27-ec6506271bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # From the Windows Notebook\n",
    "def return_window_i(ndc_word_index, tokens, size=20):\n",
    "    \"\"\"size is the number of words to include on either side of the NDC keyword whose position is given by the ndc_word_index. \n",
    "    The tokens are the original tokens in the document\"\"\"\n",
    "    lower_limit = ndc_word_index - size\n",
    "    upper_limit = ndc_word_index + size\n",
    "    token_indices = [token.i for token in tokens]\n",
    "    window_tokens = tokens[(ndc_word_index-size):(ndc_word_index+size)] \n",
    "    return lower_limit, upper_limit, window_tokens\n",
    "\n",
    "def return_window_idx(ndc_word_index, tokens, size=100):\n",
    "    \"\"\"Uses the .idx positions of the start of the invidual NDC words in the document to define the windows.\"\"\"\n",
    "    lower_limit = ndc_word_index - size\n",
    "    upper_limit = ndc_word_index + size\n",
    "    token_idxs = [token.idx for token in tokens]\n",
    "    window_token_list = []\n",
    "    #print('The window is ', lower_limit, upper_limit)\n",
    "    for index, idx in enumerate(token_idxs):\n",
    "        if (idx >= lower_limit) and (idx <= upper_limit):\n",
    "            window_token_list.append(tokens[index])\n",
    "        else:\n",
    "            pass\n",
    "    text_for_windows = ' '.join(list(token.text for token in window_token_list))\n",
    "    window_tokens = nlp(text_for_windows)\n",
    "    return lower_limit, upper_limit, window_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89747474-042d-42d9-b1cd-143b424f0ce6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Where do these words appear in the document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b53a832",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_tokens_overall = make_window_text(tokens, max_length=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9641c4f7-aff5-43cc-8be5-b17024074a65",
   "metadata": {},
   "source": [
    "#### Use indexes from spacy matcher "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d067e20e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T08:26:52.345345Z",
     "start_time": "2021-11-05T08:26:52.146105Z"
    }
   },
   "outputs": [],
   "source": [
    "#may want to increase the min_distance/set it as a function of window size\n",
    "window_size = 200\n",
    "min_dist = 200\n",
    "idx_for_window = filter_idx_for_overlap(idxs=climate_ndc_idxs_from_matcher, min_dist=min_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcf73db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:40.217870Z",
     "start_time": "2021-11-04T22:28:40.215870Z"
    }
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for index in idx_for_window[0:len(idx_for_window)]:\n",
    "    count += 1\n",
    "    print(index)\n",
    "    window_tokens = return_window(index, window_tokens_overall, size=window_size)[2]\n",
    "    window_text = ' '.join([token.text for token in window_tokens])\n",
    "    #print(window_text)\n",
    "    df_sdg = make_sdg_df(sdg_list, sdg_ontology, window_text) #may want to keep this for the windows for a more granular analysis\n",
    "    df_sdg_sorted = df_sdg.sort_values(by=['sdg_keywords_num'], ascending=False)\n",
    "    #print(df_sdg_sorted.head())\n",
    "    print(list(df_sdg_sorted[df_sdg_sorted['sdg_keywords_num'] > 0]['sdg_keywords'][:10])) #print top 10 positive valued keywords\n",
    "    #print a summary of the SDG words found: \n",
    "    summary_sdg = df_sdg.groupby('sdg_topic')['sdg_keywords_num'].sum()\n",
    "    if count == 1: \n",
    "        summary_sdg_df = summary_sdg.to_frame().rename(columns={\"sdg_keywords_num\": (\"sdg_kw_%d\"% index)})\n",
    "    else: \n",
    "        summary_sdg_df_for_merge = summary_sdg.to_frame().rename(columns={\"sdg_keywords_num\": (\"sdg_kw_%d\"% index)})\n",
    "        summary_sdg_df = summary_sdg_df.merge(summary_sdg_df_for_merge, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015bf4a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:40.279908Z",
     "start_time": "2021-11-04T22:28:40.219372Z"
    }
   },
   "outputs": [],
   "source": [
    "summary_sdg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8c9acb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:40.286182Z",
     "start_time": "2021-11-04T22:28:40.281225Z"
    }
   },
   "outputs": [],
   "source": [
    "#make heatmap of plot above\n",
    "plt.figure(figsize=(16, 6))\n",
    "ax = sns.heatmap(summary_sdg_df,\n",
    "                 annot=True, #fmt=\"d\", \n",
    "                 cmap=\"YlGnBu\")\n",
    "plt.xlabel(\"climate change NDC-associated windows\")\n",
    "plt.ylabel(\"SDG Topics\")\n",
    "title = (\"Distribution of topic keywords in climate change NDC-associated windows in %s\" % (doc_name))\n",
    "plt.title(title)\n",
    "file_name=(title + (' across windows from spacy matching.png'))\n",
    "# plt.savefig((plot_folder+file_name), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83d6b37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:40.871444Z",
     "start_time": "2021-11-04T22:28:40.287814Z"
    }
   },
   "outputs": [],
   "source": [
    "index=5868\n",
    "print(return_window(index, window_tokens_overall, size=window_size)[2]) #print(window_tokens[2])\n",
    "summary_sdg_df[('sdg_kw_%d' % index)]\n",
    "print(\"\")\n",
    "\n",
    "index=11424\n",
    "print(return_window(index, window_tokens_overall, size=window_size)[2]) #print(window_tokens[2])\n",
    "summary_sdg_df[('sdg_kw_%d' % index)]\n",
    "print(\"\")\n",
    "\n",
    "test_text = return_window(index, window_tokens_overall, size=window_size)[2]\n",
    "test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb202e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets make another function to label categories:\n",
    "\n",
    "def label_keywords(entity_reference, keyword_list, text):\n",
    "    \"\"\"ndc_keywords is a dictionary, document_text is the filtered but not lemmatized document text.\"\"\"\n",
    "    entity_label = entity_reference\n",
    "    patterns = [nlp(i) for i in keyword_list]\n",
    "    matcher = PhraseMatcher(nlp.vocab)\n",
    "    matcher.add(entity_label, None, *patterns)\n",
    "    doc = nlp(document_text)\n",
    "    matches = matcher(doc)\n",
    "    #label spans\n",
    "    for match_id, start, end in matches:\n",
    "        try:\n",
    "            span = Span(doc, start, end, label=match_id)\n",
    "            doc.ents = list(doc.ents) + [span]  # add span to doc.ents\n",
    "        except: \n",
    "            pass\n",
    "    #get list of spans related to the ndc\n",
    "    idxs_from_matcher = [ent.start for ent in doc.ents if ent.label_ == entity_label]\n",
    "    return doc, idxs_from_matcher\n",
    "\n",
    "sdg_list = ['SDG1', 'SDG2', 'SDG3', 'SDG4', 'SDG5', 'SDG6', 'SDG7', 'SDG8', 'SDG9', \n",
    "            'SDG10', 'SDG11', 'SDG12', 'SDG13', 'SDG14', 'SDG15', 'SDG16', 'SDG17', \n",
    "            \"mention_money\"]\n",
    "\n",
    "index=5868\n",
    "index=11424\n",
    "index=18783\n",
    "print(return_window(index, window_tokens_overall, size=window_size)[2]) #print(window_tokens[2])\n",
    "summary_sdg_df[('sdg_kw_%d' % index)]\n",
    "\n",
    "test_text = return_window(index, window_tokens_overall, size=window_size)[2]\n",
    "test_text\n",
    "\n",
    "text = test_text\n",
    "sdg_list = ['SDG15']\n",
    "for i in sdg_list:\n",
    "    entity_reference = i\n",
    "    keyword_list = list(sdg_ontology[sdg_ontology['clasification']==entity_reference]['keyword'])\n",
    "    text = label_keywords(entity_reference, keyword_list, text)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5558668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets see if we can find ndc phrases with spacy matcher\n",
    "#document_text\n",
    "\n",
    "sdg14_keywords = list(sdg_ontology[sdg_ontology['clasification']=='SDG14']['keyword'])\n",
    "sdg8_keywords = list(sdg_ontology[sdg_ontology['clasification']=='SDG8']['keyword'])\n",
    "\n",
    "patterns14 = [nlp(i) for i in sdg14_keywords]\n",
    "patterns8 = [nlp(i) for i in sdg8_keywords]\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add('SDG14', None, *patterns14)\n",
    "matcher.add('SDG8', None, *patterns8)\n",
    "\n",
    "#doc = nlp(\"I like bacon and chicken but unfortunately I only had an apple and a carrot in the fridge\")\n",
    "doc = nlp(test_text)\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    try:\n",
    "        span = Span(doc, start, end, label=match_id)\n",
    "        doc.ents = list(doc.ents) + [span]  # add span to doc.ents\n",
    "    except: \n",
    "        pass\n",
    "\n",
    "colors = {\"SDG14\": \"#85C1E9\", \"SDG8\": \"#ff6961\"}\n",
    "options = {\"ents\": ['SDG14', 'SDG8'], \"colors\": colors}\n",
    "displacy.render(doc, style='ent', options=options) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddeddce",
   "metadata": {},
   "source": [
    "### Find most common (and unique) words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bb1aa3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:39.216487Z",
     "start_time": "2021-11-04T22:28:39.199518Z"
    }
   },
   "outputs": [],
   "source": [
    "print(doc_name, doc_path)\n",
    "tokens, token_list, sentences = preprocess_doc(doc_path)\n",
    "filtered_tokens = filter_modify_tokens(tokens)\n",
    "print('These are some of the filtered tokens: ', filtered_tokens[0:10])\n",
    "words = [token.text for token in filtered_tokens]\n",
    "word_freq = Counter(words)\n",
    "for key in ndc_dict.keys():\n",
    "    topic_frequencies =  calculate_topic_frequency_subset(word_freq, ndc_dict, str(key))\n",
    "    print(topic_frequencies)\n",
    "    \n",
    "ndc_climate_idxs = [token.idx for token in tokens if token.text in ndc_dict['climate change']]\n",
    "\n",
    "\n",
    "# Remove stop words and punctuation symbols\n",
    "words = [token.text for token in filtered_tokens]\n",
    "word_freq = Counter(words)\n",
    "# 5 commonly occurring words with their frequencies\n",
    "common_words = word_freq.most_common(30)\n",
    "print(common_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada47c6b",
   "metadata": {},
   "source": [
    "### Make some plots of the NDC/Thematic key words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f0ef52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:39.785606Z",
     "start_time": "2021-11-04T22:28:39.220235Z"
    }
   },
   "outputs": [],
   "source": [
    "#just to test - need to modularise/make more reproducible and tailor output to be useful across documents\n",
    "#could also link this back to the df of the documents to output a table with some summary metrics for keywords \n",
    "#in each document of interest\n",
    "graphs_folder = '../../outputs/bar_charts/'    \n",
    "    \n",
    "for key in ndc_dict.keys(): \n",
    "    print(\"Graphing the occurences of %s words in the document\" % key)\n",
    "    topic_frequencies =  calculate_topic_frequency_subset(word_freq, ndc_dict, str(key))\n",
    "    plot_word_freq_barchart_ndc(topic_frequencies, str(key), doc_name, graphs_folder) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073a6606",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndc_climate_idxs = [ent.start for ent in doc.ents]\n",
    "if key == 'climate change':\n",
    "        print(\"Graphing the occurences of %s words in the document\" % key)\n",
    "        topic_frequencies = calculate_topic_frequency_subset(word_freq, ndc_dict, str(key))\n",
    "        plot_word_freq_barchart_ndc(topic_frequencies, str(key), doc_name, graphs_folder) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9db410",
   "metadata": {},
   "source": [
    "### Using other resources:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dbc823",
   "metadata": {},
   "source": [
    "### NDC Ontology with SDG classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7560567",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T08:39:50.566673Z",
     "start_time": "2021-11-05T08:39:50.555850Z"
    }
   },
   "outputs": [],
   "source": [
    "sdg_ontology = pd.read_csv('../additional_resources/Ontology_final_modified.csv', sep=';')#, #skiprows=0)\n",
    "SDG1_keywords = list(sdg_ontology[sdg_ontology['clasification']=='SDG1']['keyword'])\n",
    "#print(SDG1_keywords)\n",
    "sdg_ontology.head(20)\n",
    "#print(list(ndc_ontology[ndc_ontology['clasification']=='SDG3']['keyword']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c13e34b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T08:11:20.768901Z",
     "start_time": "2021-11-05T08:11:18.591192Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "document_text = ' '.join([token.text for token in window_tokens_overall])\n",
    "\n",
    "#look at SDGs across document/at document level\n",
    "sdg_list = ['SDG1', 'SDG2', 'SDG3', 'SDG4', 'SDG5', 'SDG6', 'SDG7', 'SDG8', 'SDG9', \n",
    "            'SDG10', 'SDG11', 'SDG12', 'SDG13', 'SDG14', 'SDG15', 'SDG16', 'SDG17', \n",
    "            \"mention_money\"]\n",
    "\n",
    "df_sdg = make_sdg_df(sdg_list, sdg_ontology, document_text)\n",
    "\n",
    "#lets see the 50 words that occur the most often\n",
    "df_sdg.sort_values(by=['sdg_keywords_num'], ascending=False)[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b5d262",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Processing doc: ', doc_name)\n",
    "    \n",
    "document_text = ' '.join([token.text for token in window_tokens_overall])\n",
    "df_sdg = make_sdg_df(sdg_list, sdg_ontology, document_text)\n",
    "doc_summary_sdg = df_sdg.groupby('sdg_topic')['sdg_keywords_num'].sum()\n",
    "doc_summary_sdg.to_frame().rename(columns={\"sdg_keywords_num\": (\"%s\"% doc_name)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f229ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T08:11:22.665452Z",
     "start_time": "2021-11-05T08:11:22.153909Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_folder = '../../outputs/heatmaps/'\n",
    "#print a summary of the SDG words found: \n",
    "doc_summary_sdg = df_sdg.groupby('sdg_topic')['sdg_keywords_num'].sum()\n",
    "doc_summary_sdg.to_frame()\n",
    "#doc_summary_sdg.to_frame()\n",
    "plt.figure(figsize=(2, 6))\n",
    "ax = sns.heatmap(doc_summary_sdg.to_frame(), \n",
    "                 annot=True, fmt=\"d\", \n",
    "                 cmap=\"YlGnBu\")\n",
    "\n",
    "#plt.xlabel(\"climate change NDC-associated windows\")\n",
    "plt.ylabel(\"Topics\")\n",
    "title = (\"Topic keywords in %s\" % (doc_name))\n",
    "plt.title(title)\n",
    "file_name=(title + (' across document v1.png'))\n",
    "# plt.savefig((plot_folder+file_name), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253bc7f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:48:16.297411Z",
     "start_time": "2021-11-04T22:48:16.291014Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sdg_sorted  = df_sdg.sort_values(by=['sdg_keywords_num'], ascending=False)\n",
    "list(df_sdg_sorted[df_sdg_sorted['sdg_keywords_num'] > 0]['sdg_keywords'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a27cd46-a854-4d2e-8769-f94cb7254ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_for_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2951206",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:48:36.716699Z",
     "start_time": "2021-11-04T22:48:35.936147Z"
    }
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for index in idx_for_window[0:3]:\n",
    "    count += 1\n",
    "    print(index)\n",
    "    window_tokens = return_window(index, window_tokens_overall, size=200)[2]\n",
    "    window_text = ' '.join([token.text for token in window_tokens])\n",
    "    print(\"Window Index: \", window_text)\n",
    "    df_sdg = make_sdg_df(sdg_list, sdg_ontology, window_text)\n",
    "    df_sdg_sorted = df_sdg.sort_values(by=['sdg_keywords_num'], ascending=False)\n",
    "    #print(df_sdg_sorted.head())\n",
    "    print(list(df_sdg_sorted[df_sdg_sorted['sdg_keywords_num'] > 0]['sdg_keywords'][:10])) #print top 10 positive valued keywords\n",
    "    #print a summary of the SDG words found: \n",
    "    summary_sdg = df_sdg.groupby('sdg_topic')['sdg_keywords_num'].sum()\n",
    "    if count == 1: \n",
    "        summary_sdg_df = summary_sdg.to_frame().rename(columns={\"sdg_keywords_num\": (\"sdg_kw_%d\"% index)})\n",
    "    else: \n",
    "        summary_sdg_df_for_merge = summary_sdg.to_frame().rename(columns={\"sdg_keywords_num\": (\"sdg_kw_%d\"% index)})\n",
    "        summary_sdg_df = summary_sdg_df.merge(summary_sdg_df_for_merge, left_index=True, right_index=True)\n",
    "    print(100 * \"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc8169c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:48:44.482914Z",
     "start_time": "2021-11-04T22:48:44.476742Z"
    }
   },
   "outputs": [],
   "source": [
    "#may want to increase the min_distance/set it as a function of window size\n",
    "window_size = 200\n",
    "min_dist = 200\n",
    "idx_for_window = filter_idx_for_overlap(idxs=ndc_climate_idxs, min_dist=min_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c59104",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:50:38.264461Z",
     "start_time": "2021-11-04T22:49:02.540867Z"
    }
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for index in idx_for_window[0:len(idx_for_window)]:\n",
    "    count += 1\n",
    "    print(index)\n",
    "    window_tokens = return_window(index, window_tokens_overall, size=window_size)[2]\n",
    "    window_text = ' '.join([token.text for token in window_tokens])\n",
    "    #print(window_text)\n",
    "    df_sdg = make_sdg_df(sdg_list, sdg_ontology, window_text) #may want to keep this for the windows for a more granular analysis\n",
    "    df_sdg_sorted = df_sdg.sort_values(by=['sdg_keywords_num'], ascending=False)\n",
    "    #print(df_sdg_sorted.head())\n",
    "    print(list(df_sdg_sorted[df_sdg_sorted['sdg_keywords_num'] > 0]['sdg_keywords'][:10])) #print top 10 positive valued keywords\n",
    "    #print a summary of the SDG words found: \n",
    "    summary_sdg = df_sdg.groupby('sdg_topic')['sdg_keywords_num'].sum()\n",
    "    if count == 1: \n",
    "        summary_sdg_df = summary_sdg.to_frame().rename(columns={\"sdg_keywords_num\": (\"sdg_kw_%d\"% index)})\n",
    "    else: \n",
    "        summary_sdg_df_for_merge = summary_sdg.to_frame().rename(columns={\"sdg_keywords_num\": (\"sdg_kw_%d\"% index)})\n",
    "        summary_sdg_df = summary_sdg_df.merge(summary_sdg_df_for_merge, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc621d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:50:38.288112Z",
     "start_time": "2021-11-04T22:50:38.266666Z"
    }
   },
   "outputs": [],
   "source": [
    "summary_sdg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5012a507",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T08:08:44.447055Z",
     "start_time": "2021-11-05T08:08:42.366958Z"
    }
   },
   "outputs": [],
   "source": [
    "summary_sdg_df\n",
    "\n",
    "#make heatmap of plot above\n",
    "plt.figure(figsize=(16, 6))\n",
    "ax = sns.heatmap(summary_sdg_df,\n",
    "                 #annot=True, fmt=\"d\", \n",
    "                 cmap=\"YlGnBu\")\n",
    "plt.xlabel(\"climate change NDC-associated windows\")\n",
    "plt.ylabel(\"SDG Topics\")\n",
    "title = (\"Distribution of topic keywords in climate change NDC-associated windows in %s\" % (doc_name))\n",
    "plt.title(title)\n",
    "file_name=(title + (' across all windows v1.png'))\n",
    "# plt.savefig((plot_folder+file_name), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19355a2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:29:03.169869Z",
     "start_time": "2021-11-04T22:29:03.169858Z"
    }
   },
   "outputs": [],
   "source": [
    "#make heatmap of plot above\n",
    "plt.figure(figsize=(16, 6))\n",
    "ax = sns.heatmap(summary_sdg_df,\n",
    "                 #annot=True, fmt=\"d\", \n",
    "                 cmap=\"YlGnBu\")\n",
    "plt.xlabel(\"Windows with NDC words in the document\")\n",
    "plt.ylabel(\"SDG Topics\")\n",
    "title = (\"Distribution of keywords related to different SDGs in NDC-associated windows in the document\")\n",
    "plt.title(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc43662",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T08:50:35.824422Z",
     "start_time": "2021-11-05T08:50:30.550327Z"
    }
   },
   "outputs": [],
   "source": [
    "#make heatmap of plot above\n",
    "plt.figure(figsize=(16, 6))\n",
    "ax = sns.heatmap(summary_sdg_df.iloc[:, 0:-1], #can show all windows with summary_sdg_df\n",
    "                 annot=True, fmt=\"d\", \n",
    "                 cmap=\"YlGnBu\")\n",
    "plt.xlabel(\"climate change NDC-associated windows\")\n",
    "plt.ylabel(\"SDG Topics\")\n",
    "title = (\"Distribution of topic keywords in climate change NDC-associated windows in %s\" % (doc_name))\n",
    "plt.title(title)\n",
    "file_name=(title + (' across windows 40-100 v1.png'))\n",
    "# plt.savefig((plot_folder+file_name), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220a4d58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T08:54:34.555993Z",
     "start_time": "2021-11-05T08:54:34.501786Z"
    }
   },
   "outputs": [],
   "source": [
    "index=41020\n",
    "print(return_window(index, window_tokens_overall, size=window_size)[2]) #print(window_tokens[2])\n",
    "summary_sdg_df[('sdg_kw_%d' % index)]\n",
    "print(\"\")\n",
    "index=41340\n",
    "print(return_window(index, window_tokens_overall, size=window_size)[2]) #print(window_tokens[2])\n",
    "summary_sdg_df[('sdg_kw_%d' % index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6b4630",
   "metadata": {},
   "source": [
    "#### SDG14: \"Conserve and sustainably use the oceans, seas and marine resources for sustainable development\"\n",
    "https://sdgs.un.org/goals/goal14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b03bf3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:50:41.267008Z",
     "start_time": "2021-11-04T22:50:41.239511Z"
    }
   },
   "outputs": [],
   "source": [
    "index=55289\n",
    "print(return_window(index, window_tokens_overall, size=window_size)[2]) #print(window_tokens[2])\n",
    "summary_sdg_df[('sdg_kw_%d' % index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a427fa",
   "metadata": {},
   "source": [
    "#### SDG08: \"Promote sustained, inclusive and sustainable economic growth, full and productive employment and decent work for all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf04d556",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:50:41.300930Z",
     "start_time": "2021-11-04T22:50:41.269142Z"
    }
   },
   "outputs": [],
   "source": [
    "index=72837\n",
    "print(return_window(index, window_tokens_overall, size=window_size)[2]) #print(window_tokens[2])\n",
    "summary_sdg_df[('sdg_kw_%d' % index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c691fa",
   "metadata": {},
   "source": [
    "#### SDG11: \"Make cities and human settlements inclusive, safe, resilient and sustainable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6018424d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:50:41.324337Z",
     "start_time": "2021-11-04T22:50:41.302317Z"
    }
   },
   "outputs": [],
   "source": [
    "index=74881\n",
    "print(return_window(index, window_tokens_overall, size=window_size)[2]) #print(window_tokens[2])\n",
    "summary_sdg_df[('sdg_kw_%d' % index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8a3549",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:50:41.355159Z",
     "start_time": "2021-11-04T22:50:41.326148Z"
    }
   },
   "outputs": [],
   "source": [
    "index=241379\n",
    "print(return_window(index, window_tokens_overall, size=window_size)[2]) #print(window_tokens[2])\n",
    "summary_sdg_df[('sdg_kw_%d' % index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6243292e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:51:48.692277Z",
     "start_time": "2021-11-04T22:51:48.668640Z"
    }
   },
   "outputs": [],
   "source": [
    "index=67819\n",
    "print(return_window(index, window_tokens_overall, size=window_size)[2]) #print(window_tokens[2])\n",
    "summary_sdg_df[('sdg_kw_%d' % index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c7511c",
   "metadata": {},
   "source": [
    "### Parts of speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b40fc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:29:03.177541Z",
     "start_time": "2021-11-04T22:29:03.177528Z"
    }
   },
   "outputs": [],
   "source": [
    "for token in filtered_tokens[:50]:\n",
    "    print(token, token.tag_, token.pos_, spacy.explain(token.tag_))\n",
    "print('\\n')\n",
    "    \n",
    "nouns = []\n",
    "adjectives = []\n",
    "for token in filtered_tokens:\n",
    "    if token.pos_ == 'NOUN':\n",
    "        nouns.append(token)\n",
    "    if token.pos_ == 'ADJ':\n",
    "        adjectives.append(token)\n",
    "\n",
    "print('some nouns spacy called in the document include:', '\\n', nouns[:50], '\\n')\n",
    "print('some adjectives spacy called in the document include:', '\\n', adjectives[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b519308c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T11:56:38.400728Z",
     "start_time": "2021-10-09T11:56:38.397730Z"
    }
   },
   "source": [
    "### Entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773f4581",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:29:03.178541Z",
     "start_time": "2021-11-04T22:29:03.178529Z"
    }
   },
   "outputs": [],
   "source": [
    "#can display entity property for the tokens as well: \n",
    "entities=[(i, i.label_, i.label) for i in filtered_tokens[1400:1700].ents]\n",
    "print(entities[:10])\n",
    "\n",
    "token_subset = tokens[100:500]\n",
    "displacy.render(token_subset, style = \"ent\", jupyter = True) #use original tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d245d4",
   "metadata": {},
   "source": [
    "### Dependency visualization in sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24796c38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:29:03.179664Z",
     "start_time": "2021-11-04T22:29:03.179653Z"
    }
   },
   "outputs": [],
   "source": [
    "sent_example = nlp(\"For example, it is estimated that between 9 and 12 million DATE people in impoverished rural areas directly use natural resources such as fuel wood, wild fruits and wooden utensils as a source of energy, food and building material respectively (Shackleton ORG 2004)\")\n",
    "sentence_spans = list(sentences)\n",
    "sentence_spans[:10]\n",
    "displacy.render(sentence_spans[80], style=\"dep\", jupyter= True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "75547552c603f22400c6f6e0e4ad2ade15359435e000e6746c53241e37331409"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
