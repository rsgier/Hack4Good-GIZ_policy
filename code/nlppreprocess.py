#NLP related helper functions
#used these resources:
#https://www.geeksforgeeks.org/nlp-expand-contractions-in-text-processing/)
#https://realpython.com/natural-language-processing-spacy-python/#how-to-download-models-and-data

import codecs
import contractions
import spacy
from collections import OrderedDict

nlp = spacy.load(
    'en_core_web_sm')  #or the multi-language one: spacy.load('xx_ent_wiki_sm')


def fix_contractions(document):
    """
    Switches contractions like can't to cannot so potentially important words/pieces of words are 
    removed with punctuation removal.
    """
    # original contracted text
    # # # TODO: REMOVE THIS AFTER MODULIZATION
    with codecs.open(document, errors="ignore", encoding="utf8") as f:
        text = f.read()
    # # #

    #creating an empty list
    expanded_words = []
    for word in text.split():
        #using contractions.fix to expand the shortened words
        expanded_words.append(contractions.fix(word))

    expanded_text = ' '.join(expanded_words)
    return expanded_text


def is_token_allowed(token):
    '''
        Only allow valid tokens which are not stop words
        and punctuation symbols.
    '''
    if (not token or not token.text.strip() or token.is_stop or token.is_punct):
        return False
    return True


def preprocess_token(token):
    # Reduce token to its lowercase lemma form
    return token.lemma_.strip().lower()


def preprocess_doc(doc_path):
    """
    Applies NLP framework to a document returns the word tokens, a list of the word tokens, and the sentence tokens.
    NOTE: may want to also return sentences and other objects too depending on the use case
    """
    #remove contracted words and tokenize the document
    tokens = nlp(fix_contractions(doc_path))  #word tokens
    token_list = [token for token in tokens]
    sentences = list(tokens.sents)  #sentence tokens
    return tokens, token_list, sentences


def filter_modify_tokens(tokens):
    """
    This function takes a collection of tokens from the nlp() function applied to text 
    and generates a list of filtered tokens that we then convert into a filtered text and 
    collection of filtered tokens.
    
    NOTE: still need to filter out super weird non words and may want to filter numbers and
    may want to find some important accronyms too (so maybe modify this function later)
    """
    #filter tokens, and make lowercase and lemmatize (with preprocess function):
    filtered_text_list = [
        preprocess_token(token) for token in tokens if is_token_allowed(token)
    ]

    filtered_text = ' '.join(filtered_text_list)
    filtered_tokens = nlp(filtered_text)
    return filtered_tokens


def make_filtered_tokens_from_ndc(ndc_dict):
    """
    Takes an NDC dictionary and processes the topics and keywords for searching within the documents, 
    assuming we are searching for individual words and to process the words in the same way are processing 
    the document text to have the best chance of finding keywords in the documents.
    """
    ndc_dict_processed = dict()
    unique_keywords = []
    for i in range(0, len(list(ndc_dict.keys()))):
        topic = list(ndc_dict.keys())[i]
        keywords = list(ndc_dict.values())[i]
        #add keywords from topic (key) to list of values and tokenize those values:
        keywords.append(topic)
        keywords_tokens = nlp(' '.join(keywords))
        #generate a filtered list of keywords using the same token preprocessing we use in the documents
        keywords_tokens_list = [
            str(token) for token in filter_modify_tokens(keywords_tokens)
        ]
        #filter non-unique words generated by splitting terms with shared words (e.g. two types of 'plan')
        unique_keywords = list(OrderedDict.fromkeys(keywords_tokens_list))
        ndc_dict_processed[topic] = unique_keywords
    return ndc_dict_processed