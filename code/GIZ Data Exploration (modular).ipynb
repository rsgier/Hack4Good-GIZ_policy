{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "704be11d",
   "metadata": {},
   "source": [
    "### GIZ Initial Data Exploration\n",
    "#### author: Emily Robitschek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ca671e",
   "metadata": {},
   "source": [
    "Purpose: Before we build any model, we need to be able to take a look at the documents we have.\n",
    "\n",
    "Some resources: \n",
    "\n",
    "#### papers mentioned in project proposal: \n",
    "https://medium.com/fiscalnoteworthy/citing-your-sources-79062248f468\n",
    "https://www2.deloitte.com/us/en/insights/focus/cognitive-technologies/natural-language-processing-examples-in-government-data.html\n",
    "https://documents1.worldbank.org/curated/en/634591516387264234/pdf/WPS8310.pdf\n",
    "\n",
    "#### NLP related links: \n",
    "- https://spacy.io/usage/spacy-101\n",
    "- https://towardsdatascience.com/text-pre-processing-stop-words-removal-using-different-libraries-f20bac19929a\n",
    "- https://arunm8489.medium.com/getting-started-with-natural-language-processing-6e593e349675\n",
    "- https://towardsdatascience.com/natural-language-processing-pipeline-decoded-f97a4da5dbb7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a581e19b",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2d30a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-26T06:19:56.314885Z",
     "start_time": "2021-10-26T06:19:55.898543Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import json\n",
    "import codecs\n",
    "\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#set up packages for processing data types and for NLP analysis\n",
    "from collections import OrderedDict\n",
    "import contractions\n",
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm') #or the multi-language one: spacy.load('xx_ent_wiki_sm')\n",
    "\n",
    "#graphing/visualization packages: \n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "#import the modular scripts \n",
    "import datahelper as dh #list_docs, read_docs_to_df\n",
    "import nlppreprocess as nlpp #fix_contractions, is_token_allowed, preprocess_token, preprocess_doc, filter_modify_tokens, make_filtered_tokens_from_ndc\n",
    "import nlpanalysis as nlpa #calculate_topic_frequency_subset, plot_word_freq_barchart_ndc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334ff084",
   "metadata": {},
   "source": [
    "### Define helper functions for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbb2783",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-12T13:12:05.915417Z",
     "start_time": "2021-10-12T13:12:05.894162Z"
    }
   },
   "outputs": [],
   "source": [
    "#input data helper functions\n",
    "def get_docs_df_from_folder(policy_doc_folder):\n",
    "    \"\"\"\n",
    "    Takes in a folder (can also be with different subfolders) with policy-related text documents \n",
    "    and gathers txt docs to analyze from those folders and makes a dataframe of their names and paths.\n",
    "    \n",
    "    NOTE: If want to preserve names and paths of the documents and make them easily searchable, it might be useful \n",
    "    to export the dictionary/keep that as well to add more summary information about the document for instance. \n",
    "    \"\"\"\n",
    "    #get the paths and file names\n",
    "    policy_doc_names, policy_doc_paths = list_docs(policy_doc_folder)\n",
    "    #print the number of docs and the names of some of them \n",
    "    print((\"There are %d policy docs\" % (len(policy_doc_names))),\n",
    "          \"Some of the policy docs include: \", policy_doc_names[:10])\n",
    "\n",
    "    policy_doc_dict = {'policy_doc_names': policy_doc_names, 'policy_doc_paths': policy_doc_paths}\n",
    "    policy_doc_df = pd.DataFrame(data=policy_doc_dict, dtype='string')\n",
    "    #set index as policy doc names (can clean up/add other column with a neater name without the .txt pieces later)\n",
    "    policy_doc_df['policy_doc_name_clean'] = (policy_doc_df['policy_doc_names']\n",
    "                                              .apply(lambda x: x.split('.txt')[0].split('.pdf.ocr')[0]))\n",
    "    policy_doc_df.index = policy_doc_df['policy_doc_names']\n",
    "    del policy_doc_df['policy_doc_names'] #remove duplicate column\n",
    "    return policy_doc_df\n",
    "\n",
    "def list_docs(folder):\n",
    "    \"\"\"\n",
    "    Generates a list of document names for reference and tracking. \n",
    "    This command currently extracts the .txt documents from all the subfolders of a parent folder, \n",
    "    and filters out the ones containing source information, which we might not want to use in our analysis.\n",
    "    \"\"\"\n",
    "    doc_names = []\n",
    "    doc_paths = []\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt') and (file not in ['Source.txt', 'Source Link.txt', 'Source Links.txt']):\n",
    "                doc_names.append(file)\n",
    "                doc_paths.append(os.path.join(root, file))  \n",
    "    return doc_names, doc_paths\n",
    "\n",
    "#NLP related helper functions\n",
    "#used these resources: \n",
    "#https://www.geeksforgeeks.org/nlp-expand-contractions-in-text-processing/)\n",
    "#https://realpython.com/natural-language-processing-spacy-python/#how-to-download-models-and-data\n",
    "\n",
    "def fix_contractions(document): \n",
    "    \"\"\"\n",
    "    Switches contractions like can't to cannot so potentially important words/pieces of words are \n",
    "    removed with punctuation removal.\n",
    "    \"\"\"\n",
    "    # original contracted text\n",
    "    # # # TODO: REMOVE THIS AFTER MODULIZATION\n",
    "    with codecs.open(document, errors=\"ignore\", encoding=\"utf8\") as f:\n",
    "        text = f.read()\n",
    "    # # #\n",
    "        \n",
    "    #creating an empty list\n",
    "    expanded_words = []    \n",
    "    for word in text.split():\n",
    "        #using contractions.fix to expand the shortened words \n",
    "        expanded_words.append(contractions.fix(word))   \n",
    "\n",
    "    expanded_text = ' '.join(expanded_words)\n",
    "    return expanded_text\n",
    "\n",
    "def is_token_allowed(token):\n",
    "    '''\n",
    "        Only allow valid tokens which are not stop words\n",
    "        and punctuation symbols.\n",
    "    '''\n",
    "    if (not token or not token.text.strip() or\n",
    "        token.is_stop or token.is_punct):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def preprocess_token(token):\n",
    "    # Reduce token to its lowercase lemma form\n",
    "    return token.lemma_.strip().lower()\n",
    "\n",
    "def preprocess_doc(doc_path): \n",
    "    \"\"\"\n",
    "    Applies NLP framework to a document returns the word tokens, a list of the word tokens, and the sentence tokens.\n",
    "    NOTE: may want to also return sentences and other objects too depending on the use case\n",
    "    \"\"\"\n",
    "    #remove contracted words and tokenize the document\n",
    "    tokens = nlp(fix_contractions(doc_path)) #word tokens\n",
    "    token_list = [token for token in tokens]\n",
    "    sentences = list(tokens.sents) #sentence tokens\n",
    "    return tokens, token_list, sentences\n",
    "\n",
    "def filter_modify_tokens(tokens):\n",
    "    \"\"\"\n",
    "    This function takes a collection of tokens from the nlp() function applied to text \n",
    "    and generates a list of filtered tokens that we then convert into a filtered text and \n",
    "    collection of filtered tokens.\n",
    "    \n",
    "    NOTE: still need to filter out super weird non words and may want to filter numbers and\n",
    "    may want to find some important accronyms too (so maybe modify this function later)\n",
    "    \"\"\"\n",
    "    #filter tokens, and make lowercase and lemmatize (with preprocess function): \n",
    "    filtered_text_list = [preprocess_token(token) for token in \n",
    "                       tokens if is_token_allowed(token)]\n",
    "\n",
    "    filtered_text = ' '.join(filtered_text_list)\n",
    "    filtered_tokens = nlp(filtered_text)\n",
    "    return filtered_tokens\n",
    "\n",
    "def make_filtered_tokens_from_ndc(ndc_dict): \n",
    "    \"\"\"\n",
    "    Takes an NDC dictionary and processes the topics and keywords for searching within the documents, \n",
    "    assuming we are searching for individual words and to process the words in the same way are processing \n",
    "    the document text to have the best chance of finding keywords in the documents.\n",
    "    \"\"\"\n",
    "    ndc_dict_processed = dict()\n",
    "    unique_keywords = []\n",
    "    for i in range(0, len(list(ndc_dict.keys()))):\n",
    "        topic = list(ndc_dict.keys())[i]\n",
    "        keywords = list(ndc_dict.values())[i]\n",
    "        #add keywords from topic (key) to list of values and tokenize those values: \n",
    "        keywords.append(topic)\n",
    "        keywords_tokens = nlp(' '.join(keywords))\n",
    "        #generate a filtered list of keywords using the same token preprocessing we use in the documents\n",
    "        keywords_tokens_list = [str(token) for token in filter_modify_tokens(keywords_tokens)]\n",
    "        #filter non-unique words generated by splitting terms with shared words (e.g. two types of 'plan')\n",
    "        unique_keywords = list(OrderedDict.fromkeys(keywords_tokens_list))\n",
    "        ndc_dict_processed[topic] = unique_keywords\n",
    "    return ndc_dict_processed\n",
    "\n",
    "def calculate_word_freq_ndc(word_freq, ndc_dict, key): \n",
    "    \"\"\"\n",
    "    Input: The word frequencies calculated by the Counter for the whole document (word_freq), \n",
    "    the dictionary of ndc key words organized by topic (ndc_dict), and the ndc topic (key).\n",
    "    Output: Pull out the word frequencies (word_scores) for each of the NDC words associated \n",
    "    with a topic (words) for graphing.\n",
    "    \"\"\"\n",
    "    words = ndc_dict[key]\n",
    "    word_scores = []\n",
    "    for word in words: \n",
    "        word_scores.append(word_freq[word])\n",
    "    print((\"This document has the following number of words related to %s NDCs: \" % (key)),\n",
    "            sum(word_scores), '\\n')\n",
    "    return words, word_scores \n",
    "\n",
    "def graph_word_freq_ndc(words, word_scores, ndc_name, doc_name, output_folder, save = False):\n",
    "    \"\"\"\n",
    "    Input: The word frequencies (word_scores) for the NDC words associated \n",
    "    with a topic (words) for graphing, including the associated NDC topic (ndc_name) and \n",
    "    document name (doc_name) to include in graph and file name for output to the output_folder. \n",
    "    Output: Bar graph in the output folder of word frequencies for the NDC words associated \n",
    "    with a topic/theme.\n",
    "    \"\"\"\n",
    "    #input data\n",
    "    x=words\n",
    "    y=word_scores\n",
    "    x_pos = [i for i, _ in enumerate(x)]\n",
    "    \n",
    "    #set plot parameters\n",
    "    plt.rcParams[\"figure.figsize\"] = ((len(words)/3),4)\n",
    "    plt.bar(x, y, color='mediumseagreen')\n",
    "    plt.xlabel(\"NDC words: %s\" % (ndc_name))\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    title = (\"%s NDC words in: %s\" % (ndc_name, doc_name))\n",
    "    plt.title(title)\n",
    "    plt.xticks(x_pos, x, rotation=90)\n",
    "    if save:\n",
    "        plt.savefig((graphs_folder + 'bar_chart_%s.pdf' % (title)), \n",
    "                    bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e16b60",
   "metadata": {},
   "source": [
    "### Import data: Keywords from NDCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6accc9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-12T13:12:05.983715Z",
     "start_time": "2021-10-12T13:12:05.917360Z"
    }
   },
   "outputs": [],
   "source": [
    "#keywords (just for testing purposes - from the policy proposal from GIZ)\n",
    "policy = [\"policy\", \"integrate\", \"implement\", \"committee\", \"consultation\"]\n",
    "food = [\"nutritions\", \"diets\", \"farm\", \"agriculture\", \"ecology\"]\n",
    "ndc_national_adaption_plan = [\"nap\", \"sector plan\", \"nccrp\", \"vulnerable sector\", \n",
    "                              \"geographic vulnerability\"]\n",
    "ndc_climate_change = [\"adaption\", \"program\", \"projects\", \"resilience\", \"institution\",\n",
    "                      \"capacity\", \"response\", \"budget\", \"reprioritisation\", \"development\", \n",
    "                      \"planner\", \"regulator\", \"practitioners\", \"geographical\", \n",
    "                      \"circumstances\", \"land\", \"scheme\", \"authorisation\", \"system\", \n",
    "                      \"spluma\"]\n",
    "ndc_early_warning = [\"system\", \"vulnerability\", \"needs\", \"assessment\", \"network\", \"weather\",\n",
    "   \"earth\", \"observation\", \"academic\", \"community\"]\n",
    "\n",
    "#keywords from json file\n",
    "json_keywords_SA_file = '../ndc_keywords/ndc_south_africa.json'\n",
    "keywords_SA_dict = None\n",
    "with open(json_keywords_SA_file, 'r') as f: \n",
    "    keywords_SA_dict = json.load(f)\n",
    "keywords_SA_dict\n",
    "\n",
    "ndc_dict = make_filtered_tokens_from_ndc(keywords_SA_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e306e11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-12T13:12:05.989841Z",
     "start_time": "2021-10-12T13:12:05.986858Z"
    }
   },
   "outputs": [],
   "source": [
    "### THIS IS A PLACEHOLDER ###\n",
    "#get words directly from NDCs (eventually want to get words from the NDC itself and group them by topics.)\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f87631",
   "metadata": {},
   "source": [
    "### Import data: Policy-related documents to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df47609",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-12T13:12:06.018498Z",
     "start_time": "2021-10-12T13:12:05.992039Z"
    }
   },
   "outputs": [],
   "source": [
    "#here is the general folder with the different types of policy documents\n",
    "policy_doc_folder = '../../giz-policy_tracking_docs/SouthAfrica/Data'\n",
    "#get df of docs\n",
    "policy_doc_df = get_docs_df_from_folder(policy_doc_folder)\n",
    "policy_doc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57d7cae",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdd64a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-12T13:12:16.090917Z",
     "start_time": "2021-10-12T13:12:06.020684Z"
    }
   },
   "outputs": [],
   "source": [
    "##lets take a look at the document from South Africa used in the example in the proposal first:\n",
    "doc_name = '2nd National Biodiversity Strategy Action Plan 2015-2025.pdf_ocr.txt'\n",
    "doc_path = policy_doc_df.loc[doc_name]['policy_doc_paths']\n",
    "\n",
    "#can also simply specify the first or whichever document in the list by order: \n",
    "#(this will be helpful for future versions that involve more interrating over different documents)\n",
    "#i=53\n",
    "#doc_name = policy_doc_df.iloc[i]['policy_doc_name_clean']\n",
    "#doc_path = policy_doc_df.iloc[i]['policy_doc_paths']\n",
    "print(doc_name, doc_path)\n",
    "tokens, token_list, sentences = preprocess_doc(doc_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300b2e61",
   "metadata": {},
   "source": [
    "#### The token object: \n",
    "The tokens have all sorts of useful information association with them, for instance their positions (in token.idx) which we can use these later to define windows. See below for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a514aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-12T13:12:16.097438Z",
     "start_time": "2021-10-12T13:12:16.093151Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for token in token_list[:20]:\n",
    "    print (token, token.idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89452ae3",
   "metadata": {},
   "source": [
    "We can see from above that the tokens need to be filtered and it might be useful if the words are all made lowercase and the words are lemmatized so the different forms of a word are recognized as the same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e2e270",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-12T13:12:20.367905Z",
     "start_time": "2021-10-12T13:12:16.099534Z"
    }
   },
   "outputs": [],
   "source": [
    "filtered_tokens = filter_modify_tokens(tokens)\n",
    "print('These are some of the filtered tokens: ', filtered_tokens[200:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddeddce",
   "metadata": {},
   "source": [
    "### Find most common (and unique) words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bb1aa3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-12T13:12:20.385627Z",
     "start_time": "2021-10-12T13:12:20.369577Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# Remove stop words and punctuation symbols\n",
    "words = [token.text for token in filtered_tokens]\n",
    "word_freq = Counter(words)\n",
    "# 5 commonly occurring words with their frequencies\n",
    "common_words = word_freq.most_common(30)\n",
    "print(common_words)\n",
    "\n",
    "# Unique words\n",
    "#unique_words = [word for (word, freq) in word_freq.items() if freq == 1]\n",
    "#print (unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada47c6b",
   "metadata": {},
   "source": [
    "### Make some plots of the NDC/Thematic key words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f0ef52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-12T13:12:20.947770Z",
     "start_time": "2021-10-12T13:12:20.389140Z"
    }
   },
   "outputs": [],
   "source": [
    "#just to test - need to modularise/make more reproducible and tailor output to be useful across documents\n",
    "#could also link this back to the df of the documents to output a table with some summary metrics for keywords \n",
    "#in eaach document of interest\n",
    "\n",
    "graphs_folder = '../../outputs/test_plots/bar_charts/'    \n",
    "    \n",
    "for key in ndc_dict.keys(): \n",
    "    print(\"Graphing the occurences of %s words in the document\" % key)\n",
    "    words, word_scores = calculate_word_freq_ndc(word_freq, ndc_dict, str(key))\n",
    "    graph_word_freq_ndc(words, word_scores, str(key), doc_name, graphs_folder) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900e8fb0",
   "metadata": {},
   "source": [
    "### Lets take a closer look at the climate change NDC keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462068c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-12T13:12:21.190160Z",
     "start_time": "2021-10-12T13:12:20.949767Z"
    }
   },
   "outputs": [],
   "source": [
    "for key in ndc_dict.keys(): \n",
    "    if key == 'climate change':\n",
    "        print(\"Graphing the occurences of %s words in the document\" % key)\n",
    "        words, word_scores = calculate_word_freq_ndc(word_freq, ndc_dict, str(key))\n",
    "        graph_word_freq_ndc(words, word_scores, str(key), doc_name, graphs_folder) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0acbd5",
   "metadata": {},
   "source": [
    "### Where do these words appear in the document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcf73db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-12T13:12:21.235879Z",
     "start_time": "2021-10-12T13:12:21.191867Z"
    }
   },
   "outputs": [],
   "source": [
    "#[token.idx in tokens for token.text in words]\n",
    "ndc_climate_idxs = [token.idx for token in tokens if token.text in words]\n",
    "ndc_climate_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b427ef99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T14:10:58.274266Z",
     "start_time": "2021-10-09T14:10:58.266729Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80c7511c",
   "metadata": {},
   "source": [
    "### Parts of speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b40fc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-12T13:12:21.259155Z",
     "start_time": "2021-10-12T13:12:21.237453Z"
    }
   },
   "outputs": [],
   "source": [
    "for token in filtered_tokens[:50]:\n",
    "    print(token, token.tag_, token.pos_, spacy.explain(token.tag_))\n",
    "print('\\n')\n",
    "    \n",
    "nouns = []\n",
    "adjectives = []\n",
    "for token in filtered_tokens:\n",
    "    if token.pos_ == 'NOUN':\n",
    "        nouns.append(token)\n",
    "    if token.pos_ == 'ADJ':\n",
    "        adjectives.append(token)\n",
    "\n",
    "print('some nouns spacy called in the document include:', '\\n', nouns[:50], '\\n')\n",
    "print('some adjectives spacy called in the document include:', '\\n', adjectives[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b519308c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T11:56:38.400728Z",
     "start_time": "2021-10-09T11:56:38.397730Z"
    }
   },
   "source": [
    "### Entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773f4581",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-12T13:12:21.278070Z",
     "start_time": "2021-10-12T13:12:21.260477Z"
    }
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "#can display entity property for the tokens as well: \n",
    "entities=[(i, i.label_, i.label) for i in filtered_tokens[1400:1700].ents]\n",
    "print(entities[:10])\n",
    "\n",
    "token_subset = tokens[100:500]\n",
    "displacy.render(token_subset, style = \"ent\", jupyter = True) #use original tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d245d4",
   "metadata": {},
   "source": [
    "### Dependency visualization in sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24796c38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-12T13:12:21.332366Z",
     "start_time": "2021-10-12T13:12:21.279500Z"
    }
   },
   "outputs": [],
   "source": [
    "sent_example = nlp(\"For example, it is estimated that between 9 and 12 million DATE people in impoverished rural areas directly use natural resources such as fuel wood, wild fruits and wooden utensils as a source of energy, food and building material respectively (Shackleton ORG 2004)\")\n",
    "sentence_spans = list(sentences)\n",
    "sentence_spans[:10]\n",
    "displacy.render(sentence_spans[80], style=\"dep\", jupyter= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfcc675",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b467681",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T16:54:48.667689Z",
     "start_time": "2021-10-09T16:54:48.659224Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d940a0dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T14:10:59.123635Z",
     "start_time": "2021-10-09T14:10:58.811085Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c6da743",
   "metadata": {},
   "source": [
    "### Experimenting with gensim and LDA modelling \n",
    "Initial results dont look great at the document level for single documents and the corpus, and highlight the need for better preprocesing and whether its better to do LDA on snippets rather than whole documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fbc745",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-12T13:12:31.397879Z",
     "start_time": "2021-10-12T13:12:21.334343Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_nlp_processing_framework(doc_name, doc_path):\n",
    "    \"\"\"Apply the NLP procesing functions to each document\"\"\"\n",
    "    tokens, token_list, sentences = preprocess_doc(doc_path)\n",
    "    filtered_tokens = filter_modify_tokens(tokens) #remember there is still a lot of weird stuff in here. \n",
    "    filtered_token_text = [token.text for token in filtered_tokens]\n",
    "    return filtered_token_text\n",
    "\n",
    "doc_list = []\n",
    "\n",
    "#for i in range(0, len(policy_doc_df)):\n",
    "for i in range(0, 5): \n",
    "    doc_name = policy_doc_df.iloc[i]['policy_doc_name_clean'] \n",
    "    doc_path = policy_doc_df.iloc[i]['policy_doc_paths']\n",
    "    print(i, doc_name, doc_path)\n",
    "    filtered_token_text = apply_nlp_processing_framework(doc_name, doc_path)\n",
    "    doc_list.append(filtered_token_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a74bdb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-12T13:12:31.936039Z",
     "start_time": "2021-10-12T13:12:31.399346Z"
    }
   },
   "outputs": [],
   "source": [
    "###using this resouce: https://towardsdatascience.com/building-a-topic-modeling-pipeline-with-spacy-and-gensim-c5dc03ffc619\n",
    "###and this resource: https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import LdaModel\n",
    "from pprint import pprint\n",
    "\n",
    "#just perform LDA for a few documents for now:\n",
    "print(doc_list[0][:15], doc_list[1][:15], doc_list[2][:15])\n",
    "\n",
    "# Creates, which is a mapping of word IDs to words.\n",
    "dictionary_LDA = corpora.Dictionary(doc_list)\n",
    "# Turns each document into a bag of words.\n",
    "corpus = [dictionary_LDA.doc2bow(list_of_tokens) for list_of_tokens in doc_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66238c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-12T13:12:32.370418Z",
     "start_time": "2021-10-12T13:12:31.937777Z"
    }
   },
   "outputs": [],
   "source": [
    "num_topics = 20\n",
    "%time lda_model = LdaModel(corpus, num_topics=num_topics, \\\n",
    "                                  id2word=dictionary_LDA, \\\n",
    "                                  passes=4, alpha=[0.01]*num_topics, \\\n",
    "                                  eta=[0.01]*len(dictionary_LDA.keys()))\n",
    "    \n",
    "pprint(lda_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1138d50b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
