{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "704be11d",
   "metadata": {},
   "source": [
    "### GIZ Initial Data Exploration\n",
    "#### author: Emily Robitschek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ca671e",
   "metadata": {},
   "source": [
    "Purpose: Before we build any model, we need to be able to take a look at the documents we have. This notebook includes functions to dive deep into the sections of a single document and the windows around the NDC keywords (in addition to giving a document overview). \n",
    "\n",
    "Some resources: \n",
    "\n",
    "#### papers mentioned in project proposal: \n",
    "https://medium.com/fiscalnoteworthy/citing-your-sources-79062248f468\n",
    "https://www2.deloitte.com/us/en/insights/focus/cognitive-technologies/natural-language-processing-examples-in-government-data.html\n",
    "https://documents1.worldbank.org/curated/en/634591516387264234/pdf/WPS8310.pdf\n",
    "\n",
    "#### NLP related links: \n",
    "- https://spacy.io/usage/spacy-101\n",
    "- https://towardsdatascience.com/text-pre-processing-stop-words-removal-using-different-libraries-f20bac19929a\n",
    "- https://arunm8489.medium.com/getting-started-with-natural-language-processing-6e593e349675\n",
    "- https://towardsdatascience.com/natural-language-processing-pipeline-decoded-f97a4da5dbb7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a581e19b",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2d30a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:27.718385Z",
     "start_time": "2021-11-04T22:28:25.061763Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import json\n",
    "import codecs\n",
    "import re\n",
    "\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#set up packages for processing data types and for NLP analysis\n",
    "from collections import OrderedDict, Counter\n",
    "import contractions\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm') #or the multi-language one: spacy.load('xx_ent_wiki_sm')\n",
    "\n",
    "#from n_gram_correlation import NGramCorrelateSpacy\n",
    "\n",
    "#graphing/visualization packages: \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334ff084",
   "metadata": {},
   "source": [
    "### Define helper functions for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbb2783",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:28.114906Z",
     "start_time": "2021-11-04T22:28:27.720531Z"
    }
   },
   "outputs": [],
   "source": [
    "from datahelper import *\n",
    "from nlppreprocess import *\n",
    "from nlpanalysis import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e16b60",
   "metadata": {},
   "source": [
    "### Import data: Keywords from NDCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6accc9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:28.158053Z",
     "start_time": "2021-11-04T22:28:28.116076Z"
    }
   },
   "outputs": [],
   "source": [
    "#keywords from json file\n",
    "json_keywords_SA_file = '../ndc_keywords/ndc_south_africa.json'\n",
    "keywords_SA_dict = None\n",
    "with open(json_keywords_SA_file, 'r') as f: \n",
    "    keywords_SA_dict = json.load(f)\n",
    "keywords_SA_dict\n",
    "ndc_dict = make_filtered_tokens_from_ndc(keywords_SA_dict)\n",
    "ndc_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319aa968",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:28.165958Z",
     "start_time": "2021-11-04T22:28:28.160530Z"
    }
   },
   "source": [
    "### Import keywords to label topics around NDCs within the documents for policy coherence - these can be modified as desired, for instance a \"mention_money\" category was created from scratch and added to the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbe7ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdg_ontology = pd.read_csv('../additional_resources/Ontology_final_modified.csv', sep=';')#, #skiprows=0)\n",
    "sdg_ontology.head(20)\n",
    "\n",
    "# uncomment for example keyword list for SDG1:  \n",
    "#SDG1_keywords = list(sdg_ontology[sdg_ontology['clasification']=='SDG1']['keyword'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f87631",
   "metadata": {},
   "source": [
    "### Import data: Policy-related documents to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df47609",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:28.176885Z",
     "start_time": "2021-11-04T22:28:28.167255Z"
    }
   },
   "outputs": [],
   "source": [
    "#here is the general folder with the different types of policy documents\n",
    "policy_doc_folder = '../test_resources'\n",
    "#get df of docs\n",
    "policy_doc_df = read_docs_to_df(policy_doc_folder)\n",
    "policy_doc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57d7cae",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdd64a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:35.772740Z",
     "start_time": "2021-11-04T22:28:28.178107Z"
    }
   },
   "outputs": [],
   "source": [
    "##lets take a look at the document from South Africa used in the example in the proposal first:\n",
    "doc_name = '2nd National Biodiversity Strategy Action Plan 2015-2025.pdf_ocr.txt'\n",
    "doc_path = policy_doc_df.loc[doc_name]['policy_doc_paths']\n",
    "\n",
    "#can also simply specify the first or whichever document in the list by order: \n",
    "#(this will be helpful for future versions that involve more interrating over different documents)\n",
    "#i=53\n",
    "#doc_name = policy_doc_df.iloc[i]['policy_doc_name_clean']\n",
    "#doc_path = policy_doc_df.iloc[i]['policy_doc_paths']\n",
    "print(doc_name, doc_path)\n",
    "tokens, token_list, sentences = preprocess_doc(doc_path)\n",
    "\n",
    "#cut out long words for this particular document: (2nd National Biodiversity Strategy Action Plan 2015-2025.pdf_ocr.txt) \n",
    "def make_window_text(tokens, max_length=30):\n",
    "    filtered_for_length = [token.text.lower() for token in tokens if len(token) < max_length]\n",
    "    text_for_windows = ' '.join(filtered_for_length)\n",
    "    window_tokens = nlp(text_for_windows)\n",
    "    return window_tokens\n",
    "\n",
    "tokens = make_window_text(tokens, max_length=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300b2e61",
   "metadata": {},
   "source": [
    "#### The token object: \n",
    "The tokens have all sorts of useful information association with them, for instance their positions (in token.idx) which we can use these later to define windows. See below for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a514aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:35.778214Z",
     "start_time": "2021-11-04T22:28:35.774355Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for token in token_list[:20]:\n",
    "    print (token, token.idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89452ae3",
   "metadata": {},
   "source": [
    "We can see from above that the tokens need to be filtered and it might be useful if the words are all made lowercase and the words are lemmatized so the different forms of a word are recognized as the same thing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89be9860",
   "metadata": {},
   "source": [
    "### Note: we didn't end up using the filtered tokens for some of the methods, but we tried doing some LDA and the intial results weren't great, but that sort of method requires more text preprocessing like in the filtered tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e2e270",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:39.197908Z",
     "start_time": "2021-11-04T22:28:35.779411Z"
    }
   },
   "outputs": [],
   "source": [
    "filtered_tokens = filter_modify_tokens(tokens)\n",
    "print('These are some of the filtered tokens: ', filtered_tokens[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddeddce",
   "metadata": {},
   "source": [
    "### Find most common (and unique) words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bb1aa3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:39.216487Z",
     "start_time": "2021-11-04T22:28:39.199518Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove stop words and punctuation symbols\n",
    "words = [token.text for token in filtered_tokens]\n",
    "word_freq = Counter(words)\n",
    "# 5 commonly occurring words with their frequencies\n",
    "common_words = word_freq.most_common(30)\n",
    "print(common_words)\n",
    "\n",
    "# Unique words\n",
    "#unique_words = [word for (word, freq) in word_freq.items() if freq == 1]\n",
    "#print (unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada47c6b",
   "metadata": {},
   "source": [
    "### Make some plots of the NDC/Thematic key words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f0ef52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:39.785606Z",
     "start_time": "2021-11-04T22:28:39.220235Z"
    }
   },
   "outputs": [],
   "source": [
    "#just to test - need to modularise/make more reproducible and tailor output to be useful across documents\n",
    "#could also link this back to the df of the documents to output a table with some summary metrics for keywords \n",
    "#in each document of interest\n",
    "\n",
    "graphs_folder = '../../outputs/bar_charts/'    \n",
    "    \n",
    "for key in ndc_dict.keys(): \n",
    "    print(\"Graphing the occurences of %s words in the document\" % key)\n",
    "    topic_frequencies =  calculate_topic_frequency_subset(word_freq, ndc_dict, str(key))\n",
    "    plot_word_freq_barchart_ndc(topic_frequencies, str(key), doc_name, graphs_folder) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900e8fb0",
   "metadata": {},
   "source": [
    "### Lets take a closer look at the climate change NDC keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0acbd5",
   "metadata": {},
   "source": [
    "### Where do these words appear in the document?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f19d46",
   "metadata": {},
   "source": [
    "#### Make dataframe for easy graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d067e20e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T08:26:52.345345Z",
     "start_time": "2021-11-05T08:26:52.146105Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_ndc_keyword_tidy_df_from_dict(key, ndc_dict, col_group_name='NDC'):\n",
    "    \"\"\"From the NDC dictionary (ndc_dict), makes a df for the category (key) of interest\"\"\"\n",
    "    ndc_df = pd.DataFrame({'keyword': ndc_dict[key], \n",
    "                       col_group_name: key})\n",
    "    return ndc_df\n",
    "\n",
    "def stack_tidy_ndc_dfs(ndc_dict, col_group_name='NDC'):\n",
    "    \"\"\"Stacks the dfs from the make_ndc_keyword_tidy_df_from_dict function, and creates a \n",
    "    category column (named with col_group_name) for which NDC category each word belongs to.\"\"\"\n",
    "    ndc_df = pd.DataFrame()\n",
    "    for key in ndc_dict.keys():\n",
    "        ndc_df_add = make_ndc_keyword_tidy_df_from_dict(key, ndc_dict, col_group_name)\n",
    "        ndc_df = pd.concat([ndc_df, ndc_df_add], axis=0)\n",
    "    return ndc_df\n",
    "\n",
    "def make_ndc_i_tidy_df(ndc_dict, tokens, col_group_name):\n",
    "    \"\"\"Creates a df of the spacy indexs in the document that match each keyword\"\"\"\n",
    "    ndc_idx_df = pd.DataFrame()\n",
    "    for key in ndc_dict.keys(): \n",
    "        ndc_idx_df_to_add = pd.DataFrame({col_group_name: key,\n",
    "                                          #('%s word_index'%(key)): [token.idx for token in tokens if token.text in ndc_dict[key]],\n",
    "                                          'word_index': [token.i for token in tokens if token.text in ndc_dict[key]]})\n",
    "        ndc_idx_df = pd.concat([ndc_idx_df, ndc_idx_df_to_add], axis=0)\n",
    "    return ndc_idx_df\n",
    "\n",
    "# lets apply: \n",
    "ndc_df = stack_tidy_ndc_dfs(ndc_dict, col_group_name='NDC')\n",
    "print(ndc_df.head())\n",
    "\n",
    "ndc_i_df = make_ndc_i_tidy_df(ndc_dict, tokens, col_group_name='NDC')\n",
    "ndc_i_df.index = ndc_i_df.NDC.copy()\n",
    "ndc_i_df.head()\n",
    "\n",
    "#if want to include the letter index as well for the window (this was the original index scheme \n",
    "#used for the presentation and other results)\n",
    "ndc_i_df['doc_position_idx_index'] = [tokens[int(i)].idx for i in list(ndc_i_df['word_index'])]\n",
    "ndc_i_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e898c178",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:40.217870Z",
     "start_time": "2021-11-04T22:28:40.215870Z"
    }
   },
   "source": [
    "## Part I: Finding NDC keywords in the document\n",
    "Three methods: \n",
    "    \n",
    "    1) uses the make_filtered_tokens_from_ndc method to create single lemmatized keywords that are preprocessed in the same way as the document text for flexible matching of single words (matches \"plan\" and \"plans\" and \"planned\" from original document for example). \n",
    "\n",
    "    2) uses the dictionary from the json file to find exact matches to the terms with the spacy Phrase matcher, which expands the searching to multiple word phrases from the input document. The entity matching, also addes functionality for easy and pretty visualization. \n",
    "\n",
    "    3) applies the token correlator to the NDC keywords from the json file for a particular topic to expand the keyword search terms based on the terms from the NDCs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67709544",
   "metadata": {},
   "source": [
    "### METHOD 1: Finding and matching single word stems from the NDC keywords to words in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015bf4a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:40.279908Z",
     "start_time": "2021-11-04T22:28:40.219372Z"
    }
   },
   "outputs": [],
   "source": [
    "NDC_topic = 'climate change'\n",
    "#the indices for the NDC keywords are found using this scheme:  (https://spacy.io/api/token)\n",
    "#using .i to get the word position of the token, and .idx gets the character offset of the token within the parent document.\n",
    "ndc_climate_idxs = [token.i for token in tokens if token.text in ndc_dict[NDC_topic]] \n",
    "print(ndc_climate_idxs[:20])\n",
    "\n",
    "#We can see what words they are with the following code: \n",
    "ndc_climate_words_in_doc = [token.text for token in tokens if token.text in ndc_dict[NDC_topic]]\n",
    "print(ndc_climate_words_in_doc[:20])\n",
    "\n",
    "#They indexes can be displayed in a nice way with the function to make the ndc_idx_df\n",
    "ndc_i_df['doc_position_idx_index'] = [tokens[int(i)].idx for i in list(ndc_i_df['word_index'])]\n",
    "ndc_i_df.head()\n",
    "ndc_i_df[ndc_i_df['NDC'] == NDC_topic].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8b2224",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11cf9032",
   "metadata": {},
   "source": [
    "### METHOD 2: Finding exact matches to the terms with the spacy Phrase matcher and entity labelling (also allows nice visualizations of the text with the labelled terms with displacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33a658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_ndc_spans(ndc_keywords, doc):\n",
    "    \"\"\"ndc_keywords is a dictionary, doc is the document text with nlp run on it that is the filtered but not lemmatized document text.\"\"\"\n",
    "    matcher = PhraseMatcher(nlp.vocab)\n",
    "    #iterate through NDC keys in NDC dictionary to create seperate label categories for the matching\n",
    "    entity_labels = []\n",
    "    for entity_reference in [key for key in ndc_keywords.keys()]:\n",
    "        entity_label = entity_reference + ' NDC'\n",
    "        #print(entity_label)\n",
    "        entity_labels.append(entity_label)\n",
    "        keywords = ndc_keywords[entity_reference]\n",
    "        patterns = [nlp(i) for i in keywords]\n",
    "        #print(keywords, patterns)\n",
    "        matcher.add(entity_label, None, *patterns)\n",
    "    matches = matcher(doc)\n",
    "        #label spans\n",
    "    for match_id, start, end in matches:\n",
    "        try:\n",
    "            span = Span(doc, start, end, label=match_id)\n",
    "            doc.ents = list(doc.ents) + [span]  # add span to doc.ents\n",
    "        except: \n",
    "            pass\n",
    "    return entity_labels, doc\n",
    "\n",
    "def make_ndc_i_df_from_spans(ndc_keywords, labelled_doc): \n",
    "    \"\"\"ndc_keywords is the ndc keyword dictionary and the labelled_doc is the one labelled with the spans of NDC keywords. \n",
    "    idx here refers to the start word token index of the spans.\"\"\"\n",
    "    ndc_idx_df = pd.DataFrame()\n",
    "    for entity_reference in [key for key in ndc_keywords.keys()]:\n",
    "        entity_label = entity_reference + ' NDC'\n",
    "        #print(entity_label) \n",
    "        ndc_idx_df_to_add = pd.DataFrame({'NDC': entity_reference,\n",
    "                                          ('word'): [ent.text for ent in labelled_doc.ents if ent.label_ == entity_label],\n",
    "                                          'word_index': [ent.start for ent in labelled_doc.ents if ent.label_ == entity_label]})\n",
    "        ndc_idx_df = pd.concat([ndc_idx_df, ndc_idx_df_to_add], axis=0)\n",
    "    return ndc_idx_df \n",
    "\n",
    "def label_keyword_spans(keyword_df, doc, topic_column='clasification'):\n",
    "    \"\"\"This function is similar to the function used to label the NDC spans in the document, but it labels all the keywords for the topics in the keyword df (in this example - mostly SDG topic keywords)\n",
    "    keyword_df is a pandas df, doc should be the document already labeled with ndc entities so both labels are kept.\"\"\"\n",
    "    matcher = PhraseMatcher(nlp.vocab)\n",
    "    #iterate through the keyword categories in the columns of the keyword_df to create labels for the matching\n",
    "    entity_labels = []\n",
    "    topics = list(keyword_df[topic_column].value_counts().index)\n",
    "    for topic in topics:\n",
    "        entity_labels.append(topic)\n",
    "        keywords = list(keyword_df[keyword_df[topic_column]==topic]['keyword'])\n",
    "        #print(keywords)\n",
    "        patterns = [nlp(i) for i in keywords]\n",
    "        #print(keywords, patterns)\n",
    "        matcher.add(topic, None, *patterns)\n",
    "    matches = matcher(doc)\n",
    "        #label spans\n",
    "    for match_id, start, end in matches:\n",
    "        try:\n",
    "            span = Span(doc, start, end, label=match_id)\n",
    "            doc.ents = list(doc.ents) + [span]  # add span to doc.ents\n",
    "        except: \n",
    "            pass\n",
    "    return entity_labels, doc\n",
    "\n",
    "def label_ndc_keyword_spans(ndc_keywords, doc, keyword_df, topic_column='clasification'):\n",
    "    \"\"\"combined version of label_ndc_spans and label_keyword_spans to only do one matching step\"\"\"\n",
    "    matcher = PhraseMatcher(nlp.vocab)\n",
    "    #iterate through NDC keys in NDC dictionary to create seperate label categories for the matching\n",
    "    ndc_entity_labels = []\n",
    "    for entity_reference in [key for key in ndc_keywords.keys()]:\n",
    "        entity_label = entity_reference + ' NDC'\n",
    "        #print(entity_label)\n",
    "        ndc_entity_labels.append(entity_label)\n",
    "        keywords = ndc_keywords[entity_reference]\n",
    "        patterns = [nlp(i) for i in keywords]\n",
    "        #print(keywords, patterns)\n",
    "        matcher.add(entity_label, None, *patterns)\n",
    "    #iterate through the keyword categories in the columns of the keyword_df to create labels for the matching\n",
    "    topics = list(keyword_df[topic_column].value_counts().index)\n",
    "    for topic in topics:\n",
    "        keywords = list(keyword_df[keyword_df[topic_column]==topic]['keyword'])\n",
    "        #print(keywords)\n",
    "        patterns = [nlp(i) for i in keywords]\n",
    "        #print(keywords, patterns)\n",
    "        matcher.add(topic, None, *patterns)\n",
    "    matches = matcher(doc)\n",
    "        #label spans\n",
    "    for match_id, start, end in matches:\n",
    "        try:\n",
    "            span = Span(doc, start, end, label=match_id)\n",
    "            doc.ents = list(doc.ents) + [span]  # add span to doc.ents\n",
    "        except: \n",
    "            pass\n",
    "    return ndc_entity_labels, topics, doc\n",
    "\n",
    "#Heres an example of how it works with just one sentence of text with keywords the finder should discover!\n",
    "test_doc = \"climate change resilience is very important to south africa and we need an adaption program and adaption projects to deal with it and increase institutional capacity. Our practices need to change to protect the planet. Biodiversity should be protected and health systems need to also be improved to fight against infectious diseases like malaria.\"\n",
    "entity_labels, ndc_labelled_doc = label_ndc_spans(keywords_SA_dict, nlp(test_doc))\n",
    "displacy.render(ndc_labelled_doc, style='ent')\n",
    "ndc_i_df_spans = make_ndc_i_df_from_spans(keywords_SA_dict, ndc_labelled_doc)\n",
    "\n",
    "#entity_labels, kw_labelled_doc = label_keyword_spans(sdg_ontology, ndc_labelled_doc)\n",
    "#displacy.render(kw_labelled_doc, style='ent')\n",
    "\n",
    "ndc_entity_labels, keyword_topics, all_labelled_doc = label_ndc_keyword_spans(keywords_SA_dict, nlp(test_doc), sdg_ontology)\n",
    "displacy.render(all_labelled_doc, style='ent')\n",
    "\n",
    "#if want to include the letter index as well for the window\n",
    "ndc_i_df_spans['doc_position_idx_index'] = [tokens[int(i)].idx for i in list(ndc_i_df_spans['word_index'])]\n",
    "ndc_i_df_spans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c30715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to actual policy document\n",
    "#ndc_category_labels, ndc_labelled_doc = label_ndc_spans(keywords_SA_dict, tokens)\n",
    "#sdg_keyword_labels, kw_labelled_doc = label_keyword_spans(sdg_ontology, ndc_labelled_doc)\n",
    "\n",
    "ndc_entity_labels, keyword_topics, all_labelled_doc = label_ndc_keyword_spans(keywords_SA_dict, tokens, sdg_ontology)\n",
    "displacy.render(all_labelled_doc[600:1000], style='ent')\n",
    "\n",
    "#the indices for the NDC keywords are found using this scheme: \n",
    "ndc_climate_idxs_from_span = [ent.start for ent in all_labelled_doc.ents if ent.label_ == (NDC_topic + ' NDC')]\n",
    "print(ndc_climate_idxs_from_span[:20])\n",
    "\n",
    "#We can see what words they are with the following code: \n",
    "ndc_climate_words_in_doc_from_span = [ent.text for ent in all_labelled_doc.ents if ent.label_ == (NDC_topic + ' NDC')]\n",
    "print(ndc_climate_words_in_doc_from_span[:20])\n",
    "\n",
    "#They can be displayed in a nice way with the function to make the ndc_i_df_from_the_spans:\n",
    "ndc_i_df_spans = make_ndc_i_df_from_spans(keywords_SA_dict, all_labelled_doc)\n",
    "ndc_i_df_spans[ndc_i_df_spans['NDC'] == NDC_topic].head(20)\n",
    "\n",
    "#if want to include the letter index as well for the window\n",
    "ndc_i_df_spans['doc_position_idx_index'] = [tokens[int(i)].idx for i in list(ndc_i_df_spans['word_index'])]\n",
    "ndc_i_df_spans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424fea2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##can see some of the rendered areas with NDC keywords and other topic keywords with the following command. One limitation or advantage of using the phrase matcher is that each found span can only be counted for one category whereas the patterns found using re might vary slightly due to the different rules\n",
    "for e in all_labelled_doc.ents[200:300]:\n",
    "    if e.label_ == (NDC_topic + ' NDC'): \n",
    "        displacy.render(all_labelled_doc[e.start-20:e.end+20], style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37efc630",
   "metadata": {},
   "source": [
    "### Method 3: Apply the token correlator to the NDC keywords from the json file for a particular topic to expand the keyword search terms based on the terms from the NDCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e082870f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### insert code here - this method was applied experimentally at the corpus level for instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e3010e",
   "metadata": {},
   "source": [
    "## PART II: Defining windows around NDC words \n",
    "### To systematically identify consistent windows around the NDC words we can do this manually with a particular word number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030843a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_window_i(ndc_word_index, tokens, size=20):\n",
    "    \"\"\"size is the number of words to include on either side of the NDC keyword whose position is given by the ndc_word_index. \n",
    "    The tokens are the original tokens in the document\"\"\"\n",
    "    lower_limit = ndc_word_index - size\n",
    "    upper_limit = ndc_word_index + size\n",
    "    token_indices = [token.i for token in tokens]\n",
    "    window_tokens = tokens[(ndc_word_index-size):(ndc_word_index+size)] \n",
    "    return lower_limit, upper_limit, window_tokens\n",
    "\n",
    "def return_window_idx(ndc_word_index, tokens, size=100):\n",
    "    \"\"\"Uses the .idx positions of the start of the invidual NDC words in the document to define the windows.\"\"\"\n",
    "    lower_limit = ndc_word_index - size\n",
    "    upper_limit = ndc_word_index + size\n",
    "    token_idxs = [token.idx for token in tokens]\n",
    "    window_token_list = []\n",
    "    #print('The window is ', lower_limit, upper_limit)\n",
    "    for index, idx in enumerate(token_idxs):\n",
    "        if (idx >= lower_limit) and (idx <= upper_limit):\n",
    "            window_token_list.append(tokens[index])\n",
    "        else:\n",
    "            pass\n",
    "    text_for_windows = ' '.join(list(token.text for token in window_token_list))\n",
    "    window_tokens = nlp(text_for_windows)\n",
    "    return lower_limit, upper_limit, window_tokens\n",
    "\n",
    "#example\n",
    "test_index=1988 #(each window may include multiple keywords because they may be close to each other)\n",
    "window_size = 40\n",
    "lower_limit, upper_limit, window_tokens = return_window_i(test_index, all_labelled_doc, size=window_size)\n",
    "displacy.render(window_tokens, style='ent') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55939297",
   "metadata": {},
   "source": [
    "#### No matter what method we use to find the keywords, some of the keywords will naturally occur very close together and we dont want to visualize each window more than once so for now we will filter the NDC word positions in the document for those that will fall within the same word window. (May eventually want to do this differently to prioritize windows with many keywords). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8c9acb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:40.286182Z",
     "start_time": "2021-11-04T22:28:40.281225Z"
    }
   },
   "outputs": [],
   "source": [
    "#filtering using word positions in document: \n",
    "def filter_i_for_overlap(word_idxs, min_dist=20):\n",
    "    \"\"\"Min distance is the minimum number of words that should seperate the NDC keyword whose positions are given in word_idxs \n",
    "    in order to create a seperate window from that index. It should be at least equal to the size value from the return_window_i function\"\"\"\n",
    "    distance_btwn_idxs = [(word_idxs[i+1]-word_idxs[i]) for i in range(0, len(word_idxs)-1)]\n",
    "    print(distance_btwn_idxs[:200])\n",
    "    filtered_idxs = []\n",
    "    for index, distance in enumerate(distance_btwn_idxs):\n",
    "        #print('index:', index, 'distance', distance)\n",
    "        if (distance >= min_dist):\n",
    "            filtered_idxs.append(word_idxs[index])\n",
    "        else:\n",
    "            pass\n",
    "    print(\"The number of times the idx words were found was: \", len(word_idxs), \"\\n\", \n",
    "          \"The number of idx words seperated by at least the min_distance was : \", len(filtered_idxs))\n",
    "    return filtered_idxs\n",
    "\n",
    "def filter_idx_for_overlap(idxs, min_dist):\n",
    "    distance_btwn_idxs = [(idxs[i+1]-idxs[i]) for i in range(0, len(idxs)-1)]\n",
    "    print(distance_btwn_idxs[:20])\n",
    "    filtered_idxs = []\n",
    "    for index, distance in enumerate(distance_btwn_idxs):\n",
    "        if (distance >= min_dist):\n",
    "            filtered_idxs.append(idxs[index])\n",
    "        else:\n",
    "            pass\n",
    "    print(\"The number of times the idx words were found was: \", len(idxs), \"\\n\", \n",
    "          \"The number of idx words seperated by at least the min_distance was : \", len(filtered_idxs))\n",
    "    return filtered_idxs\n",
    "    \n",
    "#filtering using letter positions in document (used with the NDC keywords from METHOD 1 for the presentation for Hack4Good, which is a more permissive search.)\n",
    "#however, using word based indexing is probably more robust to error and easier unless you want to compare the positions of keywords found by re and spans found by spacy directly\n",
    "ndc_climate_idxs_from_doc_pos = list(ndc_i_df[ndc_i_df['NDC'] == NDC_topic]['doc_position_idx_index'])\n",
    "idx_for_window_from_doc_pos = filter_idx_for_overlap(idxs=ndc_climate_idxs_from_doc_pos, min_dist=200)\n",
    "\n",
    "# Can see how the choice of search method changes the NDC associated words found, and we would expect the span searching which is more exact to find fewer results overall: \n",
    "#filtering using word positions in document (used with the NDC keywords from METHOD 2 to show difference with METHOD 1.)\n",
    "idx_for_window = filter_i_for_overlap(ndc_climate_idxs_from_span, min_dist=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c777e232",
   "metadata": {},
   "source": [
    "## PART III: Characterizing the topics mentioned and text context of windows around NDC words\n",
    "### This supervised approach based on pre-defined keywords with different topic categories (from the SDG Ontology uploaded at the beginning of this notebook) can be complemented with other methods in the future like LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34449bd",
   "metadata": {},
   "source": [
    "#### example using the spans and the word indexs from the token.i attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ed6ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sdg_df(sdg_list, sdg_ontology, text):\n",
    "    df_sdg = pd.DataFrame()\n",
    "    for sdg in list(sdg_list):\n",
    "        sdg_keywords = list(sdg_ontology[sdg_ontology['clasification']==sdg]['keyword'])\n",
    "        #print(sdg)\n",
    "        df_sdg_to_add = find_patterns_df(sdg_keywords, text, topic_name=sdg)\n",
    "        df_sdg = pd.concat([df_sdg, df_sdg_to_add])\n",
    "    return df_sdg\n",
    "\n",
    "#look at SDGs across document/at document level\n",
    "sdg_list = ['SDG1', 'SDG2', 'SDG3', 'SDG4', 'SDG5', 'SDG6', 'SDG7', 'SDG8', 'SDG9', \n",
    "            'SDG10', 'SDG11', 'SDG12', 'SDG13', 'SDG14', 'SDG15', 'SDG16', 'SDG17', \n",
    "            \"mention_money\"]\n",
    "\n",
    "def find_patterns_df(pattern_list, text, topic_name):\n",
    "    pattern_locations = []\n",
    "    pattern_num = []\n",
    "    for pattern in pattern_list:\n",
    "        #print(pattern)\n",
    "        re.findall(pattern, text, flags=0)\n",
    "        #pattern_locations = [(m.start(0), m.end(0)) for m in re.finditer(pattern, text)] #if want start and end\n",
    "        locations = [m.start(0) for m in re.finditer(pattern, text)]\n",
    "        pattern_locations.append(locations)\n",
    "        pattern_num.append(int(len(locations)))\n",
    "    #print(pattern_locations)\n",
    "        #if len(pattern_locations) > 0: \n",
    "        #    print(pattern, len(pattern_locations), pattern_locations)\n",
    "    return pd.DataFrame({'sdg_topic': topic_name,\n",
    "                         'sdg_keywords': pattern_list,\n",
    "                         'sdg_keywords_num': pattern_num,\n",
    "                         'sdg_keyword_locations': pattern_locations})\n",
    "\n",
    "window_size = 20\n",
    "count = 0\n",
    "for index in idx_for_window[0:len(idx_for_window)]:\n",
    "    count += 1\n",
    "    print(index)\n",
    "    window_tokens = return_window_i(index, all_labelled_doc, size=window_size)[2]\n",
    "    window_text = ' '.join([token.text for token in window_tokens])\n",
    "    #print(window_text)\n",
    "    df_sdg = make_sdg_df(sdg_list, sdg_ontology, window_text) #may want to keep this for the windows for a more granular analysis\n",
    "    df_sdg_sorted = df_sdg.sort_values(by=['sdg_keywords_num'], ascending=False)\n",
    "    #print(df_sdg_sorted.head())\n",
    "    print(list(df_sdg_sorted[df_sdg_sorted['sdg_keywords_num'] > 0]['sdg_keywords'][:10])) #print top 10 positive valued keywords\n",
    "    #print a summary of the SDG words found: \n",
    "    summary_sdg = df_sdg.groupby('sdg_topic')['sdg_keywords_num'].sum()\n",
    "    if count == 1: \n",
    "        summary_sdg_df = summary_sdg.to_frame().rename(columns={\"sdg_keywords_num\": (\"sdg_kw_%d\"% index)})\n",
    "    else: \n",
    "        summary_sdg_df_for_merge = summary_sdg.to_frame().rename(columns={\"sdg_keywords_num\": (\"sdg_kw_%d\"% index)})\n",
    "        summary_sdg_df = summary_sdg_df.merge(summary_sdg_df_for_merge, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0a67f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_sdg_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0067c1bb",
   "metadata": {},
   "source": [
    "### Can create a heatmap visualization to where the topics are rows and the columns are each window (where the columns on the left are from the start of the document and the ones on the right are from the end). The number of keyword mentions found for each topic within each window are used to fill in the heatmap and color it (showing results from METHOD 2 of labelling NDC words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83d6b37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:40.871444Z",
     "start_time": "2021-11-04T22:28:40.287814Z"
    }
   },
   "outputs": [],
   "source": [
    "#make heatmap of plot above\n",
    "plt.figure(figsize=(16, 6))\n",
    "ax = sns.heatmap(summary_sdg_df,\n",
    "                 #annot=True, fmt=\"d\", \n",
    "                 cmap=\"YlGnBu\")\n",
    "plt.xlabel(\"climate change NDC-associated windows\")\n",
    "plt.ylabel(\"SDG Topics\")\n",
    "title = (\"Distribution of topic keywords in climate change NDC-associated windows in %s\" % (doc_name))\n",
    "plt.title(title)\n",
    "file_name=(title + (' across all windows v1.png'))\n",
    "#plt.savefig((plot_folder+file_name), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf40b468",
   "metadata": {},
   "source": [
    "### can filter the heatmap visualization to focus on columns (each representing an NDC window) with the most topic keyword mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a470f4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_sdg_df_filtered = summary_sdg_df.loc[:, (summary_sdg_df.sum() >= 10)] #filter for windows with at least 15 topic keywords\n",
    "plt.figure(figsize=(20, 6))\n",
    "ax = sns.heatmap(summary_sdg_df_filtered, #can show all windows with summary_sdg_df\n",
    "                 annot=True, fmt=\"d\", \n",
    "                 cmap=\"YlGnBu\")\n",
    "plt.xlabel(\"climate change NDC-associated windows\")\n",
    "plt.ylabel(\"Topics\")\n",
    "title = (\"Distribution of topic keywords in climate change NDC-associated windows in %s\" % (doc_name))\n",
    "plt.title(title)\n",
    "file_name=(title + (' across windows with greater than 15 keywords v1.png'))\n",
    "#plt.savefig((plot_folder+file_name), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022d0a74",
   "metadata": {},
   "source": [
    "### coherence and relevance (with NDC positions from METHOD 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aea5516",
   "metadata": {},
   "outputs": [],
   "source": [
    "### compute coherence by summing the keywords that appear in NDC windows across categories (maybe introduce normalization and weighting by categories in the future)\n",
    "coherence_for_frame = summary_sdg_df.sum(axis=1)/len(summary_sdg_df.columns)\n",
    "coherence_df = coherence_for_frame.to_frame().rename(columns={0:\"Topic score\"})\n",
    "coherence_score = coherence_df['Topic score'].sum()\n",
    "print(coherence_score)\n",
    "coherence_df\n",
    "\n",
    "### compute relevance by estimating the number of ndc_climate_idxs per sentence in the document\n",
    "relevance_score = len(ndc_climate_idxs_from_span)/len(sentences) #example using the number of NDC mentions with METHOD 2 \n",
    "print('The relevance score for %s is' % doc_name , relevance_score)\n",
    "print('The coherence score for %s is' % doc_name , coherence_score)\n",
    "\n",
    "#print a summary of the SDG words found in the windows: \n",
    "plt.figure(figsize=(2, 6))\n",
    "ax = sns.heatmap(coherence_df, \n",
    "                 annot=True, #fmt=\"d\", \n",
    "                 cmap=\"YlGnBu\")\n",
    "\n",
    "#plt.xlabel(\"climate change NDC-associated windows\")\n",
    "#plt.ylabel(\"Topics\")\n",
    "title = (\"NDC-associated topic scores in %s\" % (doc_name))\n",
    "plt.title(title)\n",
    "file_name=(title + (' across document normalized by number of windows v1.png'))\n",
    "#plt.savefig((plot_folder+file_name), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36aa368",
   "metadata": {},
   "source": [
    "#### In this visualization for instance we can understand that there are on average 1.6 SDG15 related words per 40 word window and about 1.2 mentions of money per 40 word window near an NDC phrase for this document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af349eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:41.197077Z",
     "start_time": "2021-11-04T22:28:40.873746Z"
    }
   },
   "outputs": [],
   "source": [
    "#example of rendered windows (the NDC word/term will always be present at the word index associated with the window, but sometimes it is not properly labelled in the render, \n",
    "#probably due to how the span labelling function works in spacy and doesnt allow overlap, whereas re will double count terms if they appear in different categories): \n",
    "indices_for_render = [3766, 5377, 13597]#idx_for_window\n",
    "for i in indices_for_render: \n",
    "    print(\"The NDC word for the window is \", all_labelled_doc[i])\n",
    "    window_tokens = return_window_i(i, all_labelled_doc, size=window_size)[2]\n",
    "    displacy.render(window_tokens, style = \"ent\", jupyter = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c857f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:28:41.536635Z",
     "start_time": "2021-11-04T22:28:41.199352Z"
    }
   },
   "source": [
    "### Summary of topic (mostly SDG) keywords at the document level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9093db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_text = ' '.join([token.text for token in tokens])\n",
    "df_sdg = make_sdg_df(sdg_list, sdg_ontology, document_text)\n",
    "doc_summary_sdg = df_sdg.groupby('sdg_topic')['sdg_keywords_num'].sum()\n",
    "doc_summary_sdg.to_frame().rename(columns={\"sdg_keywords_num\": (\"%s\"% doc_name)})\n",
    "\n",
    "plot_folder = '../../outputs/heatmaps/'\n",
    "#print a summary of the SDG words found: \n",
    "doc_summary_sdg = df_sdg.groupby('sdg_topic')['sdg_keywords_num'].sum()\n",
    "doc_summary_sdg.to_frame()\n",
    "#doc_summary_sdg.to_frame()\n",
    "plt.figure(figsize=(2, 6))\n",
    "ax = sns.heatmap(doc_summary_sdg.to_frame(), \n",
    "                 annot=True, fmt=\"d\", \n",
    "                 cmap=\"YlGnBu\")\n",
    "\n",
    "#plt.xlabel(\"climate change NDC-associated windows\")\n",
    "plt.ylabel(\"Topics\")\n",
    "title = (\"Topic keywords in %s\" % (doc_name))\n",
    "plt.title(title)\n",
    "file_name=(title + (' across document v1.png'))\n",
    "#plt.savefig((plot_folder+file_name), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb1f86a",
   "metadata": {},
   "source": [
    "### can also see specific keywords mentioned the most in the document, not just the counts: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f229ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T08:11:22.665452Z",
     "start_time": "2021-11-05T08:11:22.153909Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sdg_sorted  = df_sdg.sort_values(by=['sdg_keywords_num'], ascending=False)\n",
    "list(df_sdg_sorted[df_sdg_sorted['sdg_keywords_num'] > 0]['sdg_keywords'][:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e379735",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:48:36.716699Z",
     "start_time": "2021-11-04T22:48:35.936147Z"
    }
   },
   "source": [
    "## Apply topic keyword searching within the windows around NDC words to localize the topic searching to those areas of the document  - using Method 1 and idx positions (similar to results from presentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc8169c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:48:44.482914Z",
     "start_time": "2021-11-04T22:48:44.476742Z"
    }
   },
   "outputs": [],
   "source": [
    "#may want to increase the min_distance/set it as a function of window size\n",
    "window_size = 200\n",
    "min_dist = 100\n",
    "idx_for_window = idx_for_window_from_doc_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c59104",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:50:38.264461Z",
     "start_time": "2021-11-04T22:49:02.540867Z"
    }
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for index in idx_for_window[0:len(idx_for_window)]:\n",
    "    count += 1\n",
    "    print(index)\n",
    "    window_tokens = return_window_idx(index, tokens, size=window_size)[2] #window_tokens_overall\n",
    "    window_text = ' '.join([token.text for token in window_tokens])\n",
    "    #print(window_text)\n",
    "    df_sdg = make_sdg_df(sdg_list, sdg_ontology, window_text) #may want to keep this for the windows for a more granular analysis\n",
    "    df_sdg_sorted = df_sdg.sort_values(by=['sdg_keywords_num'], ascending=False)\n",
    "    #print(df_sdg_sorted.head())\n",
    "    print(list(df_sdg_sorted[df_sdg_sorted['sdg_keywords_num'] > 0]['sdg_keywords'][:10])) #print top 10 positive valued keywords\n",
    "    #print a summary of the SDG words found: \n",
    "    summary_sdg = df_sdg.groupby('sdg_topic')['sdg_keywords_num'].sum()\n",
    "    if count == 1: \n",
    "        summary_sdg_df = summary_sdg.to_frame().rename(columns={\"sdg_keywords_num\": (\"sdg_kw_%d\"% index)})\n",
    "    else: \n",
    "        summary_sdg_df_for_merge = summary_sdg.to_frame().rename(columns={\"sdg_keywords_num\": (\"sdg_kw_%d\"% index)})\n",
    "        summary_sdg_df = summary_sdg_df.merge(summary_sdg_df_for_merge, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc621d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:50:38.288112Z",
     "start_time": "2021-11-04T22:50:38.266666Z"
    }
   },
   "outputs": [],
   "source": [
    "len(summary_sdg_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520b8166",
   "metadata": {},
   "source": [
    "### Can create a heatmap visualization to where the topics are rows and the columns are each window (where the columns on the left are from the start of the document and the ones on the right are from the end). The number of keyword mentions found for each topic within each window are used to fill in the heatmap and color it (SHOWING RESULTS FROM METHOD 1 of labelling NDC words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5012a507",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T08:08:44.447055Z",
     "start_time": "2021-11-05T08:08:42.366958Z"
    }
   },
   "outputs": [],
   "source": [
    "#make heatmap of plot above\n",
    "plt.figure(figsize=(16, 6))\n",
    "ax = sns.heatmap(summary_sdg_df,\n",
    "                 #annot=True, fmt=\"d\", \n",
    "                 cmap=\"YlGnBu\")\n",
    "plt.xlabel(\"climate change NDC-associated windows\")\n",
    "plt.ylabel(\"SDG Topics\")\n",
    "title = (\"Distribution of topic keywords in climate change NDC-associated windows in %s\" % (doc_name))\n",
    "plt.title(title)\n",
    "file_name=(title + (' across all windows v1.png'))\n",
    "#plt.savefig((plot_folder+file_name), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e67a739",
   "metadata": {},
   "source": [
    "### can filter the heatmap visualization to focus on one section of the document and not the whole thing and show the actual numbers of keywords in each window in that section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc43662",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T08:50:35.824422Z",
     "start_time": "2021-11-05T08:50:30.550327Z"
    }
   },
   "outputs": [],
   "source": [
    "#make heatmap of plot above\n",
    "plt.figure(figsize=(16, 6))\n",
    "ax = sns.heatmap(summary_sdg_df.iloc[:, 380:], #can show all windows with summary_sdg_df\n",
    "                 annot=True, fmt=\"d\", \n",
    "                 cmap=\"YlGnBu\")\n",
    "plt.xlabel(\"climate change NDC-associated windows\")\n",
    "plt.ylabel(\"SDG Topics\")\n",
    "title = (\"Distribution of topic keywords in climate change NDC-associated windows in %s\" % (doc_name))\n",
    "plt.title(title)\n",
    "file_name=(title + (' across windows 40-100 v1.png'))\n",
    "#plt.savefig((plot_folder+file_name), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfbcac1",
   "metadata": {},
   "source": [
    "### can filter the heatmap visualization to focus on columns (each representing an NDC window) with the most topic keyword mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b189df",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_sdg_df_filtered = summary_sdg_df.loc[:, (summary_sdg_df.sum() >= 15)] #filter for windows with at least 15 topic keywords\n",
    "plt.figure(figsize=(16, 6))\n",
    "ax = sns.heatmap(summary_sdg_df_filtered, #can show all windows with summary_sdg_df\n",
    "                 annot=True, fmt=\"d\", \n",
    "                 cmap=\"YlGnBu\")\n",
    "plt.xlabel(\"climate change NDC-associated windows\")\n",
    "plt.ylabel(\"Topics\")\n",
    "title = (\"Distribution of topic keywords in climate change NDC-associated windows in %s\" % (doc_name))\n",
    "plt.title(title)\n",
    "file_name=(title + (' across windows with greater than 15 keywords v1.png'))\n",
    "#plt.savefig((plot_folder+file_name), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734824f6",
   "metadata": {},
   "source": [
    "### Lets see some examples of the text windows that we might think are interesting from their scores in the heatmap:\n",
    "#### SDG15: \"Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss\"\n",
    "https://sdgs.un.org/goals/goal15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96409c5",
   "metadata": {},
   "source": [
    "Below see some example of rendered windows, and just keep in mind that the NDC word/term will always be present at the word index associated with the window, but sometimes it is not properly labelled in the render, probably due to how the span labelling function works in spacy). Also, the number of spans labelled by spacy will always be smaller than the values in the heatmap because no overlapping values are allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346b3ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_for_render = [78415, 81201] \n",
    "for idx in indices_for_render: \n",
    "    word_i_for_idx = ndc_i_df[ndc_i_df['doc_position_idx_index']==idx]['word_index'][0]\n",
    "    print(\"The NDC word for the window is \", [token for token in tokens if token.idx == idx])\n",
    "    displacy.render(all_labelled_doc[(word_i_for_idx-30):(word_i_for_idx+30)], style = \"ent\", jupyter = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6b4630",
   "metadata": {},
   "source": [
    "#### SDG14: \"Conserve and sustainably use the oceans, seas and marine resources for sustainable development\"\n",
    "https://sdgs.un.org/goals/goal14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b03bf3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:50:41.267008Z",
     "start_time": "2021-11-04T22:50:41.239511Z"
    }
   },
   "outputs": [],
   "source": [
    "indices_for_render = [89041] #note that system is both a climate change keyword and an early warning keyword\n",
    "for idx in indices_for_render: \n",
    "    word_i_for_idx = ndc_i_df[ndc_i_df['doc_position_idx_index']==idx]['word_index'][0]\n",
    "    print(\"The NDC word for the window is \", [token for token in tokens if token.idx == idx])\n",
    "    displacy.render(all_labelled_doc[(word_i_for_idx-30):(word_i_for_idx+30)], style = \"ent\", jupyter = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a427fa",
   "metadata": {},
   "source": [
    "#### An example of a window with several words from the mention money category among others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8a3549",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:50:41.355159Z",
     "start_time": "2021-11-04T22:50:41.326148Z"
    }
   },
   "outputs": [],
   "source": [
    "indices_for_render = [34339, 68913, 236176, 237041] \n",
    "for idx in indices_for_render: \n",
    "    word_i_for_idx = ndc_i_df[ndc_i_df['doc_position_idx_index']==idx]['word_index'][0]\n",
    "    print(\"The NDC word for the window is \", [token for token in tokens if token.idx == idx])\n",
    "    displacy.render(all_labelled_doc[(word_i_for_idx-30):(word_i_for_idx+30)], style = \"ent\", jupyter = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef91e82",
   "metadata": {},
   "source": [
    "### If want to be able to label windows with keyword mentions de novo, can use the below function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2038c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_ndc_sdg_spans_in_windows(keyword_df, doc, topic_column='clasification'):\n",
    "    \"\"\"keyword_df is a pandas df, doc is the window text with nlp run on it.\"\"\"\n",
    "    matcher = PhraseMatcher(nlp.vocab)\n",
    "    entity_labels = []\n",
    "    topics = list(keyword_df[topic_column].value_counts().index)\n",
    "    for topic in topics:\n",
    "        entity_labels.append(topic)\n",
    "        keywords = list(keyword_df[keyword_df[topic_column]==topic]['keyword'])\n",
    "        #print(keywords)\n",
    "        patterns = [nlp(i) for i in keywords]\n",
    "        #print(keywords, patterns)\n",
    "        matcher.add(topic, None, *patterns)\n",
    "    matches = matcher(doc)\n",
    "        #label spans\n",
    "    for match_id, start, end in matches:\n",
    "        try:\n",
    "            span = Span(doc, start, end, label=match_id)\n",
    "            doc.ents = list(doc.ents) + [span]  # add span to doc.ents\n",
    "        except: \n",
    "            pass\n",
    "    return entity_labels, doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ec02a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list_for_render = [34339, 68913, 236176, 237041] \n",
    "for i in index_list_for_render: \n",
    "    print(i)\n",
    "    window_to_label = nlp(return_window_idx(i, tokens, size=window_size)[2])\n",
    "    entity_labels, labelled_window = label_ndc_sdg_spans_in_windows(sdg_ontology, window_to_label, topic_column='clasification')\n",
    "    displacy.render(labelled_window, style = \"ent\", jupyter = True) #use original tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2dae3f",
   "metadata": {},
   "source": [
    "### coherence and relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686d7723",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_for_frame = summary_sdg_df.sum(axis=1)/len(summary_sdg_df.columns)\n",
    "coherence_df = coherence_for_frame.to_frame().rename(columns={0:\"Topic score\"})\n",
    "coherence_score = coherence_df['Topic score'].sum()\n",
    "print(coherence_score)\n",
    "coherence_df\n",
    "\n",
    "relevance_score = len(ndc_climate_idxs)/len(sentences)\n",
    "print('The relevance score for %s is' % doc_name , relevance_score)\n",
    "print('The coherence score for %s is' % doc_name , coherence_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bf9549",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print a summary of the SDG words found in the windows: \n",
    "plt.figure(figsize=(2, 6))\n",
    "ax = sns.heatmap(coherence_df, \n",
    "                 annot=True, #fmt=\"d\", \n",
    "                 cmap=\"YlGnBu\")\n",
    "\n",
    "plt.xlabel(\"Score in climate change NDC-associated windows\")\n",
    "plt.ylabel(\"Topics\")\n",
    "title = (\"NDC-associated topic scores in %s\" % (doc_name))\n",
    "plt.title(title)\n",
    "file_name=(title + (' across document normalized by number of windows v1.png'))\n",
    "# plt.savefig((plot_folder+file_name), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc4e596",
   "metadata": {},
   "source": [
    "## Additional functions in spacy that could be useful and worth exploring more"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c7511c",
   "metadata": {},
   "source": [
    "### Parts of speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b40fc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:29:03.177541Z",
     "start_time": "2021-11-04T22:29:03.177528Z"
    }
   },
   "outputs": [],
   "source": [
    "for token in filtered_tokens[:50]:\n",
    "    print(token, token.tag_, token.pos_, spacy.explain(token.tag_))\n",
    "print('\\n')\n",
    "    \n",
    "nouns = []\n",
    "adjectives = []\n",
    "for token in filtered_tokens:\n",
    "    if token.pos_ == 'NOUN':\n",
    "        nouns.append(token)\n",
    "    if token.pos_ == 'ADJ':\n",
    "        adjectives.append(token)\n",
    "\n",
    "print('some nouns spacy called in the document include:', '\\n', nouns[:50], '\\n')\n",
    "print('some adjectives spacy called in the document include:', '\\n', adjectives[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d245d4",
   "metadata": {},
   "source": [
    "### Dependency visualization in sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24796c38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T22:29:03.179664Z",
     "start_time": "2021-11-04T22:29:03.179653Z"
    }
   },
   "outputs": [],
   "source": [
    "sent_example = nlp(\"For example, it is estimated that between 9 and 12 million DATE people in impoverished rural areas directly use natural resources such as fuel wood, wild fruits and wooden utensils as a source of energy, food and building material respectively (Shackleton ORG 2004)\")\n",
    "sentence_spans = list(sentences)\n",
    "sentence_spans[:10]\n",
    "displacy.render(sentence_spans[80], style=\"dep\", jupyter= True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "75547552c603f22400c6f6e0e4ad2ade15359435e000e6746c53241e37331409"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
